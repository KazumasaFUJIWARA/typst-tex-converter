Llama 3.2 3Bモデル: meta-llama/Llama-3.2-3B-Instruct
データ: ../jsonl/train_003.jsonl
出力: ../outputs/llama32-3b-typst-qlora-003-000
学習率: 1e-05
エポック数: 12
バッチサイズ: 1
勾配累積: 16
LoRA rank: 8
LoRA alpha: 8
LoRA dropout: 0.1
==================================================
トークナイザーを読み込み中...
4bit量子化設定を準備中...
モデルを読み込み中...

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.01s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.02s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.32s/it]
既存のPEFTモデルを読み込み中: ../outputs/llama32-3b-typst-qlora-002-010
データセットを読み込み中...

Generating train split: 0 examples [00:00, ? examples/s]
Generating train split: 43 examples [00:00, 342.17 examples/s]
Generating train split: 43 examples [00:00, 316.91 examples/s]
/mnt/d/github/typst-tex-converter/venv/lib/python3.11/site-packages/peft/tuners/lora/bnb.py:348: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.
  warnings.warn(
/mnt/d/github/typst-tex-converter/venv/lib/python3.11/site-packages/peft/tuners/tuners_utils.py:196: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!
  warnings.warn(
データセットサイズ: 43

Tokenizing train dataset:   0%|          | 0/43 [00:00<?, ? examples/s]
Tokenizing train dataset: 100%|██████████| 43/43 [00:00<00:00, 683.76 examples/s]

Truncating train dataset:   0%|          | 0/43 [00:00<?, ? examples/s]
Truncating train dataset: 100%|██████████| 43/43 [00:00<00:00, 9119.44 examples/s]
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 128009}.
Llama 3.2 3B QLoRA学習を開始します...

