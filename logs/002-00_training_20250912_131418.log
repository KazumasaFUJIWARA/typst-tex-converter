Llama 3.2 3Bモデル: meta-llama/Llama-3.2-3B-Instruct
データ: ../jsonl/train_002.jsonl
出力: ../outputs/llama32-3b-typst-qlora-002-00
学習率: 0.0001
エポック数: 5
バッチサイズ: 2
勾配累積: 8
LoRA rank: 8
LoRA alpha: 16
LoRA dropout: 0.1
==================================================
トークナイザーを読み込み中...
4bit量子化設定を準備中...
モデルを読み込み中...

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.11s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  1.83s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.02s/it]
/mnt/d/github/typst-tex-converter/venv/lib/python3.11/site-packages/peft/tuners/lora/bnb.py:348: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.
  warnings.warn(
/mnt/d/github/typst-tex-converter/venv/lib/python3.11/site-packages/peft/tuners/tuners_utils.py:196: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!
  warnings.warn(
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 128009}.
既存のPEFTモデルを読み込み中: ../outputs/llama32-3b-typst-qlora-001/checkpoint-60
データセットを読み込み中...
データセットサイズ: 92
Llama 3.2 3B QLoRA学習を開始します...

  0%|          | 0/30 [00:00<?, ?it/s]
  3%|▎         | 1/30 [00:06<03:16,  6.78s/it]
                                              

  3%|▎         | 1/30 [00:06<03:16,  6.78s/it]
  7%|▋         | 2/30 [00:45<11:57, 25.64s/it]
                                              

  7%|▋         | 2/30 [00:45<11:57, 25.64s/it]
 10%|█         | 3/30 [01:16<12:40, 28.15s/it]
                                              

 10%|█         | 3/30 [01:16<12:40, 28.15s/it]
 13%|█▎        | 4/30 [01:48<12:51, 29.67s/it]
                                              

 13%|█▎        | 4/30 [01:48<12:51, 29.67s/it]
 17%|█▋        | 5/30 [02:28<13:52, 33.32s/it]
                                              

 17%|█▋        | 5/30 [02:28<13:52, 33.32s/it]
 20%|██        | 6/30 [03:01<13:18, 33.25s/it]
                                              

 20%|██        | 6/30 [03:01<13:18, 33.25s/it]
 23%|██▎       | 7/30 [03:39<13:17, 34.69s/it]
                                              

 23%|██▎       | 7/30 [03:39<13:17, 34.69s/it]
 27%|██▋       | 8/30 [04:22<13:42, 37.38s/it]
                                              

 27%|██▋       | 8/30 [04:22<13:42, 37.38s/it]
 30%|███       | 9/30 [04:52<12:18, 35.17s/it]
                                              

 30%|███       | 9/30 [04:52<12:18, 35.17s/it]
 33%|███▎      | 10/30 [05:27<11:41, 35.09s/it]
                                               

 33%|███▎      | 10/30 [05:27<11:41, 35.09s/it]
 37%|███▋      | 11/30 [05:57<10:34, 33.40s/it]
                                               

 37%|███▋      | 11/30 [05:57<10:34, 33.40s/it]
 40%|████      | 12/30 [06:26<09:36, 32.05s/it]
                                               

 40%|████      | 12/30 [06:26<09:36, 32.05s/it]
 43%|████▎     | 13/30 [06:56<08:55, 31.51s/it]
                                               

 43%|████▎     | 13/30 [06:56<08:55, 31.51s/it]
 47%|████▋     | 14/30 [07:32<08:48, 33.01s/it]
                                               

 47%|████▋     | 14/30 [07:32<08:48, 33.01s/it]
 50%|█████     | 15/30 [08:14<08:53, 35.56s/it]
                                               

 50%|█████     | 15/30 [08:14<08:53, 35.56s/it]
 53%|█████▎    | 16/30 [09:17<10:14, 43.89s/it]
                                               

