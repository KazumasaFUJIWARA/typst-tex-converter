Llama 3.2 3Bモデル: meta-llama/Llama-3.2-3B-Instruct
データ: ../jsonl/train_003.jsonl
出力: ../outputs/llama32-3b-typst-qlora-003-004
学習率: 1e-05
エポック数: 12
バッチサイズ: 1
勾配累積: 16
LoRA rank: 8
LoRA alpha: 8
LoRA dropout: 0.1
==================================================
トークナイザーを読み込み中...
4bit量子化設定を準備中...
モデルを読み込み中...

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.93s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.72s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.90s/it]
/mnt/d/github/typst-tex-converter/venv/lib/python3.11/site-packages/peft/tuners/lora/bnb.py:348: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.
  warnings.warn(
/mnt/d/github/typst-tex-converter/venv/lib/python3.11/site-packages/peft/tuners/tuners_utils.py:196: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!
  warnings.warn(
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 128009}.
既存のPEFTモデルを読み込み中: ../outputs/llama32-3b-typst-qlora-003-003
データセットを読み込み中...
データセットサイズ: 43
Llama 3.2 3B QLoRA学習を開始します...

  0%|          | 0/36 [00:00<?, ?it/s]
  3%|▎         | 1/36 [00:09<05:23,  9.25s/it]
                                              

  3%|▎         | 1/36 [00:09<05:23,  9.25s/it]
  6%|▌         | 2/36 [00:35<11:03, 19.52s/it]
                                              

  6%|▌         | 2/36 [00:35<11:03, 19.52s/it]
  8%|▊         | 3/36 [00:44<08:00, 14.55s/it]
                                              

  8%|▊         | 3/36 [00:44<08:00, 14.55s/it]
 11%|█         | 4/36 [00:57<07:22, 13.83s/it]
                                              

 11%|█         | 4/36 [00:57<07:22, 13.83s/it]
 14%|█▍        | 5/36 [01:21<08:59, 17.40s/it]
                                              

 14%|█▍        | 5/36 [01:21<08:59, 17.40s/it]
 17%|█▋        | 6/36 [01:38<08:38, 17.27s/it]
                                              

 17%|█▋        | 6/36 [01:38<08:38, 17.27s/it]
 19%|█▉        | 7/36 [01:53<08:07, 16.80s/it]
                                              

 19%|█▉        | 7/36 [01:53<08:07, 16.80s/it]
 22%|██▏       | 8/36 [02:11<07:56, 17.02s/it]
                                              

 22%|██▏       | 8/36 [02:11<07:56, 17.02s/it]
 25%|██▌       | 9/36 [02:32<08:10, 18.15s/it]
                                              

 25%|██▌       | 9/36 [02:32<08:10, 18.15s/it]
 28%|██▊       | 10/36 [02:55<08:32, 19.72s/it]
                                               

 28%|██▊       | 10/36 [02:55<08:32, 19.72s/it]
 31%|███       | 11/36 [03:11<07:45, 18.62s/it]
                                               

 31%|███       | 11/36 [03:11<07:45, 18.62s/it]
 33%|███▎      | 12/36 [03:25<06:50, 17.11s/it]
                                               

 33%|███▎      | 12/36 [03:25<06:50, 17.11s/it]
 36%|███▌      | 13/36 [03:51<07:35, 19.80s/it]
                                               

 36%|███▌      | 13/36 [03:51<07:35, 19.80s/it]
 39%|███▉      | 14/36 [04:06<06:50, 18.64s/it]
                                               

 39%|███▉      | 14/36 [04:06<06:50, 18.64s/it]
 42%|████▏     | 15/36 [04:19<05:49, 16.66s/it]
                                               

 42%|████▏     | 15/36 [04:19<05:49, 16.66s/it]
 44%|████▍     | 16/36 [04:30<05:01, 15.09s/it]
                                               

 44%|████▍     | 16/36 [04:30<05:01, 15.09s/it]
 47%|████▋     | 17/36 [05:03<06:28, 20.46s/it]
                                               

 47%|████▋     | 17/36 [05:03<06:28, 20.46s/it]
 50%|█████     | 18/36 [05:12<05:05, 16.99s/it]
                                               

 50%|█████     | 18/36 [05:12<05:05, 16.99s/it]
 53%|█████▎    | 19/36 [05:34<05:12, 18.40s/it]
                                               

 53%|█████▎    | 19/36 [05:34<05:12, 18.40s/it]
 56%|█████▌    | 20/36 [05:55<05:06, 19.17s/it]
                                               

 56%|█████▌    | 20/36 [05:55<05:06, 19.17s/it]
 58%|█████▊    | 21/36 [06:04<04:05, 16.36s/it]
                                               

 58%|█████▊    | 21/36 [06:04<04:05, 16.36s/it]
 61%|██████    | 22/36 [06:26<04:09, 17.85s/it]
                                               

 61%|██████    | 22/36 [06:26<04:09, 17.85s/it]
 64%|██████▍   | 23/36 [06:48<04:10, 19.25s/it]
                                               

 64%|██████▍   | 23/36 [06:48<04:10, 19.25s/it]
 67%|██████▋   | 24/36 [06:58<03:15, 16.28s/it]
                                               

 67%|██████▋   | 24/36 [06:58<03:15, 16.28s/it]
 69%|██████▉   | 25/36 [07:13<02:55, 15.97s/it]
                                               

 69%|██████▉   | 25/36 [07:13<02:55, 15.97s/it]
 72%|███████▏  | 26/36 [07:31<02:46, 16.61s/it]
                                               

 72%|███████▏  | 26/36 [07:31<02:46, 16.61s/it]
 75%|███████▌  | 27/36 [07:50<02:36, 17.40s/it]
                                               

 75%|███████▌  | 27/36 [07:50<02:36, 17.40s/it]
 78%|███████▊  | 28/36 [08:17<02:41, 20.15s/it]
                                               

 78%|███████▊  | 28/36 [08:17<02:41, 20.15s/it]
 81%|████████  | 29/36 [08:33<02:14, 19.15s/it]
                                               

 81%|████████  | 29/36 [08:33<02:14, 19.15s/it]
 83%|████████▎ | 30/36 [08:43<01:38, 16.39s/it]
                                               

 83%|████████▎ | 30/36 [08:43<01:38, 16.39s/it]
 86%|████████▌ | 31/36 [08:56<01:16, 15.22s/it]
                                               

 86%|████████▌ | 31/36 [08:56<01:16, 15.22s/it]
 89%|████████▉ | 32/36 [09:20<01:12, 18.02s/it]
                                               

 89%|████████▉ | 32/36 [09:20<01:12, 18.02s/it]
 92%|█████████▏| 33/36 [09:37<00:52, 17.49s/it]
                                               

 92%|█████████▏| 33/36 [09:37<00:52, 17.49s/it]
 94%|█████████▍| 34/36 [09:50<00:32, 16.10s/it]
                                               

 94%|█████████▍| 34/36 [09:50<00:32, 16.10s/it]
 97%|█████████▋| 35/36 [10:07<00:16, 16.50s/it]
                                               

 97%|█████████▋| 35/36 [10:07<00:16, 16.50s/it]
100%|██████████| 36/36 [10:30<00:00, 18.30s/it]
                                               

100%|██████████| 36/36 [10:30<00:00, 18.30s/it]
                                               

100%|██████████| 36/36 [10:31<00:00, 18.30s/it]
100%|██████████| 36/36 [10:31<00:00, 17.55s/it]
{'loss': 1.0019, 'grad_norm': 2.3068974018096924, 'learning_rate': 0.0, 'entropy': 1.6676861234009266, 'num_tokens': 1968.0, 'mean_token_accuracy': 0.8097846880555153, 'epoch': 0.37}
{'loss': 1.045, 'grad_norm': 2.3416287899017334, 'learning_rate': 2.5e-06, 'entropy': 1.3881262131035328, 'num_tokens': 5029.0, 'mean_token_accuracy': 0.8093724586069584, 'epoch': 0.74}
{'loss': 1.097, 'grad_norm': 2.9021127223968506, 'learning_rate': 5e-06, 'entropy': 1.5273766842755405, 'num_tokens': 6318.0, 'mean_token_accuracy': 0.8274629007686268, 'epoch': 1.0}
{'loss': 1.2103, 'grad_norm': 3.1434326171875, 'learning_rate': 7.500000000000001e-06, 'entropy': 1.6615911945700645, 'num_tokens': 8014.0, 'mean_token_accuracy': 0.7986961454153061, 'epoch': 1.37}
{'loss': 1.0112, 'grad_norm': 2.0979325771331787, 'learning_rate': 1e-05, 'entropy': 1.3551450483500957, 'num_tokens': 11175.0, 'mean_token_accuracy': 0.8248906023800373, 'epoch': 1.74}
{'loss': 0.8326, 'grad_norm': 2.0658507347106934, 'learning_rate': 9.975923633360985e-06, 'entropy': 1.58822809566151, 'num_tokens': 12636.0, 'mean_token_accuracy': 0.8517454103990034, 'epoch': 2.0}
{'loss': 0.9869, 'grad_norm': 2.22883939743042, 'learning_rate': 9.903926402016153e-06, 'entropy': 1.5208427868783474, 'num_tokens': 14723.0, 'mean_token_accuracy': 0.839100543409586, 'epoch': 2.37}
{'loss': 1.0355, 'grad_norm': 2.6847474575042725, 'learning_rate': 9.784701678661045e-06, 'entropy': 1.51134667173028, 'num_tokens': 17027.0, 'mean_token_accuracy': 0.8221327364444733, 'epoch': 2.74}
{'loss': 0.8287, 'grad_norm': 1.5327379703521729, 'learning_rate': 9.619397662556434e-06, 'entropy': 1.5797650488940151, 'num_tokens': 18954.0, 'mean_token_accuracy': 0.8232823339375582, 'epoch': 3.0}
{'loss': 0.9047, 'grad_norm': 1.857731819152832, 'learning_rate': 9.409606321741776e-06, 'entropy': 1.5173694603145123, 'num_tokens': 21674.0, 'mean_token_accuracy': 0.8265761323273182, 'epoch': 3.37}
{'loss': 0.8403, 'grad_norm': 2.2449872493743896, 'learning_rate': 9.157348061512728e-06, 'entropy': 1.6338846273720264, 'num_tokens': 23385.0, 'mean_token_accuracy': 0.8478846177458763, 'epoch': 3.74}
{'loss': 0.9171, 'grad_norm': 2.162449359893799, 'learning_rate': 8.865052266813686e-06, 'entropy': 1.4203600829297847, 'num_tokens': 25272.0, 'mean_token_accuracy': 0.8264588605273854, 'epoch': 4.0}
{'loss': 0.88, 'grad_norm': 1.8204199075698853, 'learning_rate': 8.535533905932739e-06, 'entropy': 1.4500420279800892, 'num_tokens': 28245.0, 'mean_token_accuracy': 0.8233850933611393, 'epoch': 4.37}
{'loss': 0.8432, 'grad_norm': 2.259028434753418, 'learning_rate': 8.171966420818227e-06, 'entropy': 1.5782388485968113, 'num_tokens': 30283.0, 'mean_token_accuracy': 0.8271992728114128, 'epoch': 4.74}
{'loss': 0.7479, 'grad_norm': 2.080070972442627, 'learning_rate': 7.777851165098012e-06, 'entropy': 1.613023037260229, 'num_tokens': 31590.0, 'mean_token_accuracy': 0.8610499772158536, 'epoch': 5.0}
{'loss': 0.875, 'grad_norm': 2.467240571975708, 'learning_rate': 7.3569836841299905e-06, 'entropy': 1.6345458924770355, 'num_tokens': 33360.0, 'mean_token_accuracy': 0.8238443247973919, 'epoch': 5.37}
{'loss': 0.7469, 'grad_norm': 1.553347110748291, 'learning_rate': 6.913417161825449e-06, 'entropy': 1.416241329163313, 'num_tokens': 36519.0, 'mean_token_accuracy': 0.8497111350297928, 'epoch': 5.74}
{'loss': 0.8061, 'grad_norm': 2.3499088287353516, 'learning_rate': 6.451423386272312e-06, 'entropy': 1.5946863998066296, 'num_tokens': 37908.0, 'mean_token_accuracy': 0.8347415707328103, 'epoch': 6.0}
{'loss': 0.7342, 'grad_norm': 1.8013039827346802, 'learning_rate': 5.975451610080643e-06, 'entropy': 1.4823154918849468, 'num_tokens': 40619.0, 'mean_token_accuracy': 0.8378305993974209, 'epoch': 6.37}
{'loss': 0.7747, 'grad_norm': 1.9777101278305054, 'learning_rate': 5.490085701647805e-06, 'entropy': 1.5504143610596657, 'num_tokens': 43042.0, 'mean_token_accuracy': 0.8567813038825989, 'epoch': 6.74}
{'loss': 0.7467, 'grad_norm': 2.289457082748413, 'learning_rate': 5e-06, 'entropy': 1.6311028328808872, 'num_tokens': 44226.0, 'mean_token_accuracy': 0.8438582691279325, 'epoch': 7.0}
{'loss': 0.6604, 'grad_norm': 1.6777324676513672, 'learning_rate': 4.509914298352197e-06, 'entropy': 1.5323438607156277, 'num_tokens': 46461.0, 'mean_token_accuracy': 0.8868236690759659, 'epoch': 7.37}
{'loss': 0.7272, 'grad_norm': 1.626431941986084, 'learning_rate': 4.02454838991936e-06, 'entropy': 1.5362535119056702, 'num_tokens': 49072.0, 'mean_token_accuracy': 0.849740494042635, 'epoch': 7.74}
{'loss': 0.8022, 'grad_norm': 2.434525966644287, 'learning_rate': 3.5485766137276894e-06, 'entropy': 1.586422009901567, 'num_tokens': 50544.0, 'mean_token_accuracy': 0.8151074810461565, 'epoch': 8.0}
{'loss': 0.7533, 'grad_norm': 1.9368940591812134, 'learning_rate': 3.0865828381745515e-06, 'entropy': 1.4476257152855396, 'num_tokens': 53147.0, 'mean_token_accuracy': 0.8304522782564163, 'epoch': 8.37}
{'loss': 0.6934, 'grad_norm': 1.795292615890503, 'learning_rate': 2.6430163158700116e-06, 'entropy': 1.604564182460308, 'num_tokens': 55190.0, 'mean_token_accuracy': 0.8688659891486168, 'epoch': 8.74}
{'loss': 0.6212, 'grad_norm': 1.6417334079742432, 'learning_rate': 2.2221488349019903e-06, 'entropy': 1.6127832640301099, 'num_tokens': 56862.0, 'mean_token_accuracy': 0.8961980397051031, 'epoch': 9.0}
{'loss': 0.6379, 'grad_norm': 1.5426075458526611, 'learning_rate': 1.8280335791817733e-06, 'entropy': 1.5530737936496735, 'num_tokens': 59543.0, 'mean_token_accuracy': 0.8657520264387131, 'epoch': 9.37}
{'loss': 0.7107, 'grad_norm': 2.0210888385772705, 'learning_rate': 1.4644660940672628e-06, 'entropy': 1.594536792486906, 'num_tokens': 61573.0, 'mean_token_accuracy': 0.8532840460538864, 'epoch': 9.74}
{'loss': 0.7431, 'grad_norm': 1.956520915031433, 'learning_rate': 1.134947733186315e-06, 'entropy': 1.4753011952746997, 'num_tokens': 63180.0, 'mean_token_accuracy': 0.874210926619443, 'epoch': 10.0}
{'loss': 0.7748, 'grad_norm': 2.0813040733337402, 'learning_rate': 8.426519384872733e-07, 'entropy': 1.5503505766391754, 'num_tokens': 65326.0, 'mean_token_accuracy': 0.8684266731142998, 'epoch': 10.37}
{'loss': 0.6108, 'grad_norm': 1.5283970832824707, 'learning_rate': 5.903936782582253e-07, 'entropy': 1.488686054944992, 'num_tokens': 67813.0, 'mean_token_accuracy': 0.8656345643103123, 'epoch': 10.74}
{'loss': 0.6697, 'grad_norm': 1.5679937601089478, 'learning_rate': 3.8060233744356634e-07, 'entropy': 1.6306933164596558, 'num_tokens': 69498.0, 'mean_token_accuracy': 0.8598770986903798, 'epoch': 11.0}
{'loss': 0.7007, 'grad_norm': 1.9287660121917725, 'learning_rate': 2.152983213389559e-07, 'entropy': 1.5962566509842873, 'num_tokens': 71308.0, 'mean_token_accuracy': 0.8598572909832001, 'epoch': 11.37}
{'loss': 0.6788, 'grad_norm': 1.8448803424835205, 'learning_rate': 9.607359798384785e-08, 'entropy': 1.656359013170004, 'num_tokens': 73415.0, 'mean_token_accuracy': 0.8862172104418278, 'epoch': 11.74}
{'loss': 0.6579, 'grad_norm': 1.6474968194961548, 'learning_rate': 2.4076366639015914e-08, 'entropy': 1.3237988027659329, 'num_tokens': 75816.0, 'mean_token_accuracy': 0.8395405357534235, 'epoch': 12.0}
{'train_runtime': 631.7285, 'train_samples_per_second': 0.817, 'train_steps_per_second': 0.057, 'train_loss': 0.8141101549069086, 'epoch': 12.0}
モデルを保存中...
Llama 3.2 3B QLoRA学習完了！モデルは ../outputs/llama32-3b-typst-qlora-003-004 に保存されました。
