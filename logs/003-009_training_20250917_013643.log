Llama 3.2 3Bモデル: meta-llama/Llama-3.2-3B-Instruct
データ: ../jsonl/train_003.jsonl
出力: ../outputs/llama32-3b-typst-qlora-003-009
学習率: 1e-05
エポック数: 12
バッチサイズ: 1
勾配累積: 16
LoRA rank: 8
LoRA alpha: 8
LoRA dropout: 0.1
==================================================
トークナイザーを読み込み中...
4bit量子化設定を準備中...
モデルを読み込み中...

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.72s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.63s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.79s/it]
/mnt/d/github/typst-tex-converter/venv/lib/python3.11/site-packages/peft/tuners/lora/bnb.py:348: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.
  warnings.warn(
/mnt/d/github/typst-tex-converter/venv/lib/python3.11/site-packages/peft/tuners/tuners_utils.py:196: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!
  warnings.warn(
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 128009}.
既存のPEFTモデルを読み込み中: ../outputs/llama32-3b-typst-qlora-003-008
データセットを読み込み中...
データセットサイズ: 43
Llama 3.2 3B QLoRA学習を開始します...

  0%|          | 0/36 [00:00<?, ?it/s]
  3%|▎         | 1/36 [00:09<05:30,  9.44s/it]
                                              

  3%|▎         | 1/36 [00:09<05:30,  9.44s/it]
  6%|▌         | 2/36 [00:36<11:11, 19.75s/it]
                                              

  6%|▌         | 2/36 [00:36<11:11, 19.75s/it]
  8%|▊         | 3/36 [00:45<08:11, 14.90s/it]
                                              

  8%|▊         | 3/36 [00:45<08:11, 14.90s/it]
 11%|█         | 4/36 [00:58<07:32, 14.14s/it]
                                              

 11%|█         | 4/36 [00:58<07:32, 14.14s/it]
 14%|█▍        | 5/36 [01:22<09:12, 17.83s/it]
                                              

 14%|█▍        | 5/36 [01:22<09:12, 17.83s/it]
 17%|█▋        | 6/36 [01:40<08:49, 17.64s/it]
                                              

 17%|█▋        | 6/36 [01:40<08:49, 17.64s/it]
 19%|█▉        | 7/36 [01:55<08:10, 16.93s/it]
                                              

 19%|█▉        | 7/36 [01:55<08:10, 16.93s/it]
 22%|██▏       | 8/36 [02:13<08:05, 17.33s/it]
                                              

 22%|██▏       | 8/36 [02:13<08:05, 17.33s/it]
 25%|██▌       | 9/36 [02:34<08:16, 18.37s/it]
                                              

 25%|██▌       | 9/36 [02:34<08:16, 18.37s/it]
 28%|██▊       | 10/36 [03:00<08:59, 20.73s/it]
                                               

 28%|██▊       | 10/36 [03:00<08:59, 20.73s/it]
 31%|███       | 11/36 [03:18<08:18, 19.95s/it]
                                               

 31%|███       | 11/36 [03:18<08:18, 19.95s/it]
 33%|███▎      | 12/36 [03:34<07:26, 18.62s/it]
                                               

 33%|███▎      | 12/36 [03:34<07:26, 18.62s/it]
 36%|███▌      | 13/36 [04:02<08:14, 21.52s/it]
                                               

 36%|███▌      | 13/36 [04:02<08:14, 21.52s/it]
 39%|███▉      | 14/36 [04:20<07:29, 20.41s/it]
                                               

 39%|███▉      | 14/36 [04:20<07:29, 20.41s/it]
 42%|████▏     | 15/36 [04:33<06:25, 18.38s/it]
                                               

 42%|████▏     | 15/36 [04:33<06:25, 18.38s/it]
 44%|████▍     | 16/36 [04:47<05:37, 16.88s/it]
                                               

 44%|████▍     | 16/36 [04:47<05:37, 16.88s/it]
 47%|████▋     | 17/36 [05:23<07:11, 22.70s/it]
                                               

 47%|████▋     | 17/36 [05:23<07:11, 22.70s/it]
 50%|█████     | 18/36 [05:33<05:40, 18.92s/it]
                                               

 50%|█████     | 18/36 [05:33<05:40, 18.92s/it]
 53%|█████▎    | 19/36 [05:57<05:49, 20.53s/it]
                                               

 53%|█████▎    | 19/36 [05:57<05:49, 20.53s/it]
 56%|█████▌    | 20/36 [06:21<05:42, 21.43s/it]
                                               

 56%|█████▌    | 20/36 [06:21<05:42, 21.43s/it]
 58%|█████▊    | 21/36 [06:32<04:34, 18.27s/it]
                                               

 58%|█████▊    | 21/36 [06:32<04:34, 18.27s/it]
 61%|██████    | 22/36 [06:56<04:38, 19.88s/it]
                                               

 61%|██████    | 22/36 [06:56<04:38, 19.88s/it]
 64%|██████▍   | 23/36 [07:20<04:37, 21.38s/it]
                                               

 64%|██████▍   | 23/36 [07:20<04:37, 21.38s/it]
 67%|██████▋   | 24/36 [07:31<03:38, 18.23s/it]
                                               

 67%|██████▋   | 24/36 [07:31<03:38, 18.23s/it]
 69%|██████▉   | 25/36 [07:48<03:16, 17.90s/it]
                                               

 69%|██████▉   | 25/36 [07:48<03:16, 17.90s/it]
 72%|███████▏  | 26/36 [08:09<03:05, 18.57s/it]
                                               

 72%|███████▏  | 26/36 [08:09<03:05, 18.57s/it]
 75%|███████▌  | 27/36 [08:30<02:55, 19.47s/it]
                                               

 75%|███████▌  | 27/36 [08:30<02:55, 19.47s/it]
 78%|███████▊  | 28/36 [08:59<02:58, 22.34s/it]
                                               

 78%|███████▊  | 28/36 [08:59<02:58, 22.34s/it]
 81%|████████  | 29/36 [09:18<02:29, 21.30s/it]
                                               

 81%|████████  | 29/36 [09:18<02:29, 21.30s/it]
 83%|████████▎ | 30/36 [09:29<01:49, 18.27s/it]
                                               

 83%|████████▎ | 30/36 [09:29<01:49, 18.27s/it]
 86%|████████▌ | 31/36 [09:44<01:25, 17.11s/it]
                                               

 86%|████████▌ | 31/36 [09:44<01:25, 17.11s/it]
 89%|████████▉ | 32/36 [10:11<01:20, 20.21s/it]
                                               

 89%|████████▉ | 32/36 [10:11<01:20, 20.21s/it]
 92%|█████████▏| 33/36 [10:30<00:59, 19.71s/it]
                                               

 92%|█████████▏| 33/36 [10:30<00:59, 19.71s/it]
 94%|█████████▍| 34/36 [10:44<00:36, 18.15s/it]
                                               

 94%|█████████▍| 34/36 [10:44<00:36, 18.15s/it]
 97%|█████████▋| 35/36 [11:04<00:18, 18.59s/it]
                                               

 97%|█████████▋| 35/36 [11:04<00:18, 18.59s/it]
100%|██████████| 36/36 [11:29<00:00, 20.54s/it]
                                               

100%|██████████| 36/36 [11:29<00:00, 20.54s/it]
                                               

100%|██████████| 36/36 [11:31<00:00, 20.54s/it]
100%|██████████| 36/36 [11:31<00:00, 19.20s/it]
{'loss': 1.0021, 'grad_norm': 2.1798250675201416, 'learning_rate': 0.0, 'entropy': 1.6673270389437675, 'num_tokens': 1968.0, 'mean_token_accuracy': 0.8103914856910706, 'epoch': 0.37}
{'loss': 1.0451, 'grad_norm': 2.203592538833618, 'learning_rate': 2.5e-06, 'entropy': 1.3878592662513256, 'num_tokens': 5029.0, 'mean_token_accuracy': 0.8093724586069584, 'epoch': 0.74}
{'loss': 1.0963, 'grad_norm': 2.7560312747955322, 'learning_rate': 5e-06, 'entropy': 1.5269450816241177, 'num_tokens': 6318.0, 'mean_token_accuracy': 0.8274629007686268, 'epoch': 1.0}
{'loss': 1.2109, 'grad_norm': 2.964674949645996, 'learning_rate': 7.500000000000001e-06, 'entropy': 1.6613271981477737, 'num_tokens': 8014.0, 'mean_token_accuracy': 0.7986961454153061, 'epoch': 1.37}
{'loss': 1.012, 'grad_norm': 1.9761823415756226, 'learning_rate': 1e-05, 'entropy': 1.3550515621900558, 'num_tokens': 11175.0, 'mean_token_accuracy': 0.8249186985194683, 'epoch': 1.74}
{'loss': 0.8335, 'grad_norm': 1.957632303237915, 'learning_rate': 9.975923633360985e-06, 'entropy': 1.58796766129407, 'num_tokens': 12636.0, 'mean_token_accuracy': 0.8517454103990034, 'epoch': 2.0}
{'loss': 0.9888, 'grad_norm': 2.105733633041382, 'learning_rate': 9.903926402016153e-06, 'entropy': 1.5209044478833675, 'num_tokens': 14723.0, 'mean_token_accuracy': 0.8388511762022972, 'epoch': 2.37}
{'loss': 1.0385, 'grad_norm': 2.5514633655548096, 'learning_rate': 9.784701678661045e-06, 'entropy': 1.5109331235289574, 'num_tokens': 17027.0, 'mean_token_accuracy': 0.8221327364444733, 'epoch': 2.74}
{'loss': 0.8308, 'grad_norm': 1.459030270576477, 'learning_rate': 9.619397662556434e-06, 'entropy': 1.5797536048022183, 'num_tokens': 18954.0, 'mean_token_accuracy': 0.8229299729520624, 'epoch': 3.0}
{'loss': 0.9073, 'grad_norm': 1.7921710014343262, 'learning_rate': 9.409606321741776e-06, 'entropy': 1.5172080770134926, 'num_tokens': 21674.0, 'mean_token_accuracy': 0.8259974308311939, 'epoch': 3.37}
{'loss': 0.8433, 'grad_norm': 2.14060115814209, 'learning_rate': 9.157348061512728e-06, 'entropy': 1.6336551941931248, 'num_tokens': 23385.0, 'mean_token_accuracy': 0.8478846177458763, 'epoch': 3.74}
{'loss': 0.9212, 'grad_norm': 2.058034896850586, 'learning_rate': 8.865052266813686e-06, 'entropy': 1.4199990413405679, 'num_tokens': 25272.0, 'mean_token_accuracy': 0.8263365030288696, 'epoch': 4.0}
{'loss': 0.8844, 'grad_norm': 1.7364070415496826, 'learning_rate': 8.535533905932739e-06, 'entropy': 1.4498042725026608, 'num_tokens': 28245.0, 'mean_token_accuracy': 0.8226729221642017, 'epoch': 4.37}
{'loss': 0.8481, 'grad_norm': 2.15472149848938, 'learning_rate': 8.171966420818227e-06, 'entropy': 1.5781745091080666, 'num_tokens': 30283.0, 'mean_token_accuracy': 0.8271992728114128, 'epoch': 4.74}
{'loss': 0.7505, 'grad_norm': 2.0298705101013184, 'learning_rate': 7.777851165098012e-06, 'entropy': 1.6134657913988286, 'num_tokens': 31590.0, 'mean_token_accuracy': 0.8619325919584795, 'epoch': 5.0}
{'loss': 0.8811, 'grad_norm': 2.3692386150360107, 'learning_rate': 7.3569836841299905e-06, 'entropy': 1.6344665214419365, 'num_tokens': 33360.0, 'mean_token_accuracy': 0.8332529291510582, 'epoch': 5.37}
{'loss': 0.7512, 'grad_norm': 1.50371515750885, 'learning_rate': 6.913417161825449e-06, 'entropy': 1.4162122383713722, 'num_tokens': 36519.0, 'mean_token_accuracy': 0.8497111350297928, 'epoch': 5.74}
{'loss': 0.8112, 'grad_norm': 2.2796852588653564, 'learning_rate': 6.451423386272312e-06, 'entropy': 1.5949289798736572, 'num_tokens': 37908.0, 'mean_token_accuracy': 0.8347415707328103, 'epoch': 6.0}
{'loss': 0.7392, 'grad_norm': 1.7264552116394043, 'learning_rate': 5.975451610080643e-06, 'entropy': 1.482303787022829, 'num_tokens': 40619.0, 'mean_token_accuracy': 0.8378305993974209, 'epoch': 6.37}
{'loss': 0.7803, 'grad_norm': 1.9229851961135864, 'learning_rate': 5.490085701647805e-06, 'entropy': 1.5504726581275463, 'num_tokens': 43042.0, 'mean_token_accuracy': 0.8567813038825989, 'epoch': 6.74}
{'loss': 0.7508, 'grad_norm': 2.195441722869873, 'learning_rate': 5e-06, 'entropy': 1.6319798122752796, 'num_tokens': 44226.0, 'mean_token_accuracy': 0.8426129384474321, 'epoch': 7.0}
{'loss': 0.6646, 'grad_norm': 1.6197736263275146, 'learning_rate': 4.509914298352197e-06, 'entropy': 1.5328516513109207, 'num_tokens': 46461.0, 'mean_token_accuracy': 0.8859817199409008, 'epoch': 7.37}
{'loss': 0.7326, 'grad_norm': 1.5705844163894653, 'learning_rate': 4.02454838991936e-06, 'entropy': 1.5362492203712463, 'num_tokens': 49072.0, 'mean_token_accuracy': 0.849740494042635, 'epoch': 7.74}
{'loss': 0.8089, 'grad_norm': 2.38458251953125, 'learning_rate': 3.5485766137276894e-06, 'entropy': 1.5864295742728494, 'num_tokens': 50544.0, 'mean_token_accuracy': 0.8167909817262129, 'epoch': 8.0}
{'loss': 0.7599, 'grad_norm': 1.8765149116516113, 'learning_rate': 3.0865828381745515e-06, 'entropy': 1.4470697231590748, 'num_tokens': 53147.0, 'mean_token_accuracy': 0.8317571990191936, 'epoch': 8.37}
{'loss': 0.6982, 'grad_norm': 1.7396900653839111, 'learning_rate': 2.6430163158700116e-06, 'entropy': 1.605034712702036, 'num_tokens': 55190.0, 'mean_token_accuracy': 0.8688659891486168, 'epoch': 8.74}
{'loss': 0.6256, 'grad_norm': 1.5876938104629517, 'learning_rate': 2.2221488349019903e-06, 'entropy': 1.6143015189604326, 'num_tokens': 56862.0, 'mean_token_accuracy': 0.8956262848593972, 'epoch': 9.0}
{'loss': 0.6422, 'grad_norm': 1.4940288066864014, 'learning_rate': 1.8280335791817733e-06, 'entropy': 1.5539792589843273, 'num_tokens': 59543.0, 'mean_token_accuracy': 0.8691140189766884, 'epoch': 9.37}
{'loss': 0.7158, 'grad_norm': 1.9709622859954834, 'learning_rate': 1.4644660940672628e-06, 'entropy': 1.5952626392245293, 'num_tokens': 61573.0, 'mean_token_accuracy': 0.8532840460538864, 'epoch': 9.74}
{'loss': 0.7485, 'grad_norm': 1.9154270887374878, 'learning_rate': 1.134947733186315e-06, 'entropy': 1.4754001769152554, 'num_tokens': 63180.0, 'mean_token_accuracy': 0.874210926619443, 'epoch': 10.0}
{'loss': 0.7818, 'grad_norm': 2.022409677505493, 'learning_rate': 8.426519384872733e-07, 'entropy': 1.5506043136119843, 'num_tokens': 65326.0, 'mean_token_accuracy': 0.8673301823437214, 'epoch': 10.37}
{'loss': 0.6153, 'grad_norm': 1.5097999572753906, 'learning_rate': 5.903936782582253e-07, 'entropy': 1.4894626438617706, 'num_tokens': 67813.0, 'mean_token_accuracy': 0.8656345643103123, 'epoch': 10.74}
{'loss': 0.6732, 'grad_norm': 1.5371233224868774, 'learning_rate': 3.8060233744356634e-07, 'entropy': 1.632684350013733, 'num_tokens': 69498.0, 'mean_token_accuracy': 0.8598770986903798, 'epoch': 11.0}
{'loss': 0.7056, 'grad_norm': 1.8609124422073364, 'learning_rate': 2.152983213389559e-07, 'entropy': 1.5971117913722992, 'num_tokens': 71308.0, 'mean_token_accuracy': 0.8598572909832001, 'epoch': 11.37}
{'loss': 0.6851, 'grad_norm': 1.7882188558578491, 'learning_rate': 9.607359798384785e-08, 'entropy': 1.6572042182087898, 'num_tokens': 73415.0, 'mean_token_accuracy': 0.8862172104418278, 'epoch': 11.74}
{'loss': 0.6635, 'grad_norm': 1.6038819551467896, 'learning_rate': 2.4076366639015914e-08, 'entropy': 1.3234422206878662, 'num_tokens': 75816.0, 'mean_token_accuracy': 0.843869539824399, 'epoch': 12.0}
{'train_runtime': 691.0954, 'train_samples_per_second': 0.747, 'train_steps_per_second': 0.052, 'train_loss': 0.817985917131106, 'epoch': 12.0}
モデルを保存中...
Llama 3.2 3B QLoRA学習完了！モデルは ../outputs/llama32-3b-typst-qlora-003-009 に保存されました。
