Llama 3.2 3Bモデル: meta-llama/Llama-3.2-3B-Instruct
データ: ../jsonl/train_003.jsonl
出力: ../outputs/llama32-3b-typst-qlora-003-000
学習率: 1e-05
エポック数: 12
バッチサイズ: 1
勾配累積: 16
LoRA rank: 8
LoRA alpha: 8
LoRA dropout: 0.1
==================================================
トークナイザーを読み込み中...
4bit量子化設定を準備中...
モデルを読み込み中...

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.98s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  2.98s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.28s/it]
既存のPEFTモデルを読み込み中: ../outputs/llama32-3b-typst-qlora-002-010
データセットを読み込み中...

Generating train split: 0 examples [00:00, ? examples/s]
Generating train split: 43 examples [00:00, 2241.66 examples/s]
/mnt/d/github/typst-tex-converter/venv/lib/python3.11/site-packages/peft/tuners/lora/bnb.py:348: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.
  warnings.warn(
/mnt/d/github/typst-tex-converter/venv/lib/python3.11/site-packages/peft/tuners/tuners_utils.py:196: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!
  warnings.warn(
データセットサイズ: 43

Tokenizing train dataset:   0%|          | 0/43 [00:00<?, ? examples/s]
Tokenizing train dataset: 100%|██████████| 43/43 [00:00<00:00, 946.06 examples/s]

Truncating train dataset:   0%|          | 0/43 [00:00<?, ? examples/s]
Truncating train dataset: 100%|██████████| 43/43 [00:00<00:00, 10423.34 examples/s]
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 128009}.
Llama 3.2 3B QLoRA学習を開始します...

  0%|          | 0/36 [00:00<?, ?it/s]
  3%|▎         | 1/36 [00:09<05:24,  9.27s/it]
                                              

  3%|▎         | 1/36 [00:09<05:24,  9.27s/it]
  6%|▌         | 2/36 [00:36<11:10, 19.72s/it]
                                              

  6%|▌         | 2/36 [00:36<11:10, 19.72s/it]
  8%|▊         | 3/36 [00:45<08:11, 14.91s/it]
                                              

  8%|▊         | 3/36 [00:45<08:11, 14.91s/it]
 11%|█         | 4/36 [00:58<07:32, 14.14s/it]
                                              

 11%|█         | 4/36 [00:58<07:32, 14.14s/it]
 14%|█▍        | 5/36 [01:22<09:09, 17.73s/it]
                                              

 14%|█▍        | 5/36 [01:22<09:09, 17.73s/it]
 17%|█▋        | 6/36 [01:39<08:47, 17.58s/it]
                                              

 17%|█▋        | 6/36 [01:39<08:47, 17.58s/it]
 19%|█▉        | 7/36 [01:54<08:06, 16.78s/it]
                                              

 19%|█▉        | 7/36 [01:54<08:06, 16.78s/it]
 22%|██▏       | 8/36 [02:12<08:00, 17.15s/it]
                                              

 22%|██▏       | 8/36 [02:12<08:00, 17.15s/it]
 25%|██▌       | 9/36 [02:33<08:14, 18.31s/it]
                                              

 25%|██▌       | 9/36 [02:33<08:14, 18.31s/it]
 28%|██▊       | 10/36 [02:57<08:39, 19.96s/it]
                                               

 28%|██▊       | 10/36 [02:57<08:39, 19.96s/it]
 31%|███       | 11/36 [03:14<07:53, 18.94s/it]
                                               

 31%|███       | 11/36 [03:14<07:53, 18.94s/it]
 33%|███▎      | 12/36 [03:27<06:56, 17.35s/it]
                                               

 33%|███▎      | 12/36 [03:27<06:56, 17.35s/it]
 36%|███▌      | 13/36 [03:53<07:36, 19.84s/it]
                                               

 36%|███▌      | 13/36 [03:53<07:36, 19.84s/it]
 39%|███▉      | 14/36 [04:09<06:51, 18.72s/it]
                                               

 39%|███▉      | 14/36 [04:09<06:51, 18.72s/it]
 42%|████▏     | 15/36 [04:21<05:52, 16.76s/it]
                                               

 42%|████▏     | 15/36 [04:21<05:52, 16.76s/it]
 44%|████▍     | 16/36 [04:33<05:06, 15.32s/it]
                                               

 44%|████▍     | 16/36 [04:33<05:06, 15.32s/it]
 47%|████▋     | 17/36 [05:06<06:31, 20.59s/it]
                                               

 47%|████▋     | 17/36 [05:06<06:31, 20.59s/it]
 50%|█████     | 18/36 [05:15<05:07, 17.08s/it]
                                               

 50%|█████     | 18/36 [05:15<05:07, 17.08s/it]
 53%|█████▎    | 19/36 [05:37<05:14, 18.47s/it]
                                               

 53%|█████▎    | 19/36 [05:37<05:14, 18.47s/it]
 56%|█████▌    | 20/36 [05:58<05:11, 19.49s/it]
                                               

 56%|█████▌    | 20/36 [05:58<05:11, 19.49s/it]
 58%|█████▊    | 21/36 [06:09<04:09, 16.65s/it]
                                               

 58%|█████▊    | 21/36 [06:09<04:09, 16.65s/it]
 61%|██████    | 22/36 [06:31<04:15, 18.25s/it]
                                               

 61%|██████    | 22/36 [06:31<04:15, 18.25s/it]
 64%|██████▍   | 23/36 [06:54<04:15, 19.67s/it]
                                               

 64%|██████▍   | 23/36 [06:54<04:15, 19.67s/it]
 67%|██████▋   | 24/36 [07:03<03:20, 16.67s/it]
                                               

 67%|██████▋   | 24/36 [07:03<03:20, 16.67s/it]
 69%|██████▉   | 25/36 [07:19<02:59, 16.34s/it]
                                               

 69%|██████▉   | 25/36 [07:19<02:59, 16.34s/it]
 72%|███████▏  | 26/36 [07:37<02:49, 16.99s/it]
                                               

 72%|███████▏  | 26/36 [07:37<02:49, 16.99s/it]
 75%|███████▌  | 27/36 [07:57<02:39, 17.72s/it]
                                               

 75%|███████▌  | 27/36 [07:57<02:39, 17.72s/it]
 78%|███████▊  | 28/36 [08:23<02:43, 20.41s/it]
                                               

 78%|███████▊  | 28/36 [08:23<02:43, 20.41s/it]
 81%|████████  | 29/36 [08:40<02:15, 19.42s/it]
                                               

 81%|████████  | 29/36 [08:40<02:15, 19.42s/it]
 83%|████████▎ | 30/36 [08:50<01:39, 16.59s/it]
                                               

 83%|████████▎ | 30/36 [08:50<01:39, 16.59s/it]
 86%|████████▌ | 31/36 [09:03<01:17, 15.47s/it]
                                               

 86%|████████▌ | 31/36 [09:03<01:17, 15.47s/it]
 89%|████████▉ | 32/36 [09:31<01:16, 19.10s/it]
                                               

 89%|████████▉ | 32/36 [09:31<01:16, 19.10s/it]
 92%|█████████▏| 33/36 [09:49<00:56, 18.82s/it]
                                               

 92%|█████████▏| 33/36 [09:49<00:56, 18.82s/it]
 94%|█████████▍| 34/36 [10:03<00:34, 17.48s/it]
                                               

 94%|█████████▍| 34/36 [10:03<00:34, 17.48s/it]
 97%|█████████▋| 35/36 [10:23<00:18, 18.05s/it]
                                               

 97%|█████████▋| 35/36 [10:23<00:18, 18.05s/it]
100%|██████████| 36/36 [10:48<00:00, 20.32s/it]
                                               

100%|██████████| 36/36 [10:48<00:00, 20.32s/it]
                                               

100%|██████████| 36/36 [10:50<00:00, 20.32s/it]
100%|██████████| 36/36 [10:50<00:00, 18.07s/it]
{'loss': 1.0019, 'grad_norm': 2.33729887008667, 'learning_rate': 0.0, 'entropy': 1.6680215001106262, 'num_tokens': 1968.0, 'mean_token_accuracy': 0.8103914856910706, 'epoch': 0.37}
{'loss': 1.0455, 'grad_norm': 2.3618662357330322, 'learning_rate': 2.5e-06, 'entropy': 1.3886091224849224, 'num_tokens': 5029.0, 'mean_token_accuracy': 0.8093724586069584, 'epoch': 0.74}
{'loss': 1.0963, 'grad_norm': 2.9068922996520996, 'learning_rate': 5e-06, 'entropy': 1.5278394439003684, 'num_tokens': 6318.0, 'mean_token_accuracy': 0.8274629007686268, 'epoch': 1.0}
{'loss': 1.2103, 'grad_norm': 3.174528121948242, 'learning_rate': 7.500000000000001e-06, 'entropy': 1.6622846834361553, 'num_tokens': 8014.0, 'mean_token_accuracy': 0.7986961454153061, 'epoch': 1.37}
{'loss': 1.0115, 'grad_norm': 2.111877918243408, 'learning_rate': 1e-05, 'entropy': 1.3558723703026772, 'num_tokens': 11175.0, 'mean_token_accuracy': 0.8236325532197952, 'epoch': 1.74}
{'loss': 0.8328, 'grad_norm': 2.0602097511291504, 'learning_rate': 9.975923633360985e-06, 'entropy': 1.589465618133545, 'num_tokens': 12636.0, 'mean_token_accuracy': 0.8517454103990034, 'epoch': 2.0}
{'loss': 0.9869, 'grad_norm': 2.254305839538574, 'learning_rate': 9.903926402016153e-06, 'entropy': 1.5217260979115963, 'num_tokens': 14723.0, 'mean_token_accuracy': 0.839100543409586, 'epoch': 2.37}
{'loss': 1.0358, 'grad_norm': 2.704228401184082, 'learning_rate': 9.784701678661045e-06, 'entropy': 1.5121206231415272, 'num_tokens': 17027.0, 'mean_token_accuracy': 0.8221327364444733, 'epoch': 2.74}
{'loss': 0.8291, 'grad_norm': 1.542239785194397, 'learning_rate': 9.619397662556434e-06, 'entropy': 1.5808115059679204, 'num_tokens': 18954.0, 'mean_token_accuracy': 0.8229299729520624, 'epoch': 3.0}
{'loss': 0.9048, 'grad_norm': 1.8527780771255493, 'learning_rate': 9.409606321741776e-06, 'entropy': 1.5184300988912582, 'num_tokens': 21674.0, 'mean_token_accuracy': 0.8259974308311939, 'epoch': 3.37}
{'loss': 0.8402, 'grad_norm': 2.2486648559570312, 'learning_rate': 9.157348061512728e-06, 'entropy': 1.6351203136146069, 'num_tokens': 23385.0, 'mean_token_accuracy': 0.8478846177458763, 'epoch': 3.74}
{'loss': 0.9177, 'grad_norm': 2.167335271835327, 'learning_rate': 8.865052266813686e-06, 'entropy': 1.4211718494241887, 'num_tokens': 25272.0, 'mean_token_accuracy': 0.8264588605273854, 'epoch': 4.0}
{'loss': 0.8806, 'grad_norm': 1.8346835374832153, 'learning_rate': 8.535533905932739e-06, 'entropy': 1.451025765389204, 'num_tokens': 28245.0, 'mean_token_accuracy': 0.8233850933611393, 'epoch': 4.37}
{'loss': 0.8431, 'grad_norm': 2.2779297828674316, 'learning_rate': 8.171966420818227e-06, 'entropy': 1.5798427276313305, 'num_tokens': 30283.0, 'mean_token_accuracy': 0.8271992728114128, 'epoch': 4.74}
{'loss': 0.7474, 'grad_norm': 2.095979928970337, 'learning_rate': 7.777851165098012e-06, 'entropy': 1.614700734615326, 'num_tokens': 31590.0, 'mean_token_accuracy': 0.8613714250651273, 'epoch': 5.0}
{'loss': 0.8752, 'grad_norm': 2.4660189151763916, 'learning_rate': 7.3569836841299905e-06, 'entropy': 1.6358339115977287, 'num_tokens': 33360.0, 'mean_token_accuracy': 0.8332529291510582, 'epoch': 5.37}
{'loss': 0.7472, 'grad_norm': 1.5623337030410767, 'learning_rate': 6.913417161825449e-06, 'entropy': 1.4177619405090809, 'num_tokens': 36519.0, 'mean_token_accuracy': 0.8489533066749573, 'epoch': 5.74}
{'loss': 0.8055, 'grad_norm': 2.358943223953247, 'learning_rate': 6.451423386272312e-06, 'entropy': 1.5964284159920432, 'num_tokens': 37908.0, 'mean_token_accuracy': 0.8347415707328103, 'epoch': 6.0}
{'loss': 0.7342, 'grad_norm': 1.8040869235992432, 'learning_rate': 5.975451610080643e-06, 'entropy': 1.4837151877582073, 'num_tokens': 40619.0, 'mean_token_accuracy': 0.8364202082157135, 'epoch': 6.37}
{'loss': 0.7755, 'grad_norm': 1.983323097229004, 'learning_rate': 5.490085701647805e-06, 'entropy': 1.5521885231137276, 'num_tokens': 43042.0, 'mean_token_accuracy': 0.8567813038825989, 'epoch': 6.74}
{'loss': 0.7467, 'grad_norm': 2.277348518371582, 'learning_rate': 5e-06, 'entropy': 1.6333699930797925, 'num_tokens': 44226.0, 'mean_token_accuracy': 0.8426129384474321, 'epoch': 7.0}
{'loss': 0.6608, 'grad_norm': 1.6800187826156616, 'learning_rate': 4.509914298352197e-06, 'entropy': 1.5339742489159107, 'num_tokens': 46461.0, 'mean_token_accuracy': 0.8864378668367863, 'epoch': 7.37}
{'loss': 0.7277, 'grad_norm': 1.622532606124878, 'learning_rate': 4.02454838991936e-06, 'entropy': 1.5375034101307392, 'num_tokens': 49072.0, 'mean_token_accuracy': 0.8510291539132595, 'epoch': 7.74}
{'loss': 0.8012, 'grad_norm': 2.4038050174713135, 'learning_rate': 3.5485766137276894e-06, 'entropy': 1.5883993994105945, 'num_tokens': 50544.0, 'mean_token_accuracy': 0.8189795342358676, 'epoch': 8.0}
{'loss': 0.7537, 'grad_norm': 1.9258538484573364, 'learning_rate': 3.0865828381745515e-06, 'entropy': 1.4484703429043293, 'num_tokens': 53147.0, 'mean_token_accuracy': 0.8311316259205341, 'epoch': 8.37}
{'loss': 0.6938, 'grad_norm': 1.78440260887146, 'learning_rate': 2.6430163158700116e-06, 'entropy': 1.6063241623342037, 'num_tokens': 55190.0, 'mean_token_accuracy': 0.8694727867841721, 'epoch': 8.74}
{'loss': 0.6217, 'grad_norm': 1.6425154209136963, 'learning_rate': 2.2221488349019903e-06, 'entropy': 1.614839028228413, 'num_tokens': 56862.0, 'mean_token_accuracy': 0.8961980397051031, 'epoch': 9.0}
{'loss': 0.6378, 'grad_norm': 1.5456169843673706, 'learning_rate': 1.8280335791817733e-06, 'entropy': 1.5551363676786423, 'num_tokens': 59543.0, 'mean_token_accuracy': 0.8687282167375088, 'epoch': 9.37}
{'loss': 0.7106, 'grad_norm': 2.0160725116729736, 'learning_rate': 1.4644660940672628e-06, 'entropy': 1.5964531116187572, 'num_tokens': 61573.0, 'mean_token_accuracy': 0.8532840460538864, 'epoch': 9.74}
{'loss': 0.7433, 'grad_norm': 1.9266071319580078, 'learning_rate': 1.134947733186315e-06, 'entropy': 1.47668410431255, 'num_tokens': 63180.0, 'mean_token_accuracy': 0.8758058222857389, 'epoch': 10.0}
{'loss': 0.7747, 'grad_norm': 2.0640347003936768, 'learning_rate': 8.426519384872733e-07, 'entropy': 1.5518285185098648, 'num_tokens': 65326.0, 'mean_token_accuracy': 0.8693882115185261, 'epoch': 10.37}
{'loss': 0.6121, 'grad_norm': 1.5169262886047363, 'learning_rate': 5.903936782582253e-07, 'entropy': 1.490330047905445, 'num_tokens': 67813.0, 'mean_token_accuracy': 0.8656345643103123, 'epoch': 10.74}
{'loss': 0.6699, 'grad_norm': 1.5616534948349, 'learning_rate': 3.8060233744356634e-07, 'entropy': 1.6331220269203186, 'num_tokens': 69498.0, 'mean_token_accuracy': 0.8604073470289056, 'epoch': 11.0}
{'loss': 0.7011, 'grad_norm': 1.9196267127990723, 'learning_rate': 2.152983213389559e-07, 'entropy': 1.5982626155018806, 'num_tokens': 71308.0, 'mean_token_accuracy': 0.8598572909832001, 'epoch': 11.37}
{'loss': 0.6788, 'grad_norm': 1.8335716724395752, 'learning_rate': 9.607359798384785e-08, 'entropy': 1.658377818763256, 'num_tokens': 73415.0, 'mean_token_accuracy': 0.8859284333884716, 'epoch': 11.74}
{'loss': 0.6585, 'grad_norm': 1.6351667642593384, 'learning_rate': 2.4076366639015914e-08, 'entropy': 1.3246531649069353, 'num_tokens': 75816.0, 'mean_token_accuracy': 0.8458458239381964, 'epoch': 12.0}
{'train_runtime': 650.5787, 'train_samples_per_second': 0.793, 'train_steps_per_second': 0.055, 'train_loss': 0.814275249838829, 'epoch': 12.0}
モデルを保存中...
Llama 3.2 3B QLoRA学習完了！モデルは ../outputs/llama32-3b-typst-qlora-003-000 に保存されました。
