Llama 3.2 3Bモデル: meta-llama/Llama-3.2-3B-Instruct
データ: ../jsonl/train_003.jsonl
出力: ../outputs/llama32-3b-typst-qlora-003-003
学習率: 1e-05
エポック数: 12
バッチサイズ: 1
勾配累積: 16
LoRA rank: 8
LoRA alpha: 8
LoRA dropout: 0.1
==================================================
トークナイザーを読み込み中...
4bit量子化設定を準備中...
モデルを読み込み中...

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.90s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.74s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.92s/it]
/mnt/d/github/typst-tex-converter/venv/lib/python3.11/site-packages/peft/tuners/lora/bnb.py:348: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.
  warnings.warn(
/mnt/d/github/typst-tex-converter/venv/lib/python3.11/site-packages/peft/tuners/tuners_utils.py:196: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!
  warnings.warn(
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 128009}.
既存のPEFTモデルを読み込み中: ../outputs/llama32-3b-typst-qlora-003-002
データセットを読み込み中...
データセットサイズ: 43
Llama 3.2 3B QLoRA学習を開始します...

  0%|          | 0/36 [00:00<?, ?it/s]
  3%|▎         | 1/36 [00:10<05:55, 10.17s/it]
                                              

  3%|▎         | 1/36 [00:10<05:55, 10.17s/it]
  6%|▌         | 2/36 [00:39<12:15, 21.63s/it]
                                              

  6%|▌         | 2/36 [00:39<12:15, 21.63s/it]
  8%|▊         | 3/36 [00:49<08:50, 16.08s/it]
                                              

  8%|▊         | 3/36 [00:49<08:50, 16.08s/it]
 11%|█         | 4/36 [01:03<08:05, 15.17s/it]
                                              

 11%|█         | 4/36 [01:03<08:05, 15.17s/it]
 14%|█▍        | 5/36 [01:29<09:57, 19.27s/it]
                                              

 14%|█▍        | 5/36 [01:29<09:57, 19.27s/it]
 17%|█▋        | 6/36 [01:48<09:31, 19.07s/it]
                                              

 17%|█▋        | 6/36 [01:48<09:31, 19.07s/it]
 19%|█▉        | 7/36 [02:04<08:47, 18.19s/it]
                                              

 19%|█▉        | 7/36 [02:04<08:47, 18.19s/it]
 22%|██▏       | 8/36 [02:23<08:36, 18.46s/it]
                                              

 22%|██▏       | 8/36 [02:23<08:36, 18.46s/it]
 25%|██▌       | 9/36 [02:46<08:53, 19.77s/it]
                                              

 25%|██▌       | 9/36 [02:46<08:53, 19.77s/it]
 28%|██▊       | 10/36 [03:11<09:20, 21.57s/it]
                                               

 28%|██▊       | 10/36 [03:11<09:20, 21.57s/it]
 31%|███       | 11/36 [03:29<08:27, 20.31s/it]
                                               

 31%|███       | 11/36 [03:29<08:27, 20.31s/it]
 33%|███▎      | 12/36 [03:44<07:28, 18.70s/it]
                                               

 33%|███▎      | 12/36 [03:44<07:28, 18.70s/it]
 36%|███▌      | 13/36 [04:12<08:14, 21.51s/it]
                                               

 36%|███▌      | 13/36 [04:12<08:14, 21.51s/it]
 39%|███▉      | 14/36 [04:29<07:24, 20.22s/it]
                                               

 39%|███▉      | 14/36 [04:29<07:24, 20.22s/it]
 42%|████▏     | 15/36 [04:43<06:23, 18.28s/it]
                                               

 42%|████▏     | 15/36 [04:43<06:23, 18.28s/it]
 44%|████▍     | 16/36 [04:56<05:33, 16.68s/it]
                                               

 44%|████▍     | 16/36 [04:56<05:33, 16.68s/it]
 47%|████▋     | 17/36 [05:32<07:07, 22.49s/it]
                                               

 47%|████▋     | 17/36 [05:32<07:07, 22.49s/it]
 50%|█████     | 18/36 [05:41<05:34, 18.59s/it]
                                               

 50%|█████     | 18/36 [05:41<05:34, 18.59s/it]
 53%|█████▎    | 19/36 [06:05<05:43, 20.21s/it]
                                               

 53%|█████▎    | 19/36 [06:05<05:43, 20.21s/it]
 56%|█████▌    | 20/36 [06:28<05:37, 21.08s/it]
                                               

 56%|█████▌    | 20/36 [06:28<05:37, 21.08s/it]
 58%|█████▊    | 21/36 [06:39<04:28, 17.91s/it]
                                               

 58%|█████▊    | 21/36 [06:39<04:28, 17.91s/it]
 61%|██████    | 22/36 [07:03<04:34, 19.62s/it]
                                               

 61%|██████    | 22/36 [07:03<04:34, 19.62s/it]
 64%|██████▍   | 23/36 [07:27<04:35, 21.17s/it]
                                               

 64%|██████▍   | 23/36 [07:27<04:35, 21.17s/it]
 67%|██████▋   | 24/36 [07:38<03:34, 17.90s/it]
                                               

 67%|██████▋   | 24/36 [07:38<03:34, 17.90s/it]
 69%|██████▉   | 25/36 [07:54<03:13, 17.56s/it]
                                               

 69%|██████▉   | 25/36 [07:54<03:13, 17.56s/it]
 72%|███████▏  | 26/36 [08:14<03:02, 18.29s/it]
                                               

 72%|███████▏  | 26/36 [08:14<03:02, 18.29s/it]
 75%|███████▌  | 27/36 [08:36<02:53, 19.33s/it]
                                               

 75%|███████▌  | 27/36 [08:36<02:53, 19.33s/it]
 78%|███████▊  | 28/36 [09:05<02:57, 22.18s/it]
                                               

 78%|███████▊  | 28/36 [09:05<02:57, 22.18s/it]
 81%|████████  | 29/36 [09:23<02:27, 21.07s/it]
                                               

 81%|████████  | 29/36 [09:23<02:27, 21.07s/it]
 83%|████████▎ | 30/36 [09:34<01:47, 17.87s/it]
                                               

 83%|████████▎ | 30/36 [09:34<01:47, 17.87s/it]
 86%|████████▌ | 31/36 [09:48<01:22, 16.60s/it]
                                               

 86%|████████▌ | 31/36 [09:48<01:22, 16.60s/it]
 89%|████████▉ | 32/36 [10:15<01:19, 19.77s/it]
                                               

 89%|████████▉ | 32/36 [10:15<01:19, 19.77s/it]
 92%|█████████▏| 33/36 [10:33<00:57, 19.22s/it]
                                               

 92%|█████████▏| 33/36 [10:33<00:57, 19.22s/it]
 94%|█████████▍| 34/36 [10:47<00:35, 17.64s/it]
                                               

 94%|█████████▍| 34/36 [10:47<00:35, 17.64s/it]
 97%|█████████▋| 35/36 [11:06<00:18, 18.17s/it]
                                               

 97%|█████████▋| 35/36 [11:06<00:18, 18.17s/it]
100%|██████████| 36/36 [11:31<00:00, 20.17s/it]
                                               

100%|██████████| 36/36 [11:31<00:00, 20.17s/it]
                                               

100%|██████████| 36/36 [11:32<00:00, 20.17s/it]
100%|██████████| 36/36 [11:32<00:00, 19.25s/it]
{'loss': 1.0018, 'grad_norm': 2.1563456058502197, 'learning_rate': 0.0, 'entropy': 1.6680224612355232, 'num_tokens': 1968.0, 'mean_token_accuracy': 0.8097846880555153, 'epoch': 0.37}
{'loss': 1.0452, 'grad_norm': 2.1949589252471924, 'learning_rate': 2.5e-06, 'entropy': 1.3885260112583637, 'num_tokens': 5029.0, 'mean_token_accuracy': 0.8093724586069584, 'epoch': 0.74}
{'loss': 1.0964, 'grad_norm': 2.7553718090057373, 'learning_rate': 5e-06, 'entropy': 1.5278240008787676, 'num_tokens': 6318.0, 'mean_token_accuracy': 0.8274629007686268, 'epoch': 1.0}
{'loss': 1.2107, 'grad_norm': 2.9516971111297607, 'learning_rate': 7.500000000000001e-06, 'entropy': 1.6624053791165352, 'num_tokens': 8014.0, 'mean_token_accuracy': 0.7986961454153061, 'epoch': 1.37}
{'loss': 1.012, 'grad_norm': 1.971950650215149, 'learning_rate': 1e-05, 'entropy': 1.3559046015143394, 'num_tokens': 11175.0, 'mean_token_accuracy': 0.8236325532197952, 'epoch': 1.74}
{'loss': 0.833, 'grad_norm': 1.9431785345077515, 'learning_rate': 9.975923633360985e-06, 'entropy': 1.5893222852186724, 'num_tokens': 12636.0, 'mean_token_accuracy': 0.8517454103990034, 'epoch': 2.0}
{'loss': 0.9877, 'grad_norm': 2.0938944816589355, 'learning_rate': 9.903926402016153e-06, 'entropy': 1.5216482691466808, 'num_tokens': 14723.0, 'mean_token_accuracy': 0.8397073410451412, 'epoch': 2.37}
{'loss': 1.0368, 'grad_norm': 2.5414388179779053, 'learning_rate': 9.784701678661045e-06, 'entropy': 1.5122089609503746, 'num_tokens': 17027.0, 'mean_token_accuracy': 0.8221327364444733, 'epoch': 2.74}
{'loss': 0.8302, 'grad_norm': 1.445448875427246, 'learning_rate': 9.619397662556434e-06, 'entropy': 1.5807216600938276, 'num_tokens': 18954.0, 'mean_token_accuracy': 0.8229299729520624, 'epoch': 3.0}
{'loss': 0.9056, 'grad_norm': 1.7889926433563232, 'learning_rate': 9.409606321741776e-06, 'entropy': 1.518686842173338, 'num_tokens': 21674.0, 'mean_token_accuracy': 0.8265761323273182, 'epoch': 3.37}
{'loss': 0.8424, 'grad_norm': 2.1134519577026367, 'learning_rate': 9.157348061512728e-06, 'entropy': 1.635350275784731, 'num_tokens': 23385.0, 'mean_token_accuracy': 0.8478846177458763, 'epoch': 3.74}
{'loss': 0.9195, 'grad_norm': 2.045271158218384, 'learning_rate': 8.865052266813686e-06, 'entropy': 1.421386567029086, 'num_tokens': 25272.0, 'mean_token_accuracy': 0.8264588605273854, 'epoch': 4.0}
{'loss': 0.8819, 'grad_norm': 1.7347538471221924, 'learning_rate': 8.535533905932739e-06, 'entropy': 1.4514076076447964, 'num_tokens': 28245.0, 'mean_token_accuracy': 0.8233850933611393, 'epoch': 4.37}
{'loss': 0.845, 'grad_norm': 2.1473772525787354, 'learning_rate': 8.171966420818227e-06, 'entropy': 1.5802484937012196, 'num_tokens': 30283.0, 'mean_token_accuracy': 0.8271992728114128, 'epoch': 4.74}
{'loss': 0.7488, 'grad_norm': 2.0063605308532715, 'learning_rate': 7.777851165098012e-06, 'entropy': 1.6150829954580828, 'num_tokens': 31590.0, 'mean_token_accuracy': 0.8604888103225015, 'epoch': 5.0}
{'loss': 0.8764, 'grad_norm': 2.37146258354187, 'learning_rate': 7.3569836841299905e-06, 'entropy': 1.6365853771567345, 'num_tokens': 33360.0, 'mean_token_accuracy': 0.8228362612426281, 'epoch': 5.37}
{'loss': 0.7497, 'grad_norm': 1.486633062362671, 'learning_rate': 6.913417161825449e-06, 'entropy': 1.4178225621581078, 'num_tokens': 36519.0, 'mean_token_accuracy': 0.849718414247036, 'epoch': 5.74}
{'loss': 0.8082, 'grad_norm': 2.246094226837158, 'learning_rate': 6.451423386272312e-06, 'entropy': 1.5968497558073564, 'num_tokens': 37908.0, 'mean_token_accuracy': 0.8347415707328103, 'epoch': 6.0}
{'loss': 0.7367, 'grad_norm': 1.720947265625, 'learning_rate': 5.975451610080643e-06, 'entropy': 1.4841836728155613, 'num_tokens': 40619.0, 'mean_token_accuracy': 0.8374447971582413, 'epoch': 6.37}
{'loss': 0.7773, 'grad_norm': 1.890011191368103, 'learning_rate': 5.490085701647805e-06, 'entropy': 1.5525790825486183, 'num_tokens': 43042.0, 'mean_token_accuracy': 0.8597717359662056, 'epoch': 6.74}
{'loss': 0.7487, 'grad_norm': 2.145531415939331, 'learning_rate': 5e-06, 'entropy': 1.6340060775930232, 'num_tokens': 44226.0, 'mean_token_accuracy': 0.8432964628392999, 'epoch': 7.0}
{'loss': 0.6627, 'grad_norm': 1.5966066122055054, 'learning_rate': 4.509914298352197e-06, 'entropy': 1.5348067693412304, 'num_tokens': 46461.0, 'mean_token_accuracy': 0.879493422806263, 'epoch': 7.37}
{'loss': 0.7299, 'grad_norm': 1.5489555597305298, 'learning_rate': 4.02454838991936e-06, 'entropy': 1.538487683981657, 'num_tokens': 49072.0, 'mean_token_accuracy': 0.8503848239779472, 'epoch': 7.74}
{'loss': 0.8035, 'grad_norm': 2.3359997272491455, 'learning_rate': 3.5485766137276894e-06, 'entropy': 1.5889180573550137, 'num_tokens': 50544.0, 'mean_token_accuracy': 0.8189795342358676, 'epoch': 8.0}
{'loss': 0.7561, 'grad_norm': 1.8403332233428955, 'learning_rate': 3.0865828381745515e-06, 'entropy': 1.4491279311478138, 'num_tokens': 53147.0, 'mean_token_accuracy': 0.8304522782564163, 'epoch': 8.37}
{'loss': 0.695, 'grad_norm': 1.7165122032165527, 'learning_rate': 2.6430163158700116e-06, 'entropy': 1.6078190356492996, 'num_tokens': 55190.0, 'mean_token_accuracy': 0.8688659891486168, 'epoch': 8.74}
{'loss': 0.6236, 'grad_norm': 1.5610132217407227, 'learning_rate': 2.2221488349019903e-06, 'entropy': 1.6159252253445713, 'num_tokens': 56862.0, 'mean_token_accuracy': 0.8961980397051031, 'epoch': 9.0}
{'loss': 0.6405, 'grad_norm': 1.4676858186721802, 'learning_rate': 1.8280335791817733e-06, 'entropy': 1.5563626885414124, 'num_tokens': 59543.0, 'mean_token_accuracy': 0.8657520264387131, 'epoch': 9.37}
{'loss': 0.7131, 'grad_norm': 1.923163652420044, 'learning_rate': 1.4644660940672628e-06, 'entropy': 1.5974002033472061, 'num_tokens': 61573.0, 'mean_token_accuracy': 0.8532840460538864, 'epoch': 9.74}
{'loss': 0.7448, 'grad_norm': 1.8761032819747925, 'learning_rate': 1.134947733186315e-06, 'entropy': 1.4774288860234348, 'num_tokens': 63180.0, 'mean_token_accuracy': 0.8758058222857389, 'epoch': 10.0}
{'loss': 0.7775, 'grad_norm': 1.9845523834228516, 'learning_rate': 8.426519384872733e-07, 'entropy': 1.5526240691542625, 'num_tokens': 65326.0, 'mean_token_accuracy': 0.8684266731142998, 'epoch': 10.37}
{'loss': 0.6134, 'grad_norm': 1.4746742248535156, 'learning_rate': 5.903936782582253e-07, 'entropy': 1.4914328195154667, 'num_tokens': 67813.0, 'mean_token_accuracy': 0.8663139119744301, 'epoch': 10.74}
{'loss': 0.671, 'grad_norm': 1.5200966596603394, 'learning_rate': 3.8060233744356634e-07, 'entropy': 1.6347410082817078, 'num_tokens': 69498.0, 'mean_token_accuracy': 0.8604073470289056, 'epoch': 11.0}
{'loss': 0.7028, 'grad_norm': 1.8244191408157349, 'learning_rate': 2.152983213389559e-07, 'entropy': 1.5995829477906227, 'num_tokens': 71308.0, 'mean_token_accuracy': 0.8598572909832001, 'epoch': 11.37}
{'loss': 0.6819, 'grad_norm': 1.758914589881897, 'learning_rate': 9.607359798384785e-08, 'entropy': 1.6597285531461239, 'num_tokens': 73415.0, 'mean_token_accuracy': 0.8855736888945103, 'epoch': 11.74}
{'loss': 0.6609, 'grad_norm': 1.5674716234207153, 'learning_rate': 2.4076366639015914e-08, 'entropy': 1.3253102573481472, 'num_tokens': 75816.0, 'mean_token_accuracy': 0.8405286778103221, 'epoch': 12.0}
{'train_runtime': 692.959, 'train_samples_per_second': 0.745, 'train_steps_per_second': 0.052, 'train_loss': 0.8158487098084556, 'epoch': 12.0}
モデルを保存中...
Llama 3.2 3B QLoRA学習完了！モデルは ../outputs/llama32-3b-typst-qlora-003-003 に保存されました。
