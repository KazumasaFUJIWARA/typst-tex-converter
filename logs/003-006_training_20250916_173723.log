Llama 3.2 3Bモデル: meta-llama/Llama-3.2-3B-Instruct
データ: ../jsonl/train_003.jsonl
出力: ../outputs/llama32-3b-typst-qlora-003-006
学習率: 1e-05
エポック数: 1
バッチサイズ: 1
勾配累積: 16
LoRA rank: 8
LoRA alpha: 8
LoRA dropout: 0.1
==================================================
トークナイザーを読み込み中...
4bit量子化設定を準備中...
モデルを読み込み中...

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.12s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  1.81s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.01s/it]
/mnt/d/github/typst-tex-converter/venv/lib/python3.11/site-packages/peft/tuners/lora/bnb.py:348: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.
  warnings.warn(
/mnt/d/github/typst-tex-converter/venv/lib/python3.11/site-packages/peft/tuners/tuners_utils.py:196: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!
  warnings.warn(
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 128009}.
既存のPEFTモデルを読み込み中: ../outputs/llama32-3b-typst-qlora-003-005
データセットを読み込み中...
データセットサイズ: 43
Llama 3.2 3B QLoRA学習を開始します...

  0%|          | 0/3 [00:00<?, ?it/s]
 33%|███▎      | 1/3 [00:10<00:20, 10.37s/it]
                                             

 33%|███▎      | 1/3 [00:10<00:20, 10.37s/it]
 67%|██████▋   | 2/3 [00:27<00:14, 14.20s/it]
                                             

 67%|██████▋   | 2/3 [00:27<00:14, 14.20s/it]
100%|██████████| 3/3 [00:36<00:00, 11.95s/it]
                                             

100%|██████████| 3/3 [00:36<00:00, 11.95s/it]
                                             

100%|██████████| 3/3 [00:38<00:00, 11.95s/it]
100%|██████████| 3/3 [00:38<00:00, 12.72s/it]
{'loss': 0.7288, 'grad_norm': 2.327806234359741, 'learning_rate': 0.0, 'entropy': 1.658898327499628, 'num_tokens': 1932.0, 'mean_token_accuracy': 0.8163985572755337, 'epoch': 0.37}
{'loss': 0.5999, 'grad_norm': 2.039000988006592, 'learning_rate': 1e-05, 'entropy': 1.328676514327526, 'num_tokens': 5033.0, 'mean_token_accuracy': 0.8601067401468754, 'epoch': 0.74}
{'loss': 0.958, 'grad_norm': 2.770956516265869, 'learning_rate': 5e-06, 'entropy': 1.5224171389232983, 'num_tokens': 6328.0, 'mean_token_accuracy': 0.8398954976688732, 'epoch': 1.0}
{'train_runtime': 38.1682, 'train_samples_per_second': 1.127, 'train_steps_per_second': 0.079, 'train_loss': 0.7622076272964478, 'epoch': 1.0}
モデルを保存中...
Llama 3.2 3B QLoRA学習完了！モデルは ../outputs/llama32-3b-typst-qlora-003-006 に保存されました。
