Llama 3.2 3Bモデル: meta-llama/Llama-3.2-3B-Instruct
データ: ../jsonl/train_003.jsonl
出力: ../outputs/llama32-3b-typst-qlora-003-000
学習率: 1e-05
エポック数: 8
バッチサイズ: 1
勾配累積: 16
LoRA rank: 8
LoRA alpha: 8
LoRA dropout: 0.1
==================================================
トークナイザーを読み込み中...
4bit量子化設定を準備中...
モデルを読み込み中...

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.89s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  2.97s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.26s/it]
既存のPEFTモデルを読み込み中: ../outputs/llama32-3b-typst-qlora-002-010
データセットを読み込み中...

Generating train split: 0 examples [00:00, ? examples/s]
Generating train split: 43 examples [00:00, 2861.46 examples/s]
/mnt/d/github/typst-tex-converter/venv/lib/python3.11/site-packages/peft/tuners/lora/bnb.py:348: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.
  warnings.warn(
/mnt/d/github/typst-tex-converter/venv/lib/python3.11/site-packages/peft/tuners/tuners_utils.py:196: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!
  warnings.warn(
データセットサイズ: 43

Tokenizing train dataset:   0%|          | 0/43 [00:00<?, ? examples/s]
Tokenizing train dataset: 100%|██████████| 43/43 [00:00<00:00, 737.45 examples/s]

Truncating train dataset:   0%|          | 0/43 [00:00<?, ? examples/s]
Truncating train dataset: 100%|██████████| 43/43 [00:00<00:00, 10969.84 examples/s]
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 128009}.
Llama 3.2 3B QLoRA学習を開始します...

  0%|          | 0/24 [00:00<?, ?it/s]
  4%|▍         | 1/24 [00:09<03:34,  9.33s/it]
                                              

  4%|▍         | 1/24 [00:09<03:34,  9.33s/it]
  8%|▊         | 2/24 [00:21<04:07, 11.25s/it]
                                              

  8%|▊         | 2/24 [00:21<04:07, 11.25s/it]
 12%|█▎        | 3/24 [00:28<03:13,  9.21s/it]
                                              

 12%|█▎        | 3/24 [00:28<03:13,  9.21s/it]
 17%|█▋        | 4/24 [00:38<03:10,  9.51s/it]
                                              

 17%|█▋        | 4/24 [00:38<03:10,  9.51s/it]
 21%|██        | 5/24 [00:55<03:49, 12.08s/it]
                                              

 21%|██        | 5/24 [00:55<03:49, 12.08s/it]
 25%|██▌       | 6/24 [01:03<03:11, 10.66s/it]
                                              

 25%|██▌       | 6/24 [01:03<03:11, 10.66s/it]
 29%|██▉       | 7/24 [01:13<02:59, 10.56s/it]
                                              

 29%|██▉       | 7/24 [01:13<02:59, 10.56s/it]
 33%|███▎      | 8/24 [01:25<02:55, 10.94s/it]
                                              

 33%|███▎      | 8/24 [01:25<02:55, 10.94s/it]
 38%|███▊      | 9/24 [01:40<03:03, 12.21s/it]
                                              

 38%|███▊      | 9/24 [01:40<03:03, 12.21s/it]
 42%|████▏     | 10/24 [01:58<03:17, 14.12s/it]
                                               

 42%|████▏     | 10/24 [01:58<03:17, 14.12s/it]
 46%|████▌     | 11/24 [02:09<02:50, 13.10s/it]
                                               

 46%|████▌     | 11/24 [02:09<02:50, 13.10s/it]
 50%|█████     | 12/24 [02:18<02:21, 11.79s/it]
                                               

 50%|█████     | 12/24 [02:18<02:21, 11.79s/it]
 54%|█████▍    | 13/24 [02:36<02:31, 13.80s/it]
 54%|█████▍    | 13/24 [02:36<02:31, 13.80s/it]
                                               

 62%|██████▎   | 15/24 [02:56<01:43, 11.54s/it]
                                               

 62%|██████▎   | 15/24 [02:56<01:43, 11.54s/it]
 67%|██████▋   | 16/24 [03:06<01:28, 11.06s/it]
                                               

 67%|██████▋   | 16/24 [03:06<01:28, 11.06s/it]
 71%|███████   | 17/24 [03:25<01:35, 13.70s/it]
                                               

 71%|███████   | 17/24 [03:25<01:35, 13.70s/it]
 75%|███████▌  | 18/24 [03:33<01:11, 11.97s/it]
                                               

 75%|███████▌  | 18/24 [03:33<01:11, 11.97s/it]
 79%|███████▉  | 19/24 [03:50<01:07, 13.53s/it]
                                               

 79%|███████▉  | 19/24 [03:50<01:07, 13.53s/it]
 83%|████████▎ | 20/24 [04:04<00:53, 13.39s/it]
                                               

 83%|████████▎ | 20/24 [04:04<00:53, 13.39s/it]
 88%|████████▊ | 21/24 [04:11<00:34, 11.66s/it]
                                               

 88%|████████▊ | 21/24 [04:11<00:34, 11.66s/it]
 92%|█████████▏| 22/24 [04:23<00:23, 11.65s/it]
                                               

 92%|█████████▏| 22/24 [04:23<00:23, 11.65s/it]
 96%|█████████▌| 23/24 [04:41<00:13, 13.69s/it]
                                               

 96%|█████████▌| 23/24 [04:41<00:13, 13.69s/it]
100%|██████████| 24/24 [04:49<00:00, 11.89s/it]
                                               

100%|██████████| 24/24 [04:49<00:00, 11.89s/it]
                                               

100%|██████████| 24/24 [04:51<00:00, 11.89s/it]
100%|██████████| 24/24 [04:51<00:00, 12.13s/it]
{'loss': 0.8692, 'grad_norm': 2.1862120628356934, 'learning_rate': 0.0, 'entropy': 1.66405912861228, 'num_tokens': 1931.0, 'mean_token_accuracy': 0.8166080676019192, 'epoch': 0.37}
{'loss': 0.7162, 'grad_norm': 1.8621069192886353, 'learning_rate': 3.3333333333333333e-06, 'entropy': 1.3603801243007183, 'num_tokens': 4979.0, 'mean_token_accuracy': 0.839089784771204, 'epoch': 0.74}
{'loss': 0.9717, 'grad_norm': 2.746840476989746, 'learning_rate': 6.666666666666667e-06, 'entropy': 1.5216717774217778, 'num_tokens': 6274.0, 'mean_token_accuracy': 0.8372980952262878, 'epoch': 1.0}
{'loss': 0.8728, 'grad_norm': 2.7863845825195312, 'learning_rate': 1e-05, 'entropy': 1.648754846304655, 'num_tokens': 7986.0, 'mean_token_accuracy': 0.8078477643430233, 'epoch': 1.37}
{'loss': 0.823, 'grad_norm': 1.8142019510269165, 'learning_rate': 9.944154131125643e-06, 'entropy': 1.3402169942855835, 'num_tokens': 11109.0, 'mean_token_accuracy': 0.8466840200126171, 'epoch': 1.74}
{'loss': 0.5602, 'grad_norm': 1.4890062808990479, 'learning_rate': 9.777864028930705e-06, 'entropy': 1.5806122097102078, 'num_tokens': 12548.0, 'mean_token_accuracy': 0.8703107617118142, 'epoch': 2.0}
{'loss': 0.8624, 'grad_norm': 1.9683449268341064, 'learning_rate': 9.504844339512096e-06, 'entropy': 1.5231489837169647, 'num_tokens': 14567.0, 'mean_token_accuracy': 0.8438084833323956, 'epoch': 2.37}
{'loss': 0.7389, 'grad_norm': 2.327601432800293, 'learning_rate': 9.131193871579975e-06, 'entropy': 1.4936347417533398, 'num_tokens': 16850.0, 'mean_token_accuracy': 0.8552906177937984, 'epoch': 2.74}
{'loss': 0.5667, 'grad_norm': 1.1441998481750488, 'learning_rate': 8.665259359149132e-06, 'entropy': 1.5556881373578852, 'num_tokens': 18822.0, 'mean_token_accuracy': 0.8408078063618053, 'epoch': 3.0}
{'loss': 0.7413, 'grad_norm': 1.5963994264602661, 'learning_rate': 8.117449009293668e-06, 'entropy': 1.5090272165834904, 'num_tokens': 21518.0, 'mean_token_accuracy': 0.8415696173906326, 'epoch': 3.37}
{'loss': 0.6265, 'grad_norm': 1.9487519264221191, 'learning_rate': 7.500000000000001e-06, 'entropy': 1.6322349607944489, 'num_tokens': 23215.0, 'mean_token_accuracy': 0.8565590307116508, 'epoch': 3.74}
{'loss': 0.6063, 'grad_norm': 1.5342655181884766, 'learning_rate': 6.8267051218319766e-06, 'entropy': 1.3893965536897832, 'num_tokens': 25096.0, 'mean_token_accuracy': 0.8524696556004611, 'epoch': 4.0}
{'loss': 0.6531, 'grad_norm': 1.4991215467453003, 'learning_rate': 6.112604669781572e-06, 'entropy': 1.4310537278652191, 'num_tokens': 28086.0, 'mean_token_accuracy': 0.841074712574482, 'epoch': 4.37}
{'loss': 0.5965, 'grad_norm': 1.8693363666534424, 'learning_rate': 5.373650467932122e-06, 'entropy': 1.566716741770506, 'num_tokens': 30094.0, 'mean_token_accuracy': 0.849822610616684, 'epoch': 4.74}
{'loss': 0.6423, 'grad_norm': 1.8990185260772705, 'learning_rate': 4.626349532067879e-06, 'entropy': 1.6115865978327664, 'num_tokens': 31370.0, 'mean_token_accuracy': 0.8640002391555093, 'epoch': 5.0}
{'loss': 0.7406, 'grad_norm': 2.1061644554138184, 'learning_rate': 3.887395330218429e-06, 'entropy': 1.6258467361330986, 'num_tokens': 33121.0, 'mean_token_accuracy': 0.8428926356136799, 'epoch': 5.37}
{'loss': 0.5483, 'grad_norm': 1.1281330585479736, 'learning_rate': 3.173294878168025e-06, 'entropy': 1.393462896347046, 'num_tokens': 36291.0, 'mean_token_accuracy': 0.8702456392347813, 'epoch': 5.74}
{'loss': 0.6134, 'grad_norm': 2.490050792694092, 'learning_rate': 2.5000000000000015e-06, 'entropy': 1.5871723727746443, 'num_tokens': 37644.0, 'mean_token_accuracy': 0.8485838120633905, 'epoch': 6.0}
{'loss': 0.6777, 'grad_norm': 1.5602436065673828, 'learning_rate': 1.8825509907063328e-06, 'entropy': 1.468178540468216, 'num_tokens': 40341.0, 'mean_token_accuracy': 0.847036574035883, 'epoch': 6.37}
{'loss': 0.5455, 'grad_norm': 1.653025507926941, 'learning_rate': 1.3347406408508695e-06, 'entropy': 1.5358333624899387, 'num_tokens': 42695.0, 'mean_token_accuracy': 0.8589903153479099, 'epoch': 6.74}
{'loss': 0.4783, 'grad_norm': 1.6978806257247925, 'learning_rate': 8.688061284200266e-07, 'entropy': 1.6126861572265625, 'num_tokens': 43918.0, 'mean_token_accuracy': 0.8605035922744058, 'epoch': 7.0}
{'loss': 0.446, 'grad_norm': 1.2672555446624756, 'learning_rate': 4.951556604879049e-07, 'entropy': 1.5062026046216488, 'num_tokens': 46190.0, 'mean_token_accuracy': 0.8864259645342827, 'epoch': 7.37}
{'loss': 0.6903, 'grad_norm': 1.402605652809143, 'learning_rate': 2.2213597106929608e-07, 'entropy': 1.5311267673969269, 'num_tokens': 48753.0, 'mean_token_accuracy': 0.8424062095582485, 'epoch': 7.74}
{'loss': 0.652, 'grad_norm': 2.698387622833252, 'learning_rate': 5.584586887435739e-08, 'entropy': 1.5644095865162937, 'num_tokens': 50192.0, 'mean_token_accuracy': 0.8311085430058566, 'epoch': 8.0}
{'train_runtime': 291.0364, 'train_samples_per_second': 1.182, 'train_steps_per_second': 0.082, 'train_loss': 0.6766363581021627, 'epoch': 8.0}
モデルを保存中...
Llama 3.2 3B QLoRA学習完了！モデルは ../outputs/llama32-3b-typst-qlora-003-000 に保存されました。
