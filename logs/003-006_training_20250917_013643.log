Llama 3.2 3Bモデル: meta-llama/Llama-3.2-3B-Instruct
データ: ../jsonl/train_003.jsonl
出力: ../outputs/llama32-3b-typst-qlora-003-006
学習率: 1e-05
エポック数: 12
バッチサイズ: 1
勾配累積: 16
LoRA rank: 8
LoRA alpha: 8
LoRA dropout: 0.1
==================================================
トークナイザーを読み込み中...
4bit量子化設定を準備中...
モデルを読み込み中...

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.80s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.68s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.85s/it]
/mnt/d/github/typst-tex-converter/venv/lib/python3.11/site-packages/peft/tuners/lora/bnb.py:348: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.
  warnings.warn(
/mnt/d/github/typst-tex-converter/venv/lib/python3.11/site-packages/peft/tuners/tuners_utils.py:196: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!
  warnings.warn(
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 128009}.
既存のPEFTモデルを読み込み中: ../outputs/llama32-3b-typst-qlora-003-005
データセットを読み込み中...
データセットサイズ: 43
Llama 3.2 3B QLoRA学習を開始します...

  0%|          | 0/36 [00:00<?, ?it/s]
  3%|▎         | 1/36 [00:09<05:37,  9.63s/it]
                                              

  3%|▎         | 1/36 [00:09<05:37,  9.63s/it]
  6%|▌         | 2/36 [00:36<11:18, 19.94s/it]
                                              

  6%|▌         | 2/36 [00:36<11:18, 19.94s/it]
  8%|▊         | 3/36 [00:45<08:15, 15.02s/it]
                                              

  8%|▊         | 3/36 [00:45<08:15, 15.02s/it]
 11%|█         | 4/36 [00:59<07:38, 14.33s/it]
                                              

 11%|█         | 4/36 [00:59<07:38, 14.33s/it]
 14%|█▍        | 5/36 [01:23<09:16, 17.94s/it]
                                              

 14%|█▍        | 5/36 [01:23<09:16, 17.94s/it]
 17%|█▋        | 6/36 [01:41<08:54, 17.81s/it]
                                              

 17%|█▋        | 6/36 [01:41<08:54, 17.81s/it]
 19%|█▉        | 7/36 [01:56<08:16, 17.11s/it]
                                              

 19%|█▉        | 7/36 [01:56<08:16, 17.11s/it]
 22%|██▏       | 8/36 [02:15<08:11, 17.57s/it]
                                              

 22%|██▏       | 8/36 [02:15<08:11, 17.57s/it]
 25%|██▌       | 9/36 [02:36<08:23, 18.64s/it]
                                              

 25%|██▌       | 9/36 [02:36<08:23, 18.64s/it]
 28%|██▊       | 10/36 [03:00<08:46, 20.24s/it]
                                               

 28%|██▊       | 10/36 [03:00<08:46, 20.24s/it]
 31%|███       | 11/36 [03:16<07:58, 19.13s/it]
                                               

 31%|███       | 11/36 [03:16<07:58, 19.13s/it]
 33%|███▎      | 12/36 [03:30<07:03, 17.63s/it]
                                               

 33%|███▎      | 12/36 [03:30<07:03, 17.63s/it]
 36%|███▌      | 13/36 [03:56<07:43, 20.16s/it]
                                               

 36%|███▌      | 13/36 [03:56<07:43, 20.16s/it]
 39%|███▉      | 14/36 [04:13<07:00, 19.09s/it]
                                               

 39%|███▉      | 14/36 [04:13<07:00, 19.09s/it]
 42%|████▏     | 15/36 [04:26<05:59, 17.10s/it]
                                               

 42%|████▏     | 15/36 [04:26<05:59, 17.10s/it]
 44%|████▍     | 16/36 [04:38<05:12, 15.63s/it]
                                               

 44%|████▍     | 16/36 [04:38<05:12, 15.63s/it]
 47%|████▋     | 17/36 [05:11<06:38, 20.95s/it]
                                               

 47%|████▋     | 17/36 [05:11<06:38, 20.95s/it]
 50%|█████     | 18/36 [05:20<05:14, 17.48s/it]
                                               

 50%|█████     | 18/36 [05:20<05:14, 17.48s/it]
 53%|█████▎    | 19/36 [05:43<05:21, 18.92s/it]
                                               

 53%|█████▎    | 19/36 [05:43<05:21, 18.92s/it]
 56%|█████▌    | 20/36 [06:04<05:14, 19.68s/it]
                                               

 56%|█████▌    | 20/36 [06:04<05:14, 19.68s/it]
 58%|█████▊    | 21/36 [06:15<04:13, 16.92s/it]
                                               

 58%|█████▊    | 21/36 [06:15<04:13, 16.92s/it]
 61%|██████    | 22/36 [06:37<04:21, 18.67s/it]
                                               

 61%|██████    | 22/36 [06:37<04:21, 18.67s/it]
 64%|██████▍   | 23/36 [07:01<04:19, 19.98s/it]
                                               

 64%|██████▍   | 23/36 [07:01<04:19, 19.98s/it]
 67%|██████▋   | 24/36 [07:11<03:23, 16.99s/it]
                                               

 67%|██████▋   | 24/36 [07:11<03:23, 16.99s/it]
 69%|██████▉   | 25/36 [07:26<03:03, 16.64s/it]
                                               

 69%|██████▉   | 25/36 [07:26<03:03, 16.64s/it]
 72%|███████▏  | 26/36 [07:45<02:53, 17.32s/it]
                                               

 72%|███████▏  | 26/36 [07:45<02:53, 17.32s/it]
 75%|███████▌  | 27/36 [08:05<02:42, 18.05s/it]
                                               

 75%|███████▌  | 27/36 [08:05<02:42, 18.05s/it]
 78%|███████▊  | 28/36 [08:32<02:45, 20.68s/it]
                                               

 78%|███████▊  | 28/36 [08:32<02:45, 20.68s/it]
 81%|████████  | 29/36 [08:49<02:17, 19.70s/it]
                                               

 81%|████████  | 29/36 [08:49<02:17, 19.70s/it]
 83%|████████▎ | 30/36 [08:59<01:41, 16.84s/it]
                                               

 83%|████████▎ | 30/36 [08:59<01:41, 16.84s/it]
 86%|████████▌ | 31/36 [09:13<01:18, 15.77s/it]
                                               

 86%|████████▌ | 31/36 [09:13<01:18, 15.77s/it]
 89%|████████▉ | 32/36 [09:38<01:14, 18.58s/it]
                                               

 89%|████████▉ | 32/36 [09:38<01:14, 18.58s/it]
 92%|█████████▏| 33/36 [09:55<00:54, 18.09s/it]
                                               

 92%|█████████▏| 33/36 [09:55<00:54, 18.09s/it]
 94%|█████████▍| 34/36 [10:08<00:33, 16.67s/it]
                                               

 94%|█████████▍| 34/36 [10:08<00:33, 16.67s/it]
 97%|█████████▋| 35/36 [10:26<00:17, 17.07s/it]
                                               

 97%|█████████▋| 35/36 [10:26<00:17, 17.07s/it]
100%|██████████| 36/36 [10:49<00:00, 18.83s/it]
                                               

100%|██████████| 36/36 [10:49<00:00, 18.83s/it]
                                               

100%|██████████| 36/36 [10:51<00:00, 18.83s/it]
100%|██████████| 36/36 [10:51<00:00, 18.09s/it]
{'loss': 1.0018, 'grad_norm': 2.4177138805389404, 'learning_rate': 0.0, 'entropy': 1.6683795936405659, 'num_tokens': 1968.0, 'mean_token_accuracy': 0.8097846880555153, 'epoch': 0.37}
{'loss': 1.0445, 'grad_norm': 2.4236433506011963, 'learning_rate': 2.5e-06, 'entropy': 1.3888544775545597, 'num_tokens': 5029.0, 'mean_token_accuracy': 0.8093724586069584, 'epoch': 0.74}
{'loss': 1.0961, 'grad_norm': 2.9668824672698975, 'learning_rate': 5e-06, 'entropy': 1.528201081536033, 'num_tokens': 6318.0, 'mean_token_accuracy': 0.8274629007686268, 'epoch': 1.0}
{'loss': 1.2092, 'grad_norm': 3.2720248699188232, 'learning_rate': 7.500000000000001e-06, 'entropy': 1.6628235839307308, 'num_tokens': 8014.0, 'mean_token_accuracy': 0.7986961454153061, 'epoch': 1.37}
{'loss': 1.0107, 'grad_norm': 2.1640515327453613, 'learning_rate': 1e-05, 'entropy': 1.356225661933422, 'num_tokens': 11175.0, 'mean_token_accuracy': 0.8248906023800373, 'epoch': 1.74}
{'loss': 0.8321, 'grad_norm': 2.105156183242798, 'learning_rate': 9.975923633360985e-06, 'entropy': 1.5897158709439365, 'num_tokens': 12636.0, 'mean_token_accuracy': 0.8517454103990034, 'epoch': 2.0}
{'loss': 0.986, 'grad_norm': 2.30998158454895, 'learning_rate': 9.903926402016153e-06, 'entropy': 1.5224517099559307, 'num_tokens': 14723.0, 'mean_token_accuracy': 0.8397073410451412, 'epoch': 2.37}
{'loss': 1.034, 'grad_norm': 2.766644239425659, 'learning_rate': 9.784701678661045e-06, 'entropy': 1.5129481963813305, 'num_tokens': 17027.0, 'mean_token_accuracy': 0.8221327364444733, 'epoch': 2.74}
{'loss': 0.8281, 'grad_norm': 1.5778119564056396, 'learning_rate': 9.619397662556434e-06, 'entropy': 1.5814784277569165, 'num_tokens': 18954.0, 'mean_token_accuracy': 0.8229299729520624, 'epoch': 3.0}
{'loss': 0.9039, 'grad_norm': 1.8715689182281494, 'learning_rate': 9.409606321741776e-06, 'entropy': 1.5196006894111633, 'num_tokens': 21674.0, 'mean_token_accuracy': 0.8265761323273182, 'epoch': 3.37}
{'loss': 0.8382, 'grad_norm': 2.305640697479248, 'learning_rate': 9.157348061512728e-06, 'entropy': 1.6361293271183968, 'num_tokens': 23385.0, 'mean_token_accuracy': 0.8478846177458763, 'epoch': 3.74}
{'loss': 0.9165, 'grad_norm': 2.2015862464904785, 'learning_rate': 8.865052266813686e-06, 'entropy': 1.4223850206895308, 'num_tokens': 25272.0, 'mean_token_accuracy': 0.8264588605273854, 'epoch': 4.0}
{'loss': 0.8788, 'grad_norm': 1.8726590871810913, 'learning_rate': 8.535533905932739e-06, 'entropy': 1.4522549733519554, 'num_tokens': 28245.0, 'mean_token_accuracy': 0.8233850933611393, 'epoch': 4.37}
{'loss': 0.8403, 'grad_norm': 2.315110683441162, 'learning_rate': 8.171966420818227e-06, 'entropy': 1.5813825950026512, 'num_tokens': 30283.0, 'mean_token_accuracy': 0.8271992728114128, 'epoch': 4.74}
{'loss': 0.7467, 'grad_norm': 2.0974671840667725, 'learning_rate': 7.777851165098012e-06, 'entropy': 1.6157631386410107, 'num_tokens': 31590.0, 'mean_token_accuracy': 0.8613714250651273, 'epoch': 5.0}
{'loss': 0.8728, 'grad_norm': 2.498725652694702, 'learning_rate': 7.3569836841299905e-06, 'entropy': 1.6376595944166183, 'num_tokens': 33360.0, 'mean_token_accuracy': 0.834260992705822, 'epoch': 5.37}
{'loss': 0.7459, 'grad_norm': 1.5646134614944458, 'learning_rate': 6.913417161825449e-06, 'entropy': 1.4191866889595985, 'num_tokens': 36519.0, 'mean_token_accuracy': 0.8493253327906132, 'epoch': 5.74}
{'loss': 0.8026, 'grad_norm': 2.3738090991973877, 'learning_rate': 6.451423386272312e-06, 'entropy': 1.5982248457995327, 'num_tokens': 37908.0, 'mean_token_accuracy': 0.8347415707328103, 'epoch': 6.0}
{'loss': 0.7325, 'grad_norm': 1.830989956855774, 'learning_rate': 5.975451610080643e-06, 'entropy': 1.4854546785354614, 'num_tokens': 40619.0, 'mean_token_accuracy': 0.8374447971582413, 'epoch': 6.37}
{'loss': 0.7734, 'grad_norm': 1.997825264930725, 'learning_rate': 5.490085701647805e-06, 'entropy': 1.553704597055912, 'num_tokens': 43042.0, 'mean_token_accuracy': 0.8597717359662056, 'epoch': 6.74}
{'loss': 0.7435, 'grad_norm': 2.309863328933716, 'learning_rate': 5e-06, 'entropy': 1.6349121711470864, 'num_tokens': 44226.0, 'mean_token_accuracy': 0.8431747393174605, 'epoch': 7.0}
{'loss': 0.659, 'grad_norm': 1.6951321363449097, 'learning_rate': 4.509914298352197e-06, 'entropy': 1.5352507643401623, 'num_tokens': 46461.0, 'mean_token_accuracy': 0.8864378668367863, 'epoch': 7.37}
{'loss': 0.7256, 'grad_norm': 1.6408005952835083, 'learning_rate': 4.02454838991936e-06, 'entropy': 1.539468977600336, 'num_tokens': 49072.0, 'mean_token_accuracy': 0.8520537428557873, 'epoch': 7.74}
{'loss': 0.8003, 'grad_norm': 2.4134228229522705, 'learning_rate': 3.5485766137276894e-06, 'entropy': 1.5903328440406106, 'num_tokens': 50544.0, 'mean_token_accuracy': 0.8197326822714373, 'epoch': 8.0}
{'loss': 0.752, 'grad_norm': 1.9492288827896118, 'learning_rate': 3.0865828381745515e-06, 'entropy': 1.450204748660326, 'num_tokens': 53147.0, 'mean_token_accuracy': 0.8304522782564163, 'epoch': 8.37}
{'loss': 0.6922, 'grad_norm': 1.7952930927276611, 'learning_rate': 2.6430163158700116e-06, 'entropy': 1.6082553416490555, 'num_tokens': 55190.0, 'mean_token_accuracy': 0.8688659891486168, 'epoch': 8.74}
{'loss': 0.6186, 'grad_norm': 1.6568412780761719, 'learning_rate': 2.2221488349019903e-06, 'entropy': 1.6167905601588162, 'num_tokens': 56862.0, 'mean_token_accuracy': 0.8961980397051031, 'epoch': 9.0}
{'loss': 0.6366, 'grad_norm': 1.5550566911697388, 'learning_rate': 1.8280335791817733e-06, 'entropy': 1.556851051747799, 'num_tokens': 59543.0, 'mean_token_accuracy': 0.8687282167375088, 'epoch': 9.37}
{'loss': 0.7076, 'grad_norm': 2.0406062602996826, 'learning_rate': 1.4644660940672628e-06, 'entropy': 1.598409865051508, 'num_tokens': 61573.0, 'mean_token_accuracy': 0.8532840460538864, 'epoch': 9.74}
{'loss': 0.7417, 'grad_norm': 1.9258946180343628, 'learning_rate': 1.134947733186315e-06, 'entropy': 1.4783020669763738, 'num_tokens': 63180.0, 'mean_token_accuracy': 0.8772044236009772, 'epoch': 10.0}
{'loss': 0.7722, 'grad_norm': 2.0786936283111572, 'learning_rate': 8.426519384872733e-07, 'entropy': 1.5537407100200653, 'num_tokens': 65326.0, 'mean_token_accuracy': 0.8693882115185261, 'epoch': 10.37}
{'loss': 0.6103, 'grad_norm': 1.5149060487747192, 'learning_rate': 5.903936782582253e-07, 'entropy': 1.4919258207082748, 'num_tokens': 67813.0, 'mean_token_accuracy': 0.8663139119744301, 'epoch': 10.74}
{'loss': 0.6682, 'grad_norm': 1.5657906532287598, 'learning_rate': 3.8060233744356634e-07, 'entropy': 1.6346853321248835, 'num_tokens': 69498.0, 'mean_token_accuracy': 0.8612899617715315, 'epoch': 11.0}
{'loss': 0.6984, 'grad_norm': 1.9356086254119873, 'learning_rate': 2.152983213389559e-07, 'entropy': 1.5995109640061855, 'num_tokens': 71308.0, 'mean_token_accuracy': 0.8598572909832001, 'epoch': 11.37}
{'loss': 0.6767, 'grad_norm': 1.8482896089553833, 'learning_rate': 9.607359798384785e-08, 'entropy': 1.6606888063251972, 'num_tokens': 73415.0, 'mean_token_accuracy': 0.8875432945787907, 'epoch': 11.74}
{'loss': 0.6573, 'grad_norm': 1.650292992591858, 'learning_rate': 2.4076366639015914e-08, 'entropy': 1.3266001668843357, 'num_tokens': 75816.0, 'mean_token_accuracy': 0.8488761294971813, 'epoch': 12.0}
{'train_runtime': 651.3331, 'train_samples_per_second': 0.792, 'train_steps_per_second': 0.055, 'train_loss': 0.8126114358504614, 'epoch': 12.0}
モデルを保存中...
Llama 3.2 3B QLoRA学習完了！モデルは ../outputs/llama32-3b-typst-qlora-003-006 に保存されました。
