Llama 3.2 3Bモデル: meta-llama/Llama-3.2-3B-Instruct
データ: ../jsonl/train_003.jsonl
出力: ../outputs/llama32-3b-typst-qlora-003-007
学習率: 1e-05
エポック数: 12
バッチサイズ: 1
勾配累積: 16
LoRA rank: 8
LoRA alpha: 8
LoRA dropout: 0.1
==================================================
トークナイザーを読み込み中...
4bit量子化設定を準備中...
モデルを読み込み中...

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.81s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.68s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.85s/it]
/mnt/d/github/typst-tex-converter/venv/lib/python3.11/site-packages/peft/tuners/lora/bnb.py:348: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.
  warnings.warn(
/mnt/d/github/typst-tex-converter/venv/lib/python3.11/site-packages/peft/tuners/tuners_utils.py:196: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!
  warnings.warn(
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 128009}.
既存のPEFTモデルを読み込み中: ../outputs/llama32-3b-typst-qlora-003-006
データセットを読み込み中...
データセットサイズ: 43
Llama 3.2 3B QLoRA学習を開始します...

  0%|          | 0/36 [00:00<?, ?it/s]
  3%|▎         | 1/36 [00:09<05:30,  9.45s/it]
                                              

  3%|▎         | 1/36 [00:09<05:30,  9.45s/it]
  6%|▌         | 2/36 [00:36<11:15, 19.86s/it]
                                              

  6%|▌         | 2/36 [00:36<11:15, 19.86s/it]
  8%|▊         | 3/36 [00:45<08:10, 14.87s/it]
                                              

  8%|▊         | 3/36 [00:45<08:10, 14.87s/it]
 11%|█         | 4/36 [00:58<07:32, 14.14s/it]
                                              

 11%|█         | 4/36 [00:58<07:32, 14.14s/it]
 14%|█▍        | 5/36 [01:22<09:12, 17.82s/it]
                                              

 14%|█▍        | 5/36 [01:22<09:12, 17.82s/it]
 17%|█▋        | 6/36 [01:40<08:50, 17.68s/it]
                                              

 17%|█▋        | 6/36 [01:40<08:50, 17.68s/it]
 19%|█▉        | 7/36 [01:55<08:09, 16.89s/it]
                                              

 19%|█▉        | 7/36 [01:55<08:09, 16.89s/it]
 22%|██▏       | 8/36 [02:13<08:05, 17.34s/it]
                                              

 22%|██▏       | 8/36 [02:13<08:05, 17.34s/it]
 25%|██▌       | 9/36 [02:34<08:17, 18.44s/it]
                                              

 25%|██▌       | 9/36 [02:34<08:17, 18.44s/it]
 28%|██▊       | 10/36 [02:59<08:46, 20.24s/it]
                                               

 28%|██▊       | 10/36 [02:59<08:46, 20.24s/it]
 31%|███       | 11/36 [03:15<07:58, 19.12s/it]
                                               

 31%|███       | 11/36 [03:15<07:58, 19.12s/it]
 33%|███▎      | 12/36 [03:29<07:00, 17.52s/it]
                                               

 33%|███▎      | 12/36 [03:29<07:00, 17.52s/it]
 36%|███▌      | 13/36 [03:55<07:40, 20.03s/it]
                                               

 36%|███▌      | 13/36 [03:55<07:40, 20.03s/it]
 39%|███▉      | 14/36 [04:11<06:57, 18.97s/it]
                                               

 39%|███▉      | 14/36 [04:11<06:57, 18.97s/it]
 42%|████▏     | 15/36 [04:24<05:56, 17.00s/it]
                                               

 42%|████▏     | 15/36 [04:24<05:56, 17.00s/it]
 44%|████▍     | 16/36 [04:36<05:10, 15.50s/it]
                                               

 44%|████▍     | 16/36 [04:36<05:10, 15.50s/it]
 47%|████▋     | 17/36 [05:09<06:35, 20.83s/it]
                                               

 47%|████▋     | 17/36 [05:09<06:35, 20.83s/it]
 50%|█████     | 18/36 [05:18<05:13, 17.40s/it]
                                               

 50%|█████     | 18/36 [05:18<05:13, 17.40s/it]
 53%|█████▎    | 19/36 [05:40<05:19, 18.81s/it]
                                               

 53%|█████▎    | 19/36 [05:40<05:19, 18.81s/it]
 56%|█████▌    | 20/36 [06:02<05:13, 19.59s/it]
                                               

 56%|█████▌    | 20/36 [06:02<05:13, 19.59s/it]
 58%|█████▊    | 21/36 [06:12<04:11, 16.75s/it]
                                               

 58%|█████▊    | 21/36 [06:12<04:11, 16.75s/it]
 61%|██████    | 22/36 [06:34<04:15, 18.22s/it]
                                               

 61%|██████    | 22/36 [06:34<04:15, 18.22s/it]
 64%|██████▍   | 23/36 [06:57<04:15, 19.64s/it]
                                               

 64%|██████▍   | 23/36 [06:57<04:15, 19.64s/it]
 67%|██████▋   | 24/36 [07:07<03:21, 16.76s/it]
                                               

 67%|██████▋   | 24/36 [07:07<03:21, 16.76s/it]
 69%|██████▉   | 25/36 [07:22<03:01, 16.45s/it]
                                               

 69%|██████▉   | 25/36 [07:22<03:01, 16.45s/it]
 72%|███████▏  | 26/36 [07:41<02:51, 17.11s/it]
                                               

 72%|███████▏  | 26/36 [07:41<02:51, 17.11s/it]
 75%|███████▌  | 27/36 [08:01<02:41, 17.92s/it]
                                               

 75%|███████▌  | 27/36 [08:01<02:41, 17.92s/it]
 78%|███████▊  | 28/36 [08:27<02:44, 20.52s/it]
                                               

 78%|███████▊  | 28/36 [08:27<02:44, 20.52s/it]
 81%|████████  | 29/36 [08:45<02:16, 19.57s/it]
                                               

 81%|████████  | 29/36 [08:45<02:16, 19.57s/it]
 83%|████████▎ | 30/36 [08:55<01:40, 16.75s/it]
                                               

 83%|████████▎ | 30/36 [08:55<01:40, 16.75s/it]
 86%|████████▌ | 31/36 [09:08<01:18, 15.64s/it]
                                               

 86%|████████▌ | 31/36 [09:08<01:18, 15.64s/it]
 89%|████████▉ | 32/36 [09:33<01:13, 18.47s/it]
                                               

 89%|████████▉ | 32/36 [09:33<01:13, 18.47s/it]
 92%|█████████▏| 33/36 [09:50<00:53, 17.92s/it]
                                               

 92%|█████████▏| 33/36 [09:50<00:53, 17.92s/it]
 94%|█████████▍| 34/36 [10:03<00:33, 16.52s/it]
                                               

 94%|█████████▍| 34/36 [10:03<00:33, 16.52s/it]
 97%|█████████▋| 35/36 [10:21<00:16, 16.93s/it]
                                               

 97%|█████████▋| 35/36 [10:21<00:16, 16.93s/it]
100%|██████████| 36/36 [10:44<00:00, 18.69s/it]
                                               

100%|██████████| 36/36 [10:44<00:00, 18.69s/it]
                                               

100%|██████████| 36/36 [10:45<00:00, 18.69s/it]
100%|██████████| 36/36 [10:45<00:00, 17.94s/it]
{'loss': 1.002, 'grad_norm': 2.2152116298675537, 'learning_rate': 0.0, 'entropy': 1.6677384413778782, 'num_tokens': 1968.0, 'mean_token_accuracy': 0.8103914856910706, 'epoch': 0.37}
{'loss': 1.0451, 'grad_norm': 2.239122152328491, 'learning_rate': 2.5e-06, 'entropy': 1.3883257657289505, 'num_tokens': 5029.0, 'mean_token_accuracy': 0.8093724586069584, 'epoch': 0.74}
{'loss': 1.0966, 'grad_norm': 2.804687023162842, 'learning_rate': 5e-06, 'entropy': 1.5273646333000876, 'num_tokens': 6318.0, 'mean_token_accuracy': 0.8274629007686268, 'epoch': 1.0}
{'loss': 1.21, 'grad_norm': 3.01349139213562, 'learning_rate': 7.500000000000001e-06, 'entropy': 1.6620124131441116, 'num_tokens': 8014.0, 'mean_token_accuracy': 0.7986961454153061, 'epoch': 1.37}
{'loss': 1.0112, 'grad_norm': 1.9994933605194092, 'learning_rate': 1e-05, 'entropy': 1.3556734770536423, 'num_tokens': 11175.0, 'mean_token_accuracy': 0.8242393508553505, 'epoch': 1.74}
{'loss': 0.8324, 'grad_norm': 1.9588735103607178, 'learning_rate': 9.975923633360985e-06, 'entropy': 1.5890347253192554, 'num_tokens': 12636.0, 'mean_token_accuracy': 0.8517454103990034, 'epoch': 2.0}
{'loss': 0.9866, 'grad_norm': 2.1179723739624023, 'learning_rate': 9.903926402016153e-06, 'entropy': 1.521512906998396, 'num_tokens': 14723.0, 'mean_token_accuracy': 0.8397073410451412, 'epoch': 2.37}
{'loss': 1.0355, 'grad_norm': 2.577183485031128, 'learning_rate': 9.784701678661045e-06, 'entropy': 1.5118209086358547, 'num_tokens': 17027.0, 'mean_token_accuracy': 0.8221327364444733, 'epoch': 2.74}
{'loss': 0.8288, 'grad_norm': 1.4609324932098389, 'learning_rate': 9.619397662556434e-06, 'entropy': 1.5804638483307578, 'num_tokens': 18954.0, 'mean_token_accuracy': 0.8229299729520624, 'epoch': 3.0}
{'loss': 0.9041, 'grad_norm': 1.81269109249115, 'learning_rate': 9.409606321741776e-06, 'entropy': 1.5182248279452324, 'num_tokens': 21674.0, 'mean_token_accuracy': 0.8259974308311939, 'epoch': 3.37}
{'loss': 0.8399, 'grad_norm': 2.1509244441986084, 'learning_rate': 9.157348061512728e-06, 'entropy': 1.6350531727075577, 'num_tokens': 23385.0, 'mean_token_accuracy': 0.848803736269474, 'epoch': 3.74}
{'loss': 0.9177, 'grad_norm': 2.059086322784424, 'learning_rate': 8.865052266813686e-06, 'entropy': 1.4211984114213423, 'num_tokens': 25272.0, 'mean_token_accuracy': 0.8264588605273854, 'epoch': 4.0}
{'loss': 0.8802, 'grad_norm': 1.7460602521896362, 'learning_rate': 8.535533905932739e-06, 'entropy': 1.4512071274220943, 'num_tokens': 28245.0, 'mean_token_accuracy': 0.8233850933611393, 'epoch': 4.37}
{'loss': 0.8428, 'grad_norm': 2.1725354194641113, 'learning_rate': 8.171966420818227e-06, 'entropy': 1.5797839313745499, 'num_tokens': 30283.0, 'mean_token_accuracy': 0.8271992728114128, 'epoch': 4.74}
{'loss': 0.7463, 'grad_norm': 2.0354371070861816, 'learning_rate': 7.777851165098012e-06, 'entropy': 1.6149182590571316, 'num_tokens': 31590.0, 'mean_token_accuracy': 0.8619325919584795, 'epoch': 5.0}
{'loss': 0.8742, 'grad_norm': 2.3820760250091553, 'learning_rate': 7.3569836841299905e-06, 'entropy': 1.6361107230186462, 'num_tokens': 33360.0, 'mean_token_accuracy': 0.8326461352407932, 'epoch': 5.37}
{'loss': 0.7467, 'grad_norm': 1.502326488494873, 'learning_rate': 6.913417161825449e-06, 'entropy': 1.4174961894750595, 'num_tokens': 36519.0, 'mean_token_accuracy': 0.8497321903705597, 'epoch': 5.74}
{'loss': 0.8052, 'grad_norm': 2.264049530029297, 'learning_rate': 6.451423386272312e-06, 'entropy': 1.59653808853843, 'num_tokens': 37908.0, 'mean_token_accuracy': 0.8347415707328103, 'epoch': 6.0}
{'loss': 0.7339, 'grad_norm': 1.7293614149093628, 'learning_rate': 5.975451610080643e-06, 'entropy': 1.4836338870227337, 'num_tokens': 40619.0, 'mean_token_accuracy': 0.8374447971582413, 'epoch': 6.37}
{'loss': 0.7745, 'grad_norm': 1.9033490419387817, 'learning_rate': 5.490085701647805e-06, 'entropy': 1.552107397466898, 'num_tokens': 43042.0, 'mean_token_accuracy': 0.8597717359662056, 'epoch': 6.74}
{'loss': 0.7456, 'grad_norm': 2.17427134513855, 'learning_rate': 5e-06, 'entropy': 1.633302699435841, 'num_tokens': 44226.0, 'mean_token_accuracy': 0.8426129384474321, 'epoch': 7.0}
{'loss': 0.6607, 'grad_norm': 1.6030367612838745, 'learning_rate': 4.509914298352197e-06, 'entropy': 1.5337839163839817, 'num_tokens': 46461.0, 'mean_token_accuracy': 0.8864378668367863, 'epoch': 7.37}
{'loss': 0.7272, 'grad_norm': 1.5626857280731201, 'learning_rate': 4.02454838991936e-06, 'entropy': 1.5379896312952042, 'num_tokens': 49072.0, 'mean_token_accuracy': 0.8522959910333157, 'epoch': 7.74}
{'loss': 0.8004, 'grad_norm': 2.3428759574890137, 'learning_rate': 3.5485766137276894e-06, 'entropy': 1.5880130095915361, 'num_tokens': 50544.0, 'mean_token_accuracy': 0.8205744353207675, 'epoch': 8.0}
{'loss': 0.7535, 'grad_norm': 1.8477071523666382, 'learning_rate': 3.0865828381745515e-06, 'entropy': 1.4490179605782032, 'num_tokens': 53147.0, 'mean_token_accuracy': 0.8311316259205341, 'epoch': 8.37}
{'loss': 0.6924, 'grad_norm': 1.7229424715042114, 'learning_rate': 2.6430163158700116e-06, 'entropy': 1.6062395758926868, 'num_tokens': 55190.0, 'mean_token_accuracy': 0.8694727867841721, 'epoch': 8.74}
{'loss': 0.6214, 'grad_norm': 1.5792819261550903, 'learning_rate': 2.2221488349019903e-06, 'entropy': 1.614875771782615, 'num_tokens': 56862.0, 'mean_token_accuracy': 0.8961980397051031, 'epoch': 9.0}
{'loss': 0.6382, 'grad_norm': 1.472161054611206, 'learning_rate': 1.8280335791817733e-06, 'entropy': 1.5553492531180382, 'num_tokens': 59543.0, 'mean_token_accuracy': 0.8657520264387131, 'epoch': 9.37}
{'loss': 0.7108, 'grad_norm': 1.9399759769439697, 'learning_rate': 1.4644660940672628e-06, 'entropy': 1.5961983241140842, 'num_tokens': 61573.0, 'mean_token_accuracy': 0.8542921133339405, 'epoch': 9.74}
{'loss': 0.7414, 'grad_norm': 1.8832621574401855, 'learning_rate': 1.134947733186315e-06, 'entropy': 1.4769298000769182, 'num_tokens': 63180.0, 'mean_token_accuracy': 0.8780870329249989, 'epoch': 10.0}
{'loss': 0.774, 'grad_norm': 1.9967156648635864, 'learning_rate': 8.426519384872733e-07, 'entropy': 1.5519210994243622, 'num_tokens': 65326.0, 'mean_token_accuracy': 0.8664120212197304, 'epoch': 10.37}
{'loss': 0.6113, 'grad_norm': 1.4797054529190063, 'learning_rate': 5.903936782582253e-07, 'entropy': 1.4903541281819344, 'num_tokens': 67813.0, 'mean_token_accuracy': 0.868001326918602, 'epoch': 10.74}
{'loss': 0.6685, 'grad_norm': 1.5184966325759888, 'learning_rate': 3.8060233744356634e-07, 'entropy': 1.6333333362232556, 'num_tokens': 69498.0, 'mean_token_accuracy': 0.8604073470289056, 'epoch': 11.0}
{'loss': 0.7015, 'grad_norm': 1.8411568403244019, 'learning_rate': 2.152983213389559e-07, 'entropy': 1.5977018438279629, 'num_tokens': 71308.0, 'mean_token_accuracy': 0.8598572909832001, 'epoch': 11.37}
{'loss': 0.6797, 'grad_norm': 1.7683265209197998, 'learning_rate': 9.607359798384785e-08, 'entropy': 1.6581327766180038, 'num_tokens': 73415.0, 'mean_token_accuracy': 0.8875432945787907, 'epoch': 11.74}
{'loss': 0.658, 'grad_norm': 1.579408049583435, 'learning_rate': 2.4076366639015914e-08, 'entropy': 1.324739239432595, 'num_tokens': 75816.0, 'mean_token_accuracy': 0.8445471254262057, 'epoch': 12.0}
{'train_runtime': 645.9533, 'train_samples_per_second': 0.799, 'train_steps_per_second': 0.056, 'train_loss': 0.813840475347307, 'epoch': 12.0}
モデルを保存中...
Llama 3.2 3B QLoRA学習完了！モデルは ../outputs/llama32-3b-typst-qlora-003-007 に保存されました。
