nohup: ignoring input
=== 002-010をbaseに003-000から003-010への学習開始 ===
開始時刻: Wed Sep 17 01:36:43 JST 2025
予定終了時刻: Wed Sep 17 02:36:43 JST 2025
学習済みモデル: 002-010
学習データ: train_003.jsonl
学習範囲: 003-000 → 003-010
出力形式: Markdown (.md)
試験間隔: 2回に1回
学習率: 1e-5（逐次学習用）
LoRA rank: 8
LoRA alpha: 8
VRAM: 8GB対応設定
================================
=== 汎用的継続学習開始 ===
学習済みモデル: 002-010
学習データ: train_003.jsonl
学習範囲: 0 → 10
各繰り返しのエポック数: 12
学習率: 1e-05
LoRA rank: 8
LoRA alpha: 8
テスト間隔: 2回に1回
タイムスタンプ: 20250917_013643

初期チェックポイント: ../outputs/llama32-3b-typst-qlora-002-010
学習ファイル: ../jsonl/train_003.jsonl

=== Iteration 000 の処理開始 ===
継続学習: ../jsonl/train_003.jsonl (from ../outputs/llama32-3b-typst-qlora-002-010)
学習ログ: ../logs/003-000_training_20250917_013643.log
実行中: python3 train_llama32_3b_qlora_fixed.py             --data ../jsonl/train_003.jsonl             --out ../outputs/llama32-3b-typst-qlora-003-000             --peft_model_path ../outputs/llama32-3b-typst-qlora-002-010             --epochs 12             --batch_size 1             --grad_accum 16             --learning_rate 1e-05             --lora_r 8             --lora_alpha 8             --lora_dropout 0.1
Llama 3.2 3Bモデル: meta-llama/Llama-3.2-3B-Instruct
データ: ../jsonl/train_003.jsonl
出力: ../outputs/llama32-3b-typst-qlora-003-000
学習率: 1e-05
エポック数: 12
バッチサイズ: 1
勾配累積: 16
LoRA rank: 8
LoRA alpha: 8
LoRA dropout: 0.1
==================================================
トークナイザーを読み込み中...
4bit量子化設定を準備中...
モデルを読み込み中...

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.98s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  2.98s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.28s/it]
既存のPEFTモデルを読み込み中: ../outputs/llama32-3b-typst-qlora-002-010
データセットを読み込み中...

Generating train split: 0 examples [00:00, ? examples/s]
Generating train split: 43 examples [00:00, 2241.66 examples/s]
/mnt/d/github/typst-tex-converter/venv/lib/python3.11/site-packages/peft/tuners/lora/bnb.py:348: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.
  warnings.warn(
/mnt/d/github/typst-tex-converter/venv/lib/python3.11/site-packages/peft/tuners/tuners_utils.py:196: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!
  warnings.warn(
データセットサイズ: 43

Tokenizing train dataset:   0%|          | 0/43 [00:00<?, ? examples/s]
Tokenizing train dataset: 100%|██████████| 43/43 [00:00<00:00, 946.06 examples/s]

Truncating train dataset:   0%|          | 0/43 [00:00<?, ? examples/s]
Truncating train dataset: 100%|██████████| 43/43 [00:00<00:00, 10423.34 examples/s]
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 128009}.
Llama 3.2 3B QLoRA学習を開始します...

  0%|          | 0/36 [00:00<?, ?it/s]
  3%|▎         | 1/36 [00:09<05:24,  9.27s/it]
                                              

  3%|▎         | 1/36 [00:09<05:24,  9.27s/it]
  6%|▌         | 2/36 [00:36<11:10, 19.72s/it]
                                              

  6%|▌         | 2/36 [00:36<11:10, 19.72s/it]
  8%|▊         | 3/36 [00:45<08:11, 14.91s/it]
                                              

  8%|▊         | 3/36 [00:45<08:11, 14.91s/it]
 11%|█         | 4/36 [00:58<07:32, 14.14s/it]
                                              

 11%|█         | 4/36 [00:58<07:32, 14.14s/it]
 14%|█▍        | 5/36 [01:22<09:09, 17.73s/it]
                                              

 14%|█▍        | 5/36 [01:22<09:09, 17.73s/it]
 17%|█▋        | 6/36 [01:39<08:47, 17.58s/it]
                                              

 17%|█▋        | 6/36 [01:39<08:47, 17.58s/it]
 19%|█▉        | 7/36 [01:54<08:06, 16.78s/it]
                                              

 19%|█▉        | 7/36 [01:54<08:06, 16.78s/it]
 22%|██▏       | 8/36 [02:12<08:00, 17.15s/it]
                                              

 22%|██▏       | 8/36 [02:12<08:00, 17.15s/it]
 25%|██▌       | 9/36 [02:33<08:14, 18.31s/it]
                                              

 25%|██▌       | 9/36 [02:33<08:14, 18.31s/it]
 28%|██▊       | 10/36 [02:57<08:39, 19.96s/it]
                                               

 28%|██▊       | 10/36 [02:57<08:39, 19.96s/it]
 31%|███       | 11/36 [03:14<07:53, 18.94s/it]
                                               

 31%|███       | 11/36 [03:14<07:53, 18.94s/it]
 33%|███▎      | 12/36 [03:27<06:56, 17.35s/it]
                                               

 33%|███▎      | 12/36 [03:27<06:56, 17.35s/it]
 36%|███▌      | 13/36 [03:53<07:36, 19.84s/it]
                                               

 36%|███▌      | 13/36 [03:53<07:36, 19.84s/it]
 39%|███▉      | 14/36 [04:09<06:51, 18.72s/it]
                                               

 39%|███▉      | 14/36 [04:09<06:51, 18.72s/it]
 42%|████▏     | 15/36 [04:21<05:52, 16.76s/it]
                                               

 42%|████▏     | 15/36 [04:21<05:52, 16.76s/it]
 44%|████▍     | 16/36 [04:33<05:06, 15.32s/it]
                                               

 44%|████▍     | 16/36 [04:33<05:06, 15.32s/it]
 47%|████▋     | 17/36 [05:06<06:31, 20.59s/it]
                                               

 47%|████▋     | 17/36 [05:06<06:31, 20.59s/it]
 50%|█████     | 18/36 [05:15<05:07, 17.08s/it]
                                               

 50%|█████     | 18/36 [05:15<05:07, 17.08s/it]
 53%|█████▎    | 19/36 [05:37<05:14, 18.47s/it]
                                               

 53%|█████▎    | 19/36 [05:37<05:14, 18.47s/it]
 56%|█████▌    | 20/36 [05:58<05:11, 19.49s/it]
                                               

 56%|█████▌    | 20/36 [05:58<05:11, 19.49s/it]
 58%|█████▊    | 21/36 [06:09<04:09, 16.65s/it]
                                               

 58%|█████▊    | 21/36 [06:09<04:09, 16.65s/it]
 61%|██████    | 22/36 [06:31<04:15, 18.25s/it]
                                               

 61%|██████    | 22/36 [06:31<04:15, 18.25s/it]
 64%|██████▍   | 23/36 [06:54<04:15, 19.67s/it]
                                               

 64%|██████▍   | 23/36 [06:54<04:15, 19.67s/it]
 67%|██████▋   | 24/36 [07:03<03:20, 16.67s/it]
                                               

 67%|██████▋   | 24/36 [07:03<03:20, 16.67s/it]
 69%|██████▉   | 25/36 [07:19<02:59, 16.34s/it]
                                               

 69%|██████▉   | 25/36 [07:19<02:59, 16.34s/it]
 72%|███████▏  | 26/36 [07:37<02:49, 16.99s/it]
                                               

 72%|███████▏  | 26/36 [07:37<02:49, 16.99s/it]
 75%|███████▌  | 27/36 [07:57<02:39, 17.72s/it]
                                               

 75%|███████▌  | 27/36 [07:57<02:39, 17.72s/it]
 78%|███████▊  | 28/36 [08:23<02:43, 20.41s/it]
                                               

 78%|███████▊  | 28/36 [08:23<02:43, 20.41s/it]
 81%|████████  | 29/36 [08:40<02:15, 19.42s/it]
                                               

 81%|████████  | 29/36 [08:40<02:15, 19.42s/it]
 83%|████████▎ | 30/36 [08:50<01:39, 16.59s/it]
                                               

 83%|████████▎ | 30/36 [08:50<01:39, 16.59s/it]
 86%|████████▌ | 31/36 [09:03<01:17, 15.47s/it]
                                               

 86%|████████▌ | 31/36 [09:03<01:17, 15.47s/it]
 89%|████████▉ | 32/36 [09:31<01:16, 19.10s/it]
                                               

 89%|████████▉ | 32/36 [09:31<01:16, 19.10s/it]
 92%|█████████▏| 33/36 [09:49<00:56, 18.82s/it]
                                               

 92%|█████████▏| 33/36 [09:49<00:56, 18.82s/it]
 94%|█████████▍| 34/36 [10:03<00:34, 17.48s/it]
                                               

 94%|█████████▍| 34/36 [10:03<00:34, 17.48s/it]
 97%|█████████▋| 35/36 [10:23<00:18, 18.05s/it]
                                               

 97%|█████████▋| 35/36 [10:23<00:18, 18.05s/it]
100%|██████████| 36/36 [10:48<00:00, 20.32s/it]
                                               

100%|██████████| 36/36 [10:48<00:00, 20.32s/it]
                                               

100%|██████████| 36/36 [10:50<00:00, 20.32s/it]
100%|██████████| 36/36 [10:50<00:00, 18.07s/it]
{'loss': 1.0019, 'grad_norm': 2.33729887008667, 'learning_rate': 0.0, 'entropy': 1.6680215001106262, 'num_tokens': 1968.0, 'mean_token_accuracy': 0.8103914856910706, 'epoch': 0.37}
{'loss': 1.0455, 'grad_norm': 2.3618662357330322, 'learning_rate': 2.5e-06, 'entropy': 1.3886091224849224, 'num_tokens': 5029.0, 'mean_token_accuracy': 0.8093724586069584, 'epoch': 0.74}
{'loss': 1.0963, 'grad_norm': 2.9068922996520996, 'learning_rate': 5e-06, 'entropy': 1.5278394439003684, 'num_tokens': 6318.0, 'mean_token_accuracy': 0.8274629007686268, 'epoch': 1.0}
{'loss': 1.2103, 'grad_norm': 3.174528121948242, 'learning_rate': 7.500000000000001e-06, 'entropy': 1.6622846834361553, 'num_tokens': 8014.0, 'mean_token_accuracy': 0.7986961454153061, 'epoch': 1.37}
{'loss': 1.0115, 'grad_norm': 2.111877918243408, 'learning_rate': 1e-05, 'entropy': 1.3558723703026772, 'num_tokens': 11175.0, 'mean_token_accuracy': 0.8236325532197952, 'epoch': 1.74}
{'loss': 0.8328, 'grad_norm': 2.0602097511291504, 'learning_rate': 9.975923633360985e-06, 'entropy': 1.589465618133545, 'num_tokens': 12636.0, 'mean_token_accuracy': 0.8517454103990034, 'epoch': 2.0}
{'loss': 0.9869, 'grad_norm': 2.254305839538574, 'learning_rate': 9.903926402016153e-06, 'entropy': 1.5217260979115963, 'num_tokens': 14723.0, 'mean_token_accuracy': 0.839100543409586, 'epoch': 2.37}
{'loss': 1.0358, 'grad_norm': 2.704228401184082, 'learning_rate': 9.784701678661045e-06, 'entropy': 1.5121206231415272, 'num_tokens': 17027.0, 'mean_token_accuracy': 0.8221327364444733, 'epoch': 2.74}
{'loss': 0.8291, 'grad_norm': 1.542239785194397, 'learning_rate': 9.619397662556434e-06, 'entropy': 1.5808115059679204, 'num_tokens': 18954.0, 'mean_token_accuracy': 0.8229299729520624, 'epoch': 3.0}
{'loss': 0.9048, 'grad_norm': 1.8527780771255493, 'learning_rate': 9.409606321741776e-06, 'entropy': 1.5184300988912582, 'num_tokens': 21674.0, 'mean_token_accuracy': 0.8259974308311939, 'epoch': 3.37}
{'loss': 0.8402, 'grad_norm': 2.2486648559570312, 'learning_rate': 9.157348061512728e-06, 'entropy': 1.6351203136146069, 'num_tokens': 23385.0, 'mean_token_accuracy': 0.8478846177458763, 'epoch': 3.74}
{'loss': 0.9177, 'grad_norm': 2.167335271835327, 'learning_rate': 8.865052266813686e-06, 'entropy': 1.4211718494241887, 'num_tokens': 25272.0, 'mean_token_accuracy': 0.8264588605273854, 'epoch': 4.0}
{'loss': 0.8806, 'grad_norm': 1.8346835374832153, 'learning_rate': 8.535533905932739e-06, 'entropy': 1.451025765389204, 'num_tokens': 28245.0, 'mean_token_accuracy': 0.8233850933611393, 'epoch': 4.37}
{'loss': 0.8431, 'grad_norm': 2.2779297828674316, 'learning_rate': 8.171966420818227e-06, 'entropy': 1.5798427276313305, 'num_tokens': 30283.0, 'mean_token_accuracy': 0.8271992728114128, 'epoch': 4.74}
{'loss': 0.7474, 'grad_norm': 2.095979928970337, 'learning_rate': 7.777851165098012e-06, 'entropy': 1.614700734615326, 'num_tokens': 31590.0, 'mean_token_accuracy': 0.8613714250651273, 'epoch': 5.0}
{'loss': 0.8752, 'grad_norm': 2.4660189151763916, 'learning_rate': 7.3569836841299905e-06, 'entropy': 1.6358339115977287, 'num_tokens': 33360.0, 'mean_token_accuracy': 0.8332529291510582, 'epoch': 5.37}
{'loss': 0.7472, 'grad_norm': 1.5623337030410767, 'learning_rate': 6.913417161825449e-06, 'entropy': 1.4177619405090809, 'num_tokens': 36519.0, 'mean_token_accuracy': 0.8489533066749573, 'epoch': 5.74}
{'loss': 0.8055, 'grad_norm': 2.358943223953247, 'learning_rate': 6.451423386272312e-06, 'entropy': 1.5964284159920432, 'num_tokens': 37908.0, 'mean_token_accuracy': 0.8347415707328103, 'epoch': 6.0}
{'loss': 0.7342, 'grad_norm': 1.8040869235992432, 'learning_rate': 5.975451610080643e-06, 'entropy': 1.4837151877582073, 'num_tokens': 40619.0, 'mean_token_accuracy': 0.8364202082157135, 'epoch': 6.37}
{'loss': 0.7755, 'grad_norm': 1.983323097229004, 'learning_rate': 5.490085701647805e-06, 'entropy': 1.5521885231137276, 'num_tokens': 43042.0, 'mean_token_accuracy': 0.8567813038825989, 'epoch': 6.74}
{'loss': 0.7467, 'grad_norm': 2.277348518371582, 'learning_rate': 5e-06, 'entropy': 1.6333699930797925, 'num_tokens': 44226.0, 'mean_token_accuracy': 0.8426129384474321, 'epoch': 7.0}
{'loss': 0.6608, 'grad_norm': 1.6800187826156616, 'learning_rate': 4.509914298352197e-06, 'entropy': 1.5339742489159107, 'num_tokens': 46461.0, 'mean_token_accuracy': 0.8864378668367863, 'epoch': 7.37}
{'loss': 0.7277, 'grad_norm': 1.622532606124878, 'learning_rate': 4.02454838991936e-06, 'entropy': 1.5375034101307392, 'num_tokens': 49072.0, 'mean_token_accuracy': 0.8510291539132595, 'epoch': 7.74}
{'loss': 0.8012, 'grad_norm': 2.4038050174713135, 'learning_rate': 3.5485766137276894e-06, 'entropy': 1.5883993994105945, 'num_tokens': 50544.0, 'mean_token_accuracy': 0.8189795342358676, 'epoch': 8.0}
{'loss': 0.7537, 'grad_norm': 1.9258538484573364, 'learning_rate': 3.0865828381745515e-06, 'entropy': 1.4484703429043293, 'num_tokens': 53147.0, 'mean_token_accuracy': 0.8311316259205341, 'epoch': 8.37}
{'loss': 0.6938, 'grad_norm': 1.78440260887146, 'learning_rate': 2.6430163158700116e-06, 'entropy': 1.6063241623342037, 'num_tokens': 55190.0, 'mean_token_accuracy': 0.8694727867841721, 'epoch': 8.74}
{'loss': 0.6217, 'grad_norm': 1.6425154209136963, 'learning_rate': 2.2221488349019903e-06, 'entropy': 1.614839028228413, 'num_tokens': 56862.0, 'mean_token_accuracy': 0.8961980397051031, 'epoch': 9.0}
{'loss': 0.6378, 'grad_norm': 1.5456169843673706, 'learning_rate': 1.8280335791817733e-06, 'entropy': 1.5551363676786423, 'num_tokens': 59543.0, 'mean_token_accuracy': 0.8687282167375088, 'epoch': 9.37}
{'loss': 0.7106, 'grad_norm': 2.0160725116729736, 'learning_rate': 1.4644660940672628e-06, 'entropy': 1.5964531116187572, 'num_tokens': 61573.0, 'mean_token_accuracy': 0.8532840460538864, 'epoch': 9.74}
{'loss': 0.7433, 'grad_norm': 1.9266071319580078, 'learning_rate': 1.134947733186315e-06, 'entropy': 1.47668410431255, 'num_tokens': 63180.0, 'mean_token_accuracy': 0.8758058222857389, 'epoch': 10.0}
{'loss': 0.7747, 'grad_norm': 2.0640347003936768, 'learning_rate': 8.426519384872733e-07, 'entropy': 1.5518285185098648, 'num_tokens': 65326.0, 'mean_token_accuracy': 0.8693882115185261, 'epoch': 10.37}
{'loss': 0.6121, 'grad_norm': 1.5169262886047363, 'learning_rate': 5.903936782582253e-07, 'entropy': 1.490330047905445, 'num_tokens': 67813.0, 'mean_token_accuracy': 0.8656345643103123, 'epoch': 10.74}
{'loss': 0.6699, 'grad_norm': 1.5616534948349, 'learning_rate': 3.8060233744356634e-07, 'entropy': 1.6331220269203186, 'num_tokens': 69498.0, 'mean_token_accuracy': 0.8604073470289056, 'epoch': 11.0}
{'loss': 0.7011, 'grad_norm': 1.9196267127990723, 'learning_rate': 2.152983213389559e-07, 'entropy': 1.5982626155018806, 'num_tokens': 71308.0, 'mean_token_accuracy': 0.8598572909832001, 'epoch': 11.37}
{'loss': 0.6788, 'grad_norm': 1.8335716724395752, 'learning_rate': 9.607359798384785e-08, 'entropy': 1.658377818763256, 'num_tokens': 73415.0, 'mean_token_accuracy': 0.8859284333884716, 'epoch': 11.74}
{'loss': 0.6585, 'grad_norm': 1.6351667642593384, 'learning_rate': 2.4076366639015914e-08, 'entropy': 1.3246531649069353, 'num_tokens': 75816.0, 'mean_token_accuracy': 0.8458458239381964, 'epoch': 12.0}
{'train_runtime': 650.5787, 'train_samples_per_second': 0.793, 'train_steps_per_second': 0.055, 'train_loss': 0.814275249838829, 'epoch': 12.0}
モデルを保存中...
Llama 3.2 3B QLoRA学習完了！モデルは ../outputs/llama32-3b-typst-qlora-003-000 に保存されました。
テスト実行: ../outputs/llama32-3b-typst-qlora-003-000
テストログ: ../logs/003-000_test_20250917_013643.log
実行中: python3 inference_llama32_3b.py                 --peft_model_path ../outputs/llama32-3b-typst-qlora-003-000                 --input_file ../sample/sample_small.tex                 --output_file ../trained/003-000.md
ベースモデル: meta-llama/Llama-3.2-3B-Instruct
✅ 使用モデル: PEFT（LoRA適用済み）
   PEFTモデル: ../outputs/llama32-3b-typst-qlora-003-000
入力ファイル: ../sample/sample_small.tex
出力ファイル: ../trained/003-000.md
トークナイザーを読み込み中...
4bit量子化設定を準備中...
ベースモデルを読み込み中...

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.95s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.77s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.95s/it]
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
✅ PEFTモデルを読み込み中...
   PEFTパス: ../outputs/llama32-3b-typst-qlora-003-000
   PEFTモデル読み込み完了
LaTeXファイルを読み込み中...
LaTeXテキストを 1 チャンクに分割しました
LaTeX→Typst変換を開始します...
チャンク 1/1 を変換中...
  入力テキスト長: 865 文字
  チャットテンプレート適用成功
  プロンプト長: 1223 文字
  入力トークン数: 415
  生成開始...
  生成完了: 629 トークン
  生成テキスト長: 1715 文字
  プロンプト長: 1223 文字
  生成テキスト開始: system

Cutting Knowledge Date: December 2023
Today Date: 17 Sep 2025

LaTeX 形式の文章を Typst形式 へ変換してください.
重要: TypstはLaTeXの代替となる文書組版システムです。TypeScript（プログラミング言語）ではありません。
出力はTypst構文のみ。プログラミングコードは出力しません。user...
  生成テキスト終了: ... $\left(\varepsilon_0, \varepsilon_1\right) \in \mathbb{R}^2$ are small, and $\varphi$ is a regular non-negative function. Our goal is to show the global existence of solutions under these conditions.
  生成テキスト全体の長さ: 1715 文字
  プロンプトの長さ: 1223 文字
  方法1（全体使用）: 1715 文字
  方法3（```typstマーカー）: 638 文字
  最終応答: system

Cutting Knowledge Date: December 2023
Today Date: 17 Sep 2025

LaTeX 形式の文章を Typst形式 へ変換してください.
重要: TypstはLaTeXの代替となる文書組版システムです。TypeScript（プログラミング言語）ではありません。
出力はTypst構文のみ。プログラミングコードは出力しません。user...
  最終応答の最後: ...non-negative function. Our goal is to show the global existence of solutions under these conditions.
  抽出された応答: system

Cutting Knowledge Date: December 2023
Today Date: 17 Sep 2025

LaTeX 形式の文章を Typst形式 へ変換してくださ...
結果を ../trained/003-000.md に保存中...
変換完了！
完了: Iteration 000 の学習・テストが完了しました。
チェックポイントを更新: ../outputs/llama32-3b-typst-qlora-003-000
=== Iteration 000 の処理完了 ===
--------------------------------------------------
=== Iteration 001 の処理開始 ===
継続学習: ../jsonl/train_003.jsonl (from ../outputs/llama32-3b-typst-qlora-003-000)
学習ログ: ../logs/003-001_training_20250917_013643.log
実行中: python3 train_llama32_3b_qlora_fixed.py             --data ../jsonl/train_003.jsonl             --out ../outputs/llama32-3b-typst-qlora-003-001             --peft_model_path ../outputs/llama32-3b-typst-qlora-003-000             --epochs 12             --batch_size 1             --grad_accum 16             --learning_rate 1e-05             --lora_r 8             --lora_alpha 8             --lora_dropout 0.1
Llama 3.2 3Bモデル: meta-llama/Llama-3.2-3B-Instruct
データ: ../jsonl/train_003.jsonl
出力: ../outputs/llama32-3b-typst-qlora-003-001
学習率: 1e-05
エポック数: 12
バッチサイズ: 1
勾配累積: 16
LoRA rank: 8
LoRA alpha: 8
LoRA dropout: 0.1
==================================================
トークナイザーを読み込み中...
4bit量子化設定を準備中...
モデルを読み込み中...

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.77s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.64s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.81s/it]
/mnt/d/github/typst-tex-converter/venv/lib/python3.11/site-packages/peft/tuners/lora/bnb.py:348: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.
  warnings.warn(
/mnt/d/github/typst-tex-converter/venv/lib/python3.11/site-packages/peft/tuners/tuners_utils.py:196: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!
  warnings.warn(
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 128009}.
既存のPEFTモデルを読み込み中: ../outputs/llama32-3b-typst-qlora-003-000
データセットを読み込み中...
データセットサイズ: 43
Llama 3.2 3B QLoRA学習を開始します...

  0%|          | 0/36 [00:00<?, ?it/s]
  3%|▎         | 1/36 [00:09<05:30,  9.44s/it]
                                              

  3%|▎         | 1/36 [00:09<05:30,  9.44s/it]
  6%|▌         | 2/36 [00:36<11:11, 19.74s/it]
                                              

  6%|▌         | 2/36 [00:36<11:11, 19.74s/it]
  8%|▊         | 3/36 [00:45<08:08, 14.81s/it]
                                              

  8%|▊         | 3/36 [00:45<08:08, 14.81s/it]
 11%|█         | 4/36 [00:58<07:28, 14.02s/it]
                                              

 11%|█         | 4/36 [00:58<07:28, 14.02s/it]
 14%|█▍        | 5/36 [01:21<09:04, 17.56s/it]
                                              

 14%|█▍        | 5/36 [01:21<09:04, 17.56s/it]
 17%|█▋        | 6/36 [01:39<08:43, 17.46s/it]
                                              

 17%|█▋        | 6/36 [01:39<08:43, 17.46s/it]
 19%|█▉        | 7/36 [01:54<08:05, 16.74s/it]
                                              

 19%|█▉        | 7/36 [01:54<08:05, 16.74s/it]
 22%|██▏       | 8/36 [02:12<07:58, 17.10s/it]
                                              

 22%|██▏       | 8/36 [02:12<07:58, 17.10s/it]
 25%|██▌       | 9/36 [02:33<08:13, 18.29s/it]
                                              

 25%|██▌       | 9/36 [02:33<08:13, 18.29s/it]
 28%|██▊       | 10/36 [02:56<08:36, 19.88s/it]
                                               

 28%|██▊       | 10/36 [02:56<08:36, 19.88s/it]
 31%|███       | 11/36 [03:13<07:50, 18.81s/it]
                                               

 31%|███       | 11/36 [03:13<07:50, 18.81s/it]
 33%|███▎      | 12/36 [03:26<06:55, 17.30s/it]
                                               

 33%|███▎      | 12/36 [03:26<06:55, 17.30s/it]
 36%|███▌      | 13/36 [03:52<07:35, 19.79s/it]
                                               

 36%|███▌      | 13/36 [03:52<07:35, 19.79s/it]
 39%|███▉      | 14/36 [04:08<06:51, 18.70s/it]
                                               

 39%|███▉      | 14/36 [04:08<06:51, 18.70s/it]
 42%|████▏     | 15/36 [04:20<05:51, 16.72s/it]
                                               

 42%|████▏     | 15/36 [04:20<05:51, 16.72s/it]
 44%|████▍     | 16/36 [04:32<05:03, 15.17s/it]
                                               

 44%|████▍     | 16/36 [04:32<05:03, 15.17s/it]
 47%|████▋     | 17/36 [05:05<06:31, 20.58s/it]
                                               

 47%|████▋     | 17/36 [05:05<06:31, 20.58s/it]
 50%|█████     | 18/36 [05:14<05:06, 17.05s/it]
                                               

 50%|█████     | 18/36 [05:14<05:06, 17.05s/it]
 53%|█████▎    | 19/36 [05:36<05:14, 18.49s/it]
                                               

 53%|█████▎    | 19/36 [05:36<05:14, 18.49s/it]
 56%|█████▌    | 20/36 [05:57<05:08, 19.27s/it]
                                               

 56%|█████▌    | 20/36 [05:57<05:08, 19.27s/it]
 58%|█████▊    | 21/36 [06:07<04:07, 16.47s/it]
                                               

 58%|█████▊    | 21/36 [06:07<04:07, 16.47s/it]
 61%|██████    | 22/36 [06:28<04:12, 18.00s/it]
                                               

 61%|██████    | 22/36 [06:28<04:12, 18.00s/it]
 64%|██████▍   | 23/36 [06:51<04:11, 19.38s/it]
                                               

 64%|██████▍   | 23/36 [06:51<04:11, 19.38s/it]
 67%|██████▋   | 24/36 [07:00<03:17, 16.45s/it]
                                               

 67%|██████▋   | 24/36 [07:00<03:17, 16.45s/it]
 69%|██████▉   | 25/36 [07:16<02:57, 16.17s/it]
                                               

 69%|██████▉   | 25/36 [07:16<02:57, 16.17s/it]
 72%|███████▏  | 26/36 [07:34<02:48, 16.82s/it]
                                               

 72%|███████▏  | 26/36 [07:34<02:48, 16.82s/it]
 75%|███████▌  | 27/36 [07:54<02:38, 17.61s/it]
                                               

 75%|███████▌  | 27/36 [07:54<02:38, 17.61s/it]
 78%|███████▊  | 28/36 [08:20<02:41, 20.24s/it]
                                               

 78%|███████▊  | 28/36 [08:20<02:41, 20.24s/it]
 81%|████████  | 29/36 [08:37<02:15, 19.30s/it]
                                               

 81%|████████  | 29/36 [08:37<02:15, 19.30s/it]
 83%|████████▎ | 30/36 [08:48<01:39, 16.61s/it]
                                               

 83%|████████▎ | 30/36 [08:48<01:39, 16.61s/it]
 86%|████████▌ | 31/36 [09:00<01:17, 15.43s/it]
                                               

 86%|████████▌ | 31/36 [09:00<01:17, 15.43s/it]
 89%|████████▉ | 32/36 [09:25<01:12, 18.23s/it]
                                               

 89%|████████▉ | 32/36 [09:25<01:12, 18.23s/it]
 92%|█████████▏| 33/36 [09:41<00:53, 17.68s/it]
                                               

 92%|█████████▏| 33/36 [09:41<00:53, 17.68s/it]
 94%|█████████▍| 34/36 [09:54<00:32, 16.24s/it]
                                               

 94%|█████████▍| 34/36 [09:54<00:32, 16.24s/it]
 97%|█████████▋| 35/36 [10:12<00:16, 16.69s/it]
                                               

 97%|█████████▋| 35/36 [10:12<00:16, 16.69s/it]
100%|██████████| 36/36 [10:35<00:00, 18.57s/it]
                                               

100%|██████████| 36/36 [10:35<00:00, 18.57s/it]
                                               

100%|██████████| 36/36 [10:37<00:00, 18.57s/it]
100%|██████████| 36/36 [10:37<00:00, 17.70s/it]
{'loss': 1.0016, 'grad_norm': 2.258714199066162, 'learning_rate': 0.0, 'entropy': 1.6688891127705574, 'num_tokens': 1968.0, 'mean_token_accuracy': 0.8103914856910706, 'epoch': 0.37}
{'loss': 1.0446, 'grad_norm': 2.2809298038482666, 'learning_rate': 2.5e-06, 'entropy': 1.3894551023840904, 'num_tokens': 5029.0, 'mean_token_accuracy': 0.8093724586069584, 'epoch': 0.74}
{'loss': 1.0957, 'grad_norm': 2.8253695964813232, 'learning_rate': 5e-06, 'entropy': 1.5291184728795832, 'num_tokens': 6318.0, 'mean_token_accuracy': 0.8274629007686268, 'epoch': 1.0}
{'loss': 1.2103, 'grad_norm': 3.086221218109131, 'learning_rate': 7.500000000000001e-06, 'entropy': 1.6633470952510834, 'num_tokens': 8014.0, 'mean_token_accuracy': 0.7986961454153061, 'epoch': 1.37}
{'loss': 1.0115, 'grad_norm': 2.0519018173217773, 'learning_rate': 1e-05, 'entropy': 1.3571248576045036, 'num_tokens': 11175.0, 'mean_token_accuracy': 0.824311900883913, 'epoch': 1.74}
{'loss': 0.8329, 'grad_norm': 2.009333372116089, 'learning_rate': 9.975923633360985e-06, 'entropy': 1.5900752923705361, 'num_tokens': 12636.0, 'mean_token_accuracy': 0.8517454103990034, 'epoch': 2.0}
{'loss': 0.9875, 'grad_norm': 2.1898696422576904, 'learning_rate': 9.903926402016153e-06, 'entropy': 1.5228165462613106, 'num_tokens': 14723.0, 'mean_token_accuracy': 0.8384653739631176, 'epoch': 2.37}
{'loss': 1.0366, 'grad_norm': 2.6337785720825195, 'learning_rate': 9.784701678661045e-06, 'entropy': 1.513108067214489, 'num_tokens': 17027.0, 'mean_token_accuracy': 0.8221327364444733, 'epoch': 2.74}
{'loss': 0.8297, 'grad_norm': 1.5071178674697876, 'learning_rate': 9.619397662556434e-06, 'entropy': 1.5814083326946606, 'num_tokens': 18954.0, 'mean_token_accuracy': 0.8229299729520624, 'epoch': 3.0}
{'loss': 0.9057, 'grad_norm': 1.821176528930664, 'learning_rate': 9.409606321741776e-06, 'entropy': 1.5195936597883701, 'num_tokens': 21674.0, 'mean_token_accuracy': 0.8265761323273182, 'epoch': 3.37}
{'loss': 0.841, 'grad_norm': 2.1978654861450195, 'learning_rate': 9.157348061512728e-06, 'entropy': 1.636051569133997, 'num_tokens': 23385.0, 'mean_token_accuracy': 0.8478846177458763, 'epoch': 3.74}
{'loss': 0.9194, 'grad_norm': 2.1219682693481445, 'learning_rate': 8.865052266813686e-06, 'entropy': 1.4224127747795798, 'num_tokens': 25272.0, 'mean_token_accuracy': 0.8264588605273854, 'epoch': 4.0}
{'loss': 0.8822, 'grad_norm': 1.793704867362976, 'learning_rate': 8.535533905932739e-06, 'entropy': 1.452485103160143, 'num_tokens': 28245.0, 'mean_token_accuracy': 0.8233850933611393, 'epoch': 4.37}
{'loss': 0.8449, 'grad_norm': 2.2173871994018555, 'learning_rate': 8.171966420818227e-06, 'entropy': 1.581007469445467, 'num_tokens': 30283.0, 'mean_token_accuracy': 0.8271992728114128, 'epoch': 4.74}
{'loss': 0.7477, 'grad_norm': 2.05427622795105, 'learning_rate': 7.777851165098012e-06, 'entropy': 1.6160461740060286, 'num_tokens': 31590.0, 'mean_token_accuracy': 0.8610499772158536, 'epoch': 5.0}
{'loss': 0.8774, 'grad_norm': 2.409044027328491, 'learning_rate': 7.3569836841299905e-06, 'entropy': 1.6374843046069145, 'num_tokens': 33360.0, 'mean_token_accuracy': 0.834260992705822, 'epoch': 5.37}
{'loss': 0.7489, 'grad_norm': 1.530343770980835, 'learning_rate': 6.913417161825449e-06, 'entropy': 1.4186882823705673, 'num_tokens': 36519.0, 'mean_token_accuracy': 0.8497111350297928, 'epoch': 5.74}
{'loss': 0.8066, 'grad_norm': 2.31191086769104, 'learning_rate': 6.451423386272312e-06, 'entropy': 1.5979300087148494, 'num_tokens': 37908.0, 'mean_token_accuracy': 0.8347415707328103, 'epoch': 6.0}
{'loss': 0.736, 'grad_norm': 1.769515037536621, 'learning_rate': 5.975451610080643e-06, 'entropy': 1.4853086844086647, 'num_tokens': 40619.0, 'mean_token_accuracy': 0.8388551883399487, 'epoch': 6.37}
{'loss': 0.7767, 'grad_norm': 1.9513564109802246, 'learning_rate': 5.490085701647805e-06, 'entropy': 1.5535254590213299, 'num_tokens': 43042.0, 'mean_token_accuracy': 0.8597717359662056, 'epoch': 6.74}
{'loss': 0.7478, 'grad_norm': 2.2122108936309814, 'learning_rate': 5e-06, 'entropy': 1.6349047910083423, 'num_tokens': 44226.0, 'mean_token_accuracy': 0.8438582691279325, 'epoch': 7.0}
{'loss': 0.6622, 'grad_norm': 1.639182209968567, 'learning_rate': 4.509914298352197e-06, 'entropy': 1.5353962779045105, 'num_tokens': 46461.0, 'mean_token_accuracy': 0.8798792250454426, 'epoch': 7.37}
{'loss': 0.729, 'grad_norm': 1.5950926542282104, 'learning_rate': 4.02454838991936e-06, 'entropy': 1.5395310185849667, 'num_tokens': 49072.0, 'mean_token_accuracy': 0.8510291539132595, 'epoch': 7.74}
{'loss': 0.8033, 'grad_norm': 2.3662166595458984, 'learning_rate': 3.5485766137276894e-06, 'entropy': 1.590057839046825, 'num_tokens': 50544.0, 'mean_token_accuracy': 0.8189795342358676, 'epoch': 8.0}
{'loss': 0.7557, 'grad_norm': 1.8890125751495361, 'learning_rate': 3.0865828381745515e-06, 'entropy': 1.4502099305391312, 'num_tokens': 53147.0, 'mean_token_accuracy': 0.8304522782564163, 'epoch': 8.37}
{'loss': 0.6943, 'grad_norm': 1.753154993057251, 'learning_rate': 2.6430163158700116e-06, 'entropy': 1.6079039312899113, 'num_tokens': 55190.0, 'mean_token_accuracy': 0.8688659891486168, 'epoch': 8.74}
{'loss': 0.6227, 'grad_norm': 1.6100016832351685, 'learning_rate': 2.2221488349019903e-06, 'entropy': 1.6166688203811646, 'num_tokens': 56862.0, 'mean_token_accuracy': 0.8961980397051031, 'epoch': 9.0}
{'loss': 0.6391, 'grad_norm': 1.5068988800048828, 'learning_rate': 1.8280335791817733e-06, 'entropy': 1.5569138005375862, 'num_tokens': 59543.0, 'mean_token_accuracy': 0.8657520264387131, 'epoch': 9.37}
{'loss': 0.7127, 'grad_norm': 1.9670053720474243, 'learning_rate': 1.4644660940672628e-06, 'entropy': 1.5981288589537144, 'num_tokens': 61573.0, 'mean_token_accuracy': 0.8532840460538864, 'epoch': 9.74}
{'loss': 0.7445, 'grad_norm': 1.8977408409118652, 'learning_rate': 1.134947733186315e-06, 'entropy': 1.4783541343428872, 'num_tokens': 63180.0, 'mean_token_accuracy': 0.8758058222857389, 'epoch': 10.0}
{'loss': 0.7775, 'grad_norm': 2.021225929260254, 'learning_rate': 8.426519384872733e-07, 'entropy': 1.5537862479686737, 'num_tokens': 65326.0, 'mean_token_accuracy': 0.8654504828155041, 'epoch': 10.37}
{'loss': 0.6129, 'grad_norm': 1.4932243824005127, 'learning_rate': 5.903936782582253e-07, 'entropy': 1.4920149184763432, 'num_tokens': 67813.0, 'mean_token_accuracy': 0.8663139119744301, 'epoch': 10.74}
{'loss': 0.6704, 'grad_norm': 1.5322967767715454, 'learning_rate': 3.8060233744356634e-07, 'entropy': 1.6353472742167385, 'num_tokens': 69498.0, 'mean_token_accuracy': 0.8595247377048839, 'epoch': 11.0}
{'loss': 0.7014, 'grad_norm': 1.8774633407592773, 'learning_rate': 2.152983213389559e-07, 'entropy': 1.5998433902859688, 'num_tokens': 71308.0, 'mean_token_accuracy': 0.8598572909832001, 'epoch': 11.37}
{'loss': 0.6811, 'grad_norm': 1.7983436584472656, 'learning_rate': 9.607359798384785e-08, 'entropy': 1.6604079864919186, 'num_tokens': 73415.0, 'mean_token_accuracy': 0.8869365006685257, 'epoch': 11.74}
{'loss': 0.66, 'grad_norm': 1.6069175004959106, 'learning_rate': 2.4076366639015914e-08, 'entropy': 1.326623103835366, 'num_tokens': 75816.0, 'mean_token_accuracy': 0.8435589833693071, 'epoch': 12.0}
{'train_runtime': 637.131, 'train_samples_per_second': 0.81, 'train_steps_per_second': 0.057, 'train_loss': 0.8153164303965039, 'epoch': 12.0}
モデルを保存中...
Llama 3.2 3B QLoRA学習完了！モデルは ../outputs/llama32-3b-typst-qlora-003-001 に保存されました。
スキップ: Iteration 001 のテストをスキップしました（2回に1回の実行）
チェックポイントを更新: ../outputs/llama32-3b-typst-qlora-003-001
=== Iteration 001 の処理完了 ===
--------------------------------------------------
=== Iteration 002 の処理開始 ===
継続学習: ../jsonl/train_003.jsonl (from ../outputs/llama32-3b-typst-qlora-003-001)
学習ログ: ../logs/003-002_training_20250917_013643.log
実行中: python3 train_llama32_3b_qlora_fixed.py             --data ../jsonl/train_003.jsonl             --out ../outputs/llama32-3b-typst-qlora-003-002             --peft_model_path ../outputs/llama32-3b-typst-qlora-003-001             --epochs 12             --batch_size 1             --grad_accum 16             --learning_rate 1e-05             --lora_r 8             --lora_alpha 8             --lora_dropout 0.1
Llama 3.2 3Bモデル: meta-llama/Llama-3.2-3B-Instruct
データ: ../jsonl/train_003.jsonl
出力: ../outputs/llama32-3b-typst-qlora-003-002
学習率: 1e-05
エポック数: 12
バッチサイズ: 1
勾配累積: 16
LoRA rank: 8
LoRA alpha: 8
LoRA dropout: 0.1
==================================================
トークナイザーを読み込み中...
4bit量子化設定を準備中...
モデルを読み込み中...

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.76s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.63s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.80s/it]
/mnt/d/github/typst-tex-converter/venv/lib/python3.11/site-packages/peft/tuners/lora/bnb.py:348: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.
  warnings.warn(
/mnt/d/github/typst-tex-converter/venv/lib/python3.11/site-packages/peft/tuners/tuners_utils.py:196: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!
  warnings.warn(
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 128009}.
既存のPEFTモデルを読み込み中: ../outputs/llama32-3b-typst-qlora-003-001
データセットを読み込み中...
データセットサイズ: 43
Llama 3.2 3B QLoRA学習を開始します...

  0%|          | 0/36 [00:00<?, ?it/s]
  3%|▎         | 1/36 [00:09<05:34,  9.56s/it]
                                              

  3%|▎         | 1/36 [00:09<05:34,  9.56s/it]
  6%|▌         | 2/36 [00:36<11:11, 19.74s/it]
                                              

  6%|▌         | 2/36 [00:36<11:11, 19.74s/it]
  8%|▊         | 3/36 [00:45<08:10, 14.85s/it]
                                              

  8%|▊         | 3/36 [00:45<08:10, 14.85s/it]
 11%|█         | 4/36 [00:58<07:32, 14.14s/it]
                                              

 11%|█         | 4/36 [00:58<07:32, 14.14s/it]
 14%|█▍        | 5/36 [01:22<09:08, 17.70s/it]
                                              

 14%|█▍        | 5/36 [01:22<09:08, 17.70s/it]
 17%|█▋        | 6/36 [01:40<08:50, 17.69s/it]
                                              

 17%|█▋        | 6/36 [01:40<08:50, 17.69s/it]
 19%|█▉        | 7/36 [01:55<08:10, 16.93s/it]
                                              

 19%|█▉        | 7/36 [01:55<08:10, 16.93s/it]
 22%|██▏       | 8/36 [02:14<08:08, 17.45s/it]
                                              

 22%|██▏       | 8/36 [02:14<08:08, 17.45s/it]
 25%|██▌       | 9/36 [02:35<08:22, 18.59s/it]
                                              

 25%|██▌       | 9/36 [02:35<08:22, 18.59s/it]
 28%|██▊       | 10/36 [02:59<08:46, 20.26s/it]
                                               

 28%|██▊       | 10/36 [02:59<08:46, 20.26s/it]
 31%|███       | 11/36 [03:15<07:59, 19.19s/it]
                                               

 31%|███       | 11/36 [03:15<07:59, 19.19s/it]
 33%|███▎      | 12/36 [03:29<07:02, 17.61s/it]
                                               

 33%|███▎      | 12/36 [03:29<07:02, 17.61s/it]
 36%|███▌      | 13/36 [03:55<07:42, 20.12s/it]
                                               

 36%|███▌      | 13/36 [03:55<07:42, 20.12s/it]
 39%|███▉      | 14/36 [04:12<07:01, 19.15s/it]
                                               

 39%|███▉      | 14/36 [04:12<07:01, 19.15s/it]
 42%|████▏     | 15/36 [04:25<06:00, 17.16s/it]
                                               

 42%|████▏     | 15/36 [04:25<06:00, 17.16s/it]
 44%|████▍     | 16/36 [04:37<05:16, 15.81s/it]
                                               

 44%|████▍     | 16/36 [04:37<05:16, 15.81s/it]
 47%|████▋     | 17/36 [05:11<06:39, 21.04s/it]
                                               

 47%|████▋     | 17/36 [05:11<06:39, 21.04s/it]
 50%|█████     | 18/36 [05:20<05:15, 17.53s/it]
                                               

 50%|█████     | 18/36 [05:20<05:15, 17.53s/it]
 53%|█████▎    | 19/36 [05:43<05:23, 19.01s/it]
                                               

 53%|█████▎    | 19/36 [05:43<05:23, 19.01s/it]
 56%|█████▌    | 20/36 [06:04<05:16, 19.80s/it]
                                               

 56%|█████▌    | 20/36 [06:04<05:16, 19.80s/it]
 58%|█████▊    | 21/36 [06:15<04:17, 17.17s/it]
                                               

 58%|█████▊    | 21/36 [06:15<04:17, 17.17s/it]
 61%|██████    | 22/36 [06:37<04:20, 18.59s/it]
                                               

 61%|██████    | 22/36 [06:37<04:20, 18.59s/it]
 64%|██████▍   | 23/36 [07:01<04:20, 20.06s/it]
                                               

 64%|██████▍   | 23/36 [07:01<04:20, 20.06s/it]
 67%|██████▋   | 24/36 [07:11<03:25, 17.10s/it]
                                               

 67%|██████▋   | 24/36 [07:11<03:25, 17.10s/it]
 69%|██████▉   | 25/36 [07:27<03:04, 16.78s/it]
                                               

 69%|██████▉   | 25/36 [07:27<03:04, 16.78s/it]
 72%|███████▏  | 26/36 [07:46<02:55, 17.51s/it]
                                               

 72%|███████▏  | 26/36 [07:46<02:55, 17.51s/it]
 75%|███████▌  | 27/36 [08:06<02:43, 18.21s/it]
                                               

 75%|███████▌  | 27/36 [08:06<02:43, 18.21s/it]
 78%|███████▊  | 28/36 [08:33<02:46, 20.86s/it]
                                               

 78%|███████▊  | 28/36 [08:33<02:46, 20.86s/it]
 81%|████████  | 29/36 [08:51<02:20, 20.03s/it]
                                               

 81%|████████  | 29/36 [08:51<02:20, 20.03s/it]
 83%|████████▎ | 30/36 [09:02<01:43, 17.21s/it]
                                               

 83%|████████▎ | 30/36 [09:02<01:43, 17.21s/it]
 86%|████████▌ | 31/36 [09:15<01:20, 16.09s/it]
                                               

 86%|████████▌ | 31/36 [09:15<01:20, 16.09s/it]
 89%|████████▉ | 32/36 [09:41<01:15, 18.93s/it]
                                               

 89%|████████▉ | 32/36 [09:41<01:15, 18.93s/it]
 92%|█████████▏| 33/36 [09:58<00:55, 18.36s/it]
                                               

 92%|█████████▏| 33/36 [09:58<00:55, 18.36s/it]
 94%|█████████▍| 34/36 [10:11<00:33, 16.98s/it]
                                               

 94%|█████████▍| 34/36 [10:11<00:33, 16.98s/it]
 97%|█████████▋| 35/36 [10:30<00:17, 17.41s/it]
                                               

 97%|█████████▋| 35/36 [10:30<00:17, 17.41s/it]
100%|██████████| 36/36 [10:53<00:00, 19.14s/it]
                                               

100%|██████████| 36/36 [10:53<00:00, 19.14s/it]
                                               

100%|██████████| 36/36 [10:55<00:00, 19.14s/it]
100%|██████████| 36/36 [10:55<00:00, 18.20s/it]
{'loss': 1.0019, 'grad_norm': 2.450902223587036, 'learning_rate': 0.0, 'entropy': 1.6679209992289543, 'num_tokens': 1968.0, 'mean_token_accuracy': 0.8097846880555153, 'epoch': 0.37}
{'loss': 1.0448, 'grad_norm': 2.480457305908203, 'learning_rate': 2.5e-06, 'entropy': 1.3886689879000187, 'num_tokens': 5029.0, 'mean_token_accuracy': 0.8093724586069584, 'epoch': 0.74}
{'loss': 1.0963, 'grad_norm': 3.0176987648010254, 'learning_rate': 5e-06, 'entropy': 1.5279894525354558, 'num_tokens': 6318.0, 'mean_token_accuracy': 0.8274629007686268, 'epoch': 1.0}
{'loss': 1.2099, 'grad_norm': 3.3491013050079346, 'learning_rate': 7.500000000000001e-06, 'entropy': 1.662413202226162, 'num_tokens': 8014.0, 'mean_token_accuracy': 0.7986961454153061, 'epoch': 1.37}
{'loss': 1.0111, 'grad_norm': 2.215109348297119, 'learning_rate': 1e-05, 'entropy': 1.3560837619006634, 'num_tokens': 11175.0, 'mean_token_accuracy': 0.824311900883913, 'epoch': 1.74}
{'loss': 0.8327, 'grad_norm': 2.152785062789917, 'learning_rate': 9.975923633360985e-06, 'entropy': 1.589459316297011, 'num_tokens': 12636.0, 'mean_token_accuracy': 0.8517454103990034, 'epoch': 2.0}
{'loss': 0.9871, 'grad_norm': 2.356004476547241, 'learning_rate': 9.903926402016153e-06, 'entropy': 1.5222299359738827, 'num_tokens': 14723.0, 'mean_token_accuracy': 0.839100543409586, 'epoch': 2.37}
{'loss': 1.0356, 'grad_norm': 2.8367760181427, 'learning_rate': 9.784701678661045e-06, 'entropy': 1.5124020278453827, 'num_tokens': 17027.0, 'mean_token_accuracy': 0.8221327364444733, 'epoch': 2.74}
{'loss': 0.8293, 'grad_norm': 1.6054368019104004, 'learning_rate': 9.619397662556434e-06, 'entropy': 1.5811418674208901, 'num_tokens': 18954.0, 'mean_token_accuracy': 0.8222464431415905, 'epoch': 3.0}
{'loss': 0.9054, 'grad_norm': 1.9126653671264648, 'learning_rate': 9.409606321741776e-06, 'entropy': 1.518830068409443, 'num_tokens': 21674.0, 'mean_token_accuracy': 0.8265761323273182, 'epoch': 3.37}
{'loss': 0.8404, 'grad_norm': 2.3444552421569824, 'learning_rate': 9.157348061512728e-06, 'entropy': 1.6357714012265205, 'num_tokens': 23385.0, 'mean_token_accuracy': 0.848803736269474, 'epoch': 3.74}
{'loss': 0.9184, 'grad_norm': 2.2420644760131836, 'learning_rate': 8.865052266813686e-06, 'entropy': 1.4215910217978738, 'num_tokens': 25272.0, 'mean_token_accuracy': 0.8257753361355175, 'epoch': 4.0}
{'loss': 0.8803, 'grad_norm': 1.9177298545837402, 'learning_rate': 8.535533905932739e-06, 'entropy': 1.451755452901125, 'num_tokens': 28245.0, 'mean_token_accuracy': 0.8229151703417301, 'epoch': 4.37}
{'loss': 0.8435, 'grad_norm': 2.363062620162964, 'learning_rate': 8.171966420818227e-06, 'entropy': 1.580625381320715, 'num_tokens': 30283.0, 'mean_token_accuracy': 0.8271992728114128, 'epoch': 4.74}
{'loss': 0.7499, 'grad_norm': 2.1249783039093018, 'learning_rate': 7.777851165098012e-06, 'entropy': 1.6151537028225986, 'num_tokens': 31590.0, 'mean_token_accuracy': 0.8610499772158536, 'epoch': 5.0}
{'loss': 0.8761, 'grad_norm': 2.536555051803589, 'learning_rate': 7.3569836841299905e-06, 'entropy': 1.6365783959627151, 'num_tokens': 33360.0, 'mean_token_accuracy': 0.8326461352407932, 'epoch': 5.37}
{'loss': 0.7483, 'grad_norm': 1.6029719114303589, 'learning_rate': 6.913417161825449e-06, 'entropy': 1.4182224795222282, 'num_tokens': 36519.0, 'mean_token_accuracy': 0.8493253327906132, 'epoch': 5.74}
{'loss': 0.807, 'grad_norm': 2.433889389038086, 'learning_rate': 6.451423386272312e-06, 'entropy': 1.5970922383395108, 'num_tokens': 37908.0, 'mean_token_accuracy': 0.8347415707328103, 'epoch': 6.0}
{'loss': 0.7349, 'grad_norm': 1.872286081314087, 'learning_rate': 5.975451610080643e-06, 'entropy': 1.4846229776740074, 'num_tokens': 40619.0, 'mean_token_accuracy': 0.8364202082157135, 'epoch': 6.37}
{'loss': 0.7763, 'grad_norm': 2.0534353256225586, 'learning_rate': 5.490085701647805e-06, 'entropy': 1.552746120840311, 'num_tokens': 43042.0, 'mean_token_accuracy': 0.8578777946531773, 'epoch': 6.74}
{'loss': 0.7457, 'grad_norm': 2.3615403175354004, 'learning_rate': 5e-06, 'entropy': 1.633952108296481, 'num_tokens': 44226.0, 'mean_token_accuracy': 0.8431747393174605, 'epoch': 7.0}
{'loss': 0.6615, 'grad_norm': 1.7429503202438354, 'learning_rate': 4.509914298352197e-06, 'entropy': 1.5348168350756168, 'num_tokens': 46461.0, 'mean_token_accuracy': 0.879493422806263, 'epoch': 7.37}
{'loss': 0.7283, 'grad_norm': 1.674381971359253, 'learning_rate': 4.02454838991936e-06, 'entropy': 1.5383450128138065, 'num_tokens': 49072.0, 'mean_token_accuracy': 0.8506270721554756, 'epoch': 7.74}
{'loss': 0.803, 'grad_norm': 2.4620683193206787, 'learning_rate': 3.5485766137276894e-06, 'entropy': 1.5884887305173008, 'num_tokens': 50544.0, 'mean_token_accuracy': 0.8197326822714373, 'epoch': 8.0}
{'loss': 0.7537, 'grad_norm': 2.007430076599121, 'learning_rate': 3.0865828381745515e-06, 'entropy': 1.4495652429759502, 'num_tokens': 53147.0, 'mean_token_accuracy': 0.8304522782564163, 'epoch': 8.37}
{'loss': 0.6947, 'grad_norm': 1.8363037109375, 'learning_rate': 2.6430163158700116e-06, 'entropy': 1.6073604747653008, 'num_tokens': 55190.0, 'mean_token_accuracy': 0.8688659891486168, 'epoch': 8.74}
{'loss': 0.6217, 'grad_norm': 1.6988929510116577, 'learning_rate': 2.2221488349019903e-06, 'entropy': 1.6153841506351123, 'num_tokens': 56862.0, 'mean_token_accuracy': 0.8961980397051031, 'epoch': 9.0}
{'loss': 0.6388, 'grad_norm': 1.5949705839157104, 'learning_rate': 1.8280335791817733e-06, 'entropy': 1.5560480579733849, 'num_tokens': 59543.0, 'mean_token_accuracy': 0.8687282167375088, 'epoch': 9.37}
{'loss': 0.7112, 'grad_norm': 2.0828301906585693, 'learning_rate': 1.4644660940672628e-06, 'entropy': 1.597403671592474, 'num_tokens': 61573.0, 'mean_token_accuracy': 0.8532840460538864, 'epoch': 9.74}
{'loss': 0.7456, 'grad_norm': 1.9526242017745972, 'learning_rate': 1.134947733186315e-06, 'entropy': 1.477042024785822, 'num_tokens': 63180.0, 'mean_token_accuracy': 0.874923207543113, 'epoch': 10.0}
{'loss': 0.7746, 'grad_norm': 2.133892059326172, 'learning_rate': 8.426519384872733e-07, 'entropy': 1.5524442121386528, 'num_tokens': 65326.0, 'mean_token_accuracy': 0.8693882115185261, 'epoch': 10.37}
{'loss': 0.6127, 'grad_norm': 1.5493525266647339, 'learning_rate': 5.903936782582253e-07, 'entropy': 1.4911327995359898, 'num_tokens': 67813.0, 'mean_token_accuracy': 0.8656345643103123, 'epoch': 10.74}
{'loss': 0.6716, 'grad_norm': 1.5976914167404175, 'learning_rate': 3.8060233744356634e-07, 'entropy': 1.6337862665002996, 'num_tokens': 69498.0, 'mean_token_accuracy': 0.8595247377048839, 'epoch': 11.0}
{'loss': 0.7016, 'grad_norm': 1.9854949712753296, 'learning_rate': 2.152983213389559e-07, 'entropy': 1.598825380206108, 'num_tokens': 71308.0, 'mean_token_accuracy': 0.8598572909832001, 'epoch': 11.37}
{'loss': 0.6796, 'grad_norm': 1.8912359476089478, 'learning_rate': 9.607359798384785e-08, 'entropy': 1.6593494899570942, 'num_tokens': 73415.0, 'mean_token_accuracy': 0.8871787488460541, 'epoch': 11.74}
{'loss': 0.6587, 'grad_norm': 1.6967980861663818, 'learning_rate': 2.4076366639015914e-08, 'entropy': 1.325485500422391, 'num_tokens': 75816.0, 'mean_token_accuracy': 0.8365102356130426, 'epoch': 12.0}
{'train_runtime': 655.2428, 'train_samples_per_second': 0.787, 'train_steps_per_second': 0.055, 'train_loss': 0.8147886925273471, 'epoch': 12.0}
モデルを保存中...
Llama 3.2 3B QLoRA学習完了！モデルは ../outputs/llama32-3b-typst-qlora-003-002 に保存されました。
テスト実行: ../outputs/llama32-3b-typst-qlora-003-002
テストログ: ../logs/003-002_test_20250917_013643.log
実行中: python3 inference_llama32_3b.py                 --peft_model_path ../outputs/llama32-3b-typst-qlora-003-002                 --input_file ../sample/sample_small.tex                 --output_file ../trained/003-002.md
ベースモデル: meta-llama/Llama-3.2-3B-Instruct
✅ 使用モデル: PEFT（LoRA適用済み）
   PEFTモデル: ../outputs/llama32-3b-typst-qlora-003-002
入力ファイル: ../sample/sample_small.tex
出力ファイル: ../trained/003-002.md
トークナイザーを読み込み中...
4bit量子化設定を準備中...
ベースモデルを読み込み中...

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.80s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.62s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.80s/it]
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
✅ PEFTモデルを読み込み中...
   PEFTパス: ../outputs/llama32-3b-typst-qlora-003-002
   PEFTモデル読み込み完了
LaTeXファイルを読み込み中...
LaTeXテキストを 1 チャンクに分割しました
LaTeX→Typst変換を開始します...
チャンク 1/1 を変換中...
  入力テキスト長: 865 文字
  チャットテンプレート適用成功
  プロンプト長: 1223 文字
  入力トークン数: 415
  生成開始...
  生成完了: 629 トークン
  生成テキスト長: 1715 文字
  プロンプト長: 1223 文字
  生成テキスト開始: system

Cutting Knowledge Date: December 2023
Today Date: 17 Sep 2025

LaTeX 形式の文章を Typst形式 へ変換してください.
重要: TypstはLaTeXの代替となる文書組版システムです。TypeScript（プログラミング言語）ではありません。
出力はTypst構文のみ。プログラミングコードは出力しません。user...
  生成テキスト終了: ... $\left(\varepsilon_0, \varepsilon_1\right) \in \mathbb{R}^2$ are small, and $\varphi$ is a regular non-negative function. Our goal is to show the global existence of solutions under these conditions.
  生成テキスト全体の長さ: 1715 文字
  プロンプトの長さ: 1223 文字
  方法1（全体使用）: 1715 文字
  方法3（```typstマーカー）: 638 文字
  最終応答: system

Cutting Knowledge Date: December 2023
Today Date: 17 Sep 2025

LaTeX 形式の文章を Typst形式 へ変換してください.
重要: TypstはLaTeXの代替となる文書組版システムです。TypeScript（プログラミング言語）ではありません。
出力はTypst構文のみ。プログラミングコードは出力しません。user...
  最終応答の最後: ...non-negative function. Our goal is to show the global existence of solutions under these conditions.
  抽出された応答: system

Cutting Knowledge Date: December 2023
Today Date: 17 Sep 2025

LaTeX 形式の文章を Typst形式 へ変換してくださ...
結果を ../trained/003-002.md に保存中...
変換完了！
完了: Iteration 002 の学習・テストが完了しました。
チェックポイントを更新: ../outputs/llama32-3b-typst-qlora-003-002
=== Iteration 002 の処理完了 ===
--------------------------------------------------
=== Iteration 003 の処理開始 ===
継続学習: ../jsonl/train_003.jsonl (from ../outputs/llama32-3b-typst-qlora-003-002)
学習ログ: ../logs/003-003_training_20250917_013643.log
実行中: python3 train_llama32_3b_qlora_fixed.py             --data ../jsonl/train_003.jsonl             --out ../outputs/llama32-3b-typst-qlora-003-003             --peft_model_path ../outputs/llama32-3b-typst-qlora-003-002             --epochs 12             --batch_size 1             --grad_accum 16             --learning_rate 1e-05             --lora_r 8             --lora_alpha 8             --lora_dropout 0.1
Llama 3.2 3Bモデル: meta-llama/Llama-3.2-3B-Instruct
データ: ../jsonl/train_003.jsonl
出力: ../outputs/llama32-3b-typst-qlora-003-003
学習率: 1e-05
エポック数: 12
バッチサイズ: 1
勾配累積: 16
LoRA rank: 8
LoRA alpha: 8
LoRA dropout: 0.1
==================================================
トークナイザーを読み込み中...
4bit量子化設定を準備中...
モデルを読み込み中...

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.90s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.74s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.92s/it]
/mnt/d/github/typst-tex-converter/venv/lib/python3.11/site-packages/peft/tuners/lora/bnb.py:348: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.
  warnings.warn(
/mnt/d/github/typst-tex-converter/venv/lib/python3.11/site-packages/peft/tuners/tuners_utils.py:196: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!
  warnings.warn(
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 128009}.
既存のPEFTモデルを読み込み中: ../outputs/llama32-3b-typst-qlora-003-002
データセットを読み込み中...
データセットサイズ: 43
Llama 3.2 3B QLoRA学習を開始します...

  0%|          | 0/36 [00:00<?, ?it/s]
  3%|▎         | 1/36 [00:10<05:55, 10.17s/it]
                                              

  3%|▎         | 1/36 [00:10<05:55, 10.17s/it]
  6%|▌         | 2/36 [00:39<12:15, 21.63s/it]
                                              

  6%|▌         | 2/36 [00:39<12:15, 21.63s/it]
  8%|▊         | 3/36 [00:49<08:50, 16.08s/it]
                                              

  8%|▊         | 3/36 [00:49<08:50, 16.08s/it]
 11%|█         | 4/36 [01:03<08:05, 15.17s/it]
                                              

 11%|█         | 4/36 [01:03<08:05, 15.17s/it]
 14%|█▍        | 5/36 [01:29<09:57, 19.27s/it]
                                              

 14%|█▍        | 5/36 [01:29<09:57, 19.27s/it]
 17%|█▋        | 6/36 [01:48<09:31, 19.07s/it]
                                              

 17%|█▋        | 6/36 [01:48<09:31, 19.07s/it]
 19%|█▉        | 7/36 [02:04<08:47, 18.19s/it]
                                              

 19%|█▉        | 7/36 [02:04<08:47, 18.19s/it]
 22%|██▏       | 8/36 [02:23<08:36, 18.46s/it]
                                              

 22%|██▏       | 8/36 [02:23<08:36, 18.46s/it]
 25%|██▌       | 9/36 [02:46<08:53, 19.77s/it]
                                              

 25%|██▌       | 9/36 [02:46<08:53, 19.77s/it]
 28%|██▊       | 10/36 [03:11<09:20, 21.57s/it]
                                               

 28%|██▊       | 10/36 [03:11<09:20, 21.57s/it]
 31%|███       | 11/36 [03:29<08:27, 20.31s/it]
                                               

 31%|███       | 11/36 [03:29<08:27, 20.31s/it]
 33%|███▎      | 12/36 [03:44<07:28, 18.70s/it]
                                               

 33%|███▎      | 12/36 [03:44<07:28, 18.70s/it]
 36%|███▌      | 13/36 [04:12<08:14, 21.51s/it]
                                               

 36%|███▌      | 13/36 [04:12<08:14, 21.51s/it]
 39%|███▉      | 14/36 [04:29<07:24, 20.22s/it]
                                               

 39%|███▉      | 14/36 [04:29<07:24, 20.22s/it]
 42%|████▏     | 15/36 [04:43<06:23, 18.28s/it]
                                               

 42%|████▏     | 15/36 [04:43<06:23, 18.28s/it]
 44%|████▍     | 16/36 [04:56<05:33, 16.68s/it]
                                               

 44%|████▍     | 16/36 [04:56<05:33, 16.68s/it]
 47%|████▋     | 17/36 [05:32<07:07, 22.49s/it]
                                               

 47%|████▋     | 17/36 [05:32<07:07, 22.49s/it]
 50%|█████     | 18/36 [05:41<05:34, 18.59s/it]
                                               

 50%|█████     | 18/36 [05:41<05:34, 18.59s/it]
 53%|█████▎    | 19/36 [06:05<05:43, 20.21s/it]
                                               

 53%|█████▎    | 19/36 [06:05<05:43, 20.21s/it]
 56%|█████▌    | 20/36 [06:28<05:37, 21.08s/it]
                                               

 56%|█████▌    | 20/36 [06:28<05:37, 21.08s/it]
 58%|█████▊    | 21/36 [06:39<04:28, 17.91s/it]
                                               

 58%|█████▊    | 21/36 [06:39<04:28, 17.91s/it]
 61%|██████    | 22/36 [07:03<04:34, 19.62s/it]
                                               

 61%|██████    | 22/36 [07:03<04:34, 19.62s/it]
 64%|██████▍   | 23/36 [07:27<04:35, 21.17s/it]
                                               

 64%|██████▍   | 23/36 [07:27<04:35, 21.17s/it]
 67%|██████▋   | 24/36 [07:38<03:34, 17.90s/it]
                                               

 67%|██████▋   | 24/36 [07:38<03:34, 17.90s/it]
 69%|██████▉   | 25/36 [07:54<03:13, 17.56s/it]
                                               

 69%|██████▉   | 25/36 [07:54<03:13, 17.56s/it]
 72%|███████▏  | 26/36 [08:14<03:02, 18.29s/it]
                                               

 72%|███████▏  | 26/36 [08:14<03:02, 18.29s/it]
 75%|███████▌  | 27/36 [08:36<02:53, 19.33s/it]
                                               

 75%|███████▌  | 27/36 [08:36<02:53, 19.33s/it]
 78%|███████▊  | 28/36 [09:05<02:57, 22.18s/it]
                                               

 78%|███████▊  | 28/36 [09:05<02:57, 22.18s/it]
 81%|████████  | 29/36 [09:23<02:27, 21.07s/it]
                                               

 81%|████████  | 29/36 [09:23<02:27, 21.07s/it]
 83%|████████▎ | 30/36 [09:34<01:47, 17.87s/it]
                                               

 83%|████████▎ | 30/36 [09:34<01:47, 17.87s/it]
 86%|████████▌ | 31/36 [09:48<01:22, 16.60s/it]
                                               

 86%|████████▌ | 31/36 [09:48<01:22, 16.60s/it]
 89%|████████▉ | 32/36 [10:15<01:19, 19.77s/it]
                                               

 89%|████████▉ | 32/36 [10:15<01:19, 19.77s/it]
 92%|█████████▏| 33/36 [10:33<00:57, 19.22s/it]
                                               

 92%|█████████▏| 33/36 [10:33<00:57, 19.22s/it]
 94%|█████████▍| 34/36 [10:47<00:35, 17.64s/it]
                                               

 94%|█████████▍| 34/36 [10:47<00:35, 17.64s/it]
 97%|█████████▋| 35/36 [11:06<00:18, 18.17s/it]
                                               

 97%|█████████▋| 35/36 [11:06<00:18, 18.17s/it]
100%|██████████| 36/36 [11:31<00:00, 20.17s/it]
                                               

100%|██████████| 36/36 [11:31<00:00, 20.17s/it]
                                               

100%|██████████| 36/36 [11:32<00:00, 20.17s/it]
100%|██████████| 36/36 [11:32<00:00, 19.25s/it]
{'loss': 1.0018, 'grad_norm': 2.1563456058502197, 'learning_rate': 0.0, 'entropy': 1.6680224612355232, 'num_tokens': 1968.0, 'mean_token_accuracy': 0.8097846880555153, 'epoch': 0.37}
{'loss': 1.0452, 'grad_norm': 2.1949589252471924, 'learning_rate': 2.5e-06, 'entropy': 1.3885260112583637, 'num_tokens': 5029.0, 'mean_token_accuracy': 0.8093724586069584, 'epoch': 0.74}
{'loss': 1.0964, 'grad_norm': 2.7553718090057373, 'learning_rate': 5e-06, 'entropy': 1.5278240008787676, 'num_tokens': 6318.0, 'mean_token_accuracy': 0.8274629007686268, 'epoch': 1.0}
{'loss': 1.2107, 'grad_norm': 2.9516971111297607, 'learning_rate': 7.500000000000001e-06, 'entropy': 1.6624053791165352, 'num_tokens': 8014.0, 'mean_token_accuracy': 0.7986961454153061, 'epoch': 1.37}
{'loss': 1.012, 'grad_norm': 1.971950650215149, 'learning_rate': 1e-05, 'entropy': 1.3559046015143394, 'num_tokens': 11175.0, 'mean_token_accuracy': 0.8236325532197952, 'epoch': 1.74}
{'loss': 0.833, 'grad_norm': 1.9431785345077515, 'learning_rate': 9.975923633360985e-06, 'entropy': 1.5893222852186724, 'num_tokens': 12636.0, 'mean_token_accuracy': 0.8517454103990034, 'epoch': 2.0}
{'loss': 0.9877, 'grad_norm': 2.0938944816589355, 'learning_rate': 9.903926402016153e-06, 'entropy': 1.5216482691466808, 'num_tokens': 14723.0, 'mean_token_accuracy': 0.8397073410451412, 'epoch': 2.37}
{'loss': 1.0368, 'grad_norm': 2.5414388179779053, 'learning_rate': 9.784701678661045e-06, 'entropy': 1.5122089609503746, 'num_tokens': 17027.0, 'mean_token_accuracy': 0.8221327364444733, 'epoch': 2.74}
{'loss': 0.8302, 'grad_norm': 1.445448875427246, 'learning_rate': 9.619397662556434e-06, 'entropy': 1.5807216600938276, 'num_tokens': 18954.0, 'mean_token_accuracy': 0.8229299729520624, 'epoch': 3.0}
{'loss': 0.9056, 'grad_norm': 1.7889926433563232, 'learning_rate': 9.409606321741776e-06, 'entropy': 1.518686842173338, 'num_tokens': 21674.0, 'mean_token_accuracy': 0.8265761323273182, 'epoch': 3.37}
{'loss': 0.8424, 'grad_norm': 2.1134519577026367, 'learning_rate': 9.157348061512728e-06, 'entropy': 1.635350275784731, 'num_tokens': 23385.0, 'mean_token_accuracy': 0.8478846177458763, 'epoch': 3.74}
{'loss': 0.9195, 'grad_norm': 2.045271158218384, 'learning_rate': 8.865052266813686e-06, 'entropy': 1.421386567029086, 'num_tokens': 25272.0, 'mean_token_accuracy': 0.8264588605273854, 'epoch': 4.0}
{'loss': 0.8819, 'grad_norm': 1.7347538471221924, 'learning_rate': 8.535533905932739e-06, 'entropy': 1.4514076076447964, 'num_tokens': 28245.0, 'mean_token_accuracy': 0.8233850933611393, 'epoch': 4.37}
{'loss': 0.845, 'grad_norm': 2.1473772525787354, 'learning_rate': 8.171966420818227e-06, 'entropy': 1.5802484937012196, 'num_tokens': 30283.0, 'mean_token_accuracy': 0.8271992728114128, 'epoch': 4.74}
{'loss': 0.7488, 'grad_norm': 2.0063605308532715, 'learning_rate': 7.777851165098012e-06, 'entropy': 1.6150829954580828, 'num_tokens': 31590.0, 'mean_token_accuracy': 0.8604888103225015, 'epoch': 5.0}
{'loss': 0.8764, 'grad_norm': 2.37146258354187, 'learning_rate': 7.3569836841299905e-06, 'entropy': 1.6365853771567345, 'num_tokens': 33360.0, 'mean_token_accuracy': 0.8228362612426281, 'epoch': 5.37}
{'loss': 0.7497, 'grad_norm': 1.486633062362671, 'learning_rate': 6.913417161825449e-06, 'entropy': 1.4178225621581078, 'num_tokens': 36519.0, 'mean_token_accuracy': 0.849718414247036, 'epoch': 5.74}
{'loss': 0.8082, 'grad_norm': 2.246094226837158, 'learning_rate': 6.451423386272312e-06, 'entropy': 1.5968497558073564, 'num_tokens': 37908.0, 'mean_token_accuracy': 0.8347415707328103, 'epoch': 6.0}
{'loss': 0.7367, 'grad_norm': 1.720947265625, 'learning_rate': 5.975451610080643e-06, 'entropy': 1.4841836728155613, 'num_tokens': 40619.0, 'mean_token_accuracy': 0.8374447971582413, 'epoch': 6.37}
{'loss': 0.7773, 'grad_norm': 1.890011191368103, 'learning_rate': 5.490085701647805e-06, 'entropy': 1.5525790825486183, 'num_tokens': 43042.0, 'mean_token_accuracy': 0.8597717359662056, 'epoch': 6.74}
{'loss': 0.7487, 'grad_norm': 2.145531415939331, 'learning_rate': 5e-06, 'entropy': 1.6340060775930232, 'num_tokens': 44226.0, 'mean_token_accuracy': 0.8432964628392999, 'epoch': 7.0}
{'loss': 0.6627, 'grad_norm': 1.5966066122055054, 'learning_rate': 4.509914298352197e-06, 'entropy': 1.5348067693412304, 'num_tokens': 46461.0, 'mean_token_accuracy': 0.879493422806263, 'epoch': 7.37}
{'loss': 0.7299, 'grad_norm': 1.5489555597305298, 'learning_rate': 4.02454838991936e-06, 'entropy': 1.538487683981657, 'num_tokens': 49072.0, 'mean_token_accuracy': 0.8503848239779472, 'epoch': 7.74}
{'loss': 0.8035, 'grad_norm': 2.3359997272491455, 'learning_rate': 3.5485766137276894e-06, 'entropy': 1.5889180573550137, 'num_tokens': 50544.0, 'mean_token_accuracy': 0.8189795342358676, 'epoch': 8.0}
{'loss': 0.7561, 'grad_norm': 1.8403332233428955, 'learning_rate': 3.0865828381745515e-06, 'entropy': 1.4491279311478138, 'num_tokens': 53147.0, 'mean_token_accuracy': 0.8304522782564163, 'epoch': 8.37}
{'loss': 0.695, 'grad_norm': 1.7165122032165527, 'learning_rate': 2.6430163158700116e-06, 'entropy': 1.6078190356492996, 'num_tokens': 55190.0, 'mean_token_accuracy': 0.8688659891486168, 'epoch': 8.74}
{'loss': 0.6236, 'grad_norm': 1.5610132217407227, 'learning_rate': 2.2221488349019903e-06, 'entropy': 1.6159252253445713, 'num_tokens': 56862.0, 'mean_token_accuracy': 0.8961980397051031, 'epoch': 9.0}
{'loss': 0.6405, 'grad_norm': 1.4676858186721802, 'learning_rate': 1.8280335791817733e-06, 'entropy': 1.5563626885414124, 'num_tokens': 59543.0, 'mean_token_accuracy': 0.8657520264387131, 'epoch': 9.37}
{'loss': 0.7131, 'grad_norm': 1.923163652420044, 'learning_rate': 1.4644660940672628e-06, 'entropy': 1.5974002033472061, 'num_tokens': 61573.0, 'mean_token_accuracy': 0.8532840460538864, 'epoch': 9.74}
{'loss': 0.7448, 'grad_norm': 1.8761032819747925, 'learning_rate': 1.134947733186315e-06, 'entropy': 1.4774288860234348, 'num_tokens': 63180.0, 'mean_token_accuracy': 0.8758058222857389, 'epoch': 10.0}
{'loss': 0.7775, 'grad_norm': 1.9845523834228516, 'learning_rate': 8.426519384872733e-07, 'entropy': 1.5526240691542625, 'num_tokens': 65326.0, 'mean_token_accuracy': 0.8684266731142998, 'epoch': 10.37}
{'loss': 0.6134, 'grad_norm': 1.4746742248535156, 'learning_rate': 5.903936782582253e-07, 'entropy': 1.4914328195154667, 'num_tokens': 67813.0, 'mean_token_accuracy': 0.8663139119744301, 'epoch': 10.74}
{'loss': 0.671, 'grad_norm': 1.5200966596603394, 'learning_rate': 3.8060233744356634e-07, 'entropy': 1.6347410082817078, 'num_tokens': 69498.0, 'mean_token_accuracy': 0.8604073470289056, 'epoch': 11.0}
{'loss': 0.7028, 'grad_norm': 1.8244191408157349, 'learning_rate': 2.152983213389559e-07, 'entropy': 1.5995829477906227, 'num_tokens': 71308.0, 'mean_token_accuracy': 0.8598572909832001, 'epoch': 11.37}
{'loss': 0.6819, 'grad_norm': 1.758914589881897, 'learning_rate': 9.607359798384785e-08, 'entropy': 1.6597285531461239, 'num_tokens': 73415.0, 'mean_token_accuracy': 0.8855736888945103, 'epoch': 11.74}
{'loss': 0.6609, 'grad_norm': 1.5674716234207153, 'learning_rate': 2.4076366639015914e-08, 'entropy': 1.3253102573481472, 'num_tokens': 75816.0, 'mean_token_accuracy': 0.8405286778103221, 'epoch': 12.0}
{'train_runtime': 692.959, 'train_samples_per_second': 0.745, 'train_steps_per_second': 0.052, 'train_loss': 0.8158487098084556, 'epoch': 12.0}
モデルを保存中...
Llama 3.2 3B QLoRA学習完了！モデルは ../outputs/llama32-3b-typst-qlora-003-003 に保存されました。
スキップ: Iteration 003 のテストをスキップしました（2回に1回の実行）
チェックポイントを更新: ../outputs/llama32-3b-typst-qlora-003-003
=== Iteration 003 の処理完了 ===
--------------------------------------------------
=== Iteration 004 の処理開始 ===
継続学習: ../jsonl/train_003.jsonl (from ../outputs/llama32-3b-typst-qlora-003-003)
学習ログ: ../logs/003-004_training_20250917_013643.log
実行中: python3 train_llama32_3b_qlora_fixed.py             --data ../jsonl/train_003.jsonl             --out ../outputs/llama32-3b-typst-qlora-003-004             --peft_model_path ../outputs/llama32-3b-typst-qlora-003-003             --epochs 12             --batch_size 1             --grad_accum 16             --learning_rate 1e-05             --lora_r 8             --lora_alpha 8             --lora_dropout 0.1
Llama 3.2 3Bモデル: meta-llama/Llama-3.2-3B-Instruct
データ: ../jsonl/train_003.jsonl
出力: ../outputs/llama32-3b-typst-qlora-003-004
学習率: 1e-05
エポック数: 12
バッチサイズ: 1
勾配累積: 16
LoRA rank: 8
LoRA alpha: 8
LoRA dropout: 0.1
==================================================
トークナイザーを読み込み中...
4bit量子化設定を準備中...
モデルを読み込み中...

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.93s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.72s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.90s/it]
/mnt/d/github/typst-tex-converter/venv/lib/python3.11/site-packages/peft/tuners/lora/bnb.py:348: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.
  warnings.warn(
/mnt/d/github/typst-tex-converter/venv/lib/python3.11/site-packages/peft/tuners/tuners_utils.py:196: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!
  warnings.warn(
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 128009}.
既存のPEFTモデルを読み込み中: ../outputs/llama32-3b-typst-qlora-003-003
データセットを読み込み中...
データセットサイズ: 43
Llama 3.2 3B QLoRA学習を開始します...

  0%|          | 0/36 [00:00<?, ?it/s]
  3%|▎         | 1/36 [00:09<05:23,  9.25s/it]
                                              

  3%|▎         | 1/36 [00:09<05:23,  9.25s/it]
  6%|▌         | 2/36 [00:35<11:03, 19.52s/it]
                                              

  6%|▌         | 2/36 [00:35<11:03, 19.52s/it]
  8%|▊         | 3/36 [00:44<08:00, 14.55s/it]
                                              

  8%|▊         | 3/36 [00:44<08:00, 14.55s/it]
 11%|█         | 4/36 [00:57<07:22, 13.83s/it]
                                              

 11%|█         | 4/36 [00:57<07:22, 13.83s/it]
 14%|█▍        | 5/36 [01:21<08:59, 17.40s/it]
                                              

 14%|█▍        | 5/36 [01:21<08:59, 17.40s/it]
 17%|█▋        | 6/36 [01:38<08:38, 17.27s/it]
                                              

 17%|█▋        | 6/36 [01:38<08:38, 17.27s/it]
 19%|█▉        | 7/36 [01:53<08:07, 16.80s/it]
                                              

 19%|█▉        | 7/36 [01:53<08:07, 16.80s/it]
 22%|██▏       | 8/36 [02:11<07:56, 17.02s/it]
                                              

 22%|██▏       | 8/36 [02:11<07:56, 17.02s/it]
 25%|██▌       | 9/36 [02:32<08:10, 18.15s/it]
                                              

 25%|██▌       | 9/36 [02:32<08:10, 18.15s/it]
 28%|██▊       | 10/36 [02:55<08:32, 19.72s/it]
                                               

 28%|██▊       | 10/36 [02:55<08:32, 19.72s/it]
 31%|███       | 11/36 [03:11<07:45, 18.62s/it]
                                               

 31%|███       | 11/36 [03:11<07:45, 18.62s/it]
 33%|███▎      | 12/36 [03:25<06:50, 17.11s/it]
                                               

 33%|███▎      | 12/36 [03:25<06:50, 17.11s/it]
 36%|███▌      | 13/36 [03:51<07:35, 19.80s/it]
                                               

 36%|███▌      | 13/36 [03:51<07:35, 19.80s/it]
 39%|███▉      | 14/36 [04:06<06:50, 18.64s/it]
                                               

 39%|███▉      | 14/36 [04:06<06:50, 18.64s/it]
 42%|████▏     | 15/36 [04:19<05:49, 16.66s/it]
                                               

 42%|████▏     | 15/36 [04:19<05:49, 16.66s/it]
 44%|████▍     | 16/36 [04:30<05:01, 15.09s/it]
                                               

 44%|████▍     | 16/36 [04:30<05:01, 15.09s/it]
 47%|████▋     | 17/36 [05:03<06:28, 20.46s/it]
                                               

 47%|████▋     | 17/36 [05:03<06:28, 20.46s/it]
 50%|█████     | 18/36 [05:12<05:05, 16.99s/it]
                                               

 50%|█████     | 18/36 [05:12<05:05, 16.99s/it]
 53%|█████▎    | 19/36 [05:34<05:12, 18.40s/it]
                                               

 53%|█████▎    | 19/36 [05:34<05:12, 18.40s/it]
 56%|█████▌    | 20/36 [05:55<05:06, 19.17s/it]
                                               

 56%|█████▌    | 20/36 [05:55<05:06, 19.17s/it]
 58%|█████▊    | 21/36 [06:04<04:05, 16.36s/it]
                                               

 58%|█████▊    | 21/36 [06:04<04:05, 16.36s/it]
 61%|██████    | 22/36 [06:26<04:09, 17.85s/it]
                                               

 61%|██████    | 22/36 [06:26<04:09, 17.85s/it]
 64%|██████▍   | 23/36 [06:48<04:10, 19.25s/it]
                                               

 64%|██████▍   | 23/36 [06:48<04:10, 19.25s/it]
 67%|██████▋   | 24/36 [06:58<03:15, 16.28s/it]
                                               

 67%|██████▋   | 24/36 [06:58<03:15, 16.28s/it]
 69%|██████▉   | 25/36 [07:13<02:55, 15.97s/it]
                                               

 69%|██████▉   | 25/36 [07:13<02:55, 15.97s/it]
 72%|███████▏  | 26/36 [07:31<02:46, 16.61s/it]
                                               

 72%|███████▏  | 26/36 [07:31<02:46, 16.61s/it]
 75%|███████▌  | 27/36 [07:50<02:36, 17.40s/it]
                                               

 75%|███████▌  | 27/36 [07:50<02:36, 17.40s/it]
 78%|███████▊  | 28/36 [08:17<02:41, 20.15s/it]
                                               

 78%|███████▊  | 28/36 [08:17<02:41, 20.15s/it]
 81%|████████  | 29/36 [08:33<02:14, 19.15s/it]
                                               

 81%|████████  | 29/36 [08:33<02:14, 19.15s/it]
 83%|████████▎ | 30/36 [08:43<01:38, 16.39s/it]
                                               

 83%|████████▎ | 30/36 [08:43<01:38, 16.39s/it]
 86%|████████▌ | 31/36 [08:56<01:16, 15.22s/it]
                                               

 86%|████████▌ | 31/36 [08:56<01:16, 15.22s/it]
 89%|████████▉ | 32/36 [09:20<01:12, 18.02s/it]
                                               

 89%|████████▉ | 32/36 [09:20<01:12, 18.02s/it]
 92%|█████████▏| 33/36 [09:37<00:52, 17.49s/it]
                                               

 92%|█████████▏| 33/36 [09:37<00:52, 17.49s/it]
 94%|█████████▍| 34/36 [09:50<00:32, 16.10s/it]
                                               

 94%|█████████▍| 34/36 [09:50<00:32, 16.10s/it]
 97%|█████████▋| 35/36 [10:07<00:16, 16.50s/it]
                                               

 97%|█████████▋| 35/36 [10:07<00:16, 16.50s/it]
100%|██████████| 36/36 [10:30<00:00, 18.30s/it]
                                               

100%|██████████| 36/36 [10:30<00:00, 18.30s/it]
                                               

100%|██████████| 36/36 [10:31<00:00, 18.30s/it]
100%|██████████| 36/36 [10:31<00:00, 17.55s/it]
{'loss': 1.0019, 'grad_norm': 2.3068974018096924, 'learning_rate': 0.0, 'entropy': 1.6676861234009266, 'num_tokens': 1968.0, 'mean_token_accuracy': 0.8097846880555153, 'epoch': 0.37}
{'loss': 1.045, 'grad_norm': 2.3416287899017334, 'learning_rate': 2.5e-06, 'entropy': 1.3881262131035328, 'num_tokens': 5029.0, 'mean_token_accuracy': 0.8093724586069584, 'epoch': 0.74}
{'loss': 1.097, 'grad_norm': 2.9021127223968506, 'learning_rate': 5e-06, 'entropy': 1.5273766842755405, 'num_tokens': 6318.0, 'mean_token_accuracy': 0.8274629007686268, 'epoch': 1.0}
{'loss': 1.2103, 'grad_norm': 3.1434326171875, 'learning_rate': 7.500000000000001e-06, 'entropy': 1.6615911945700645, 'num_tokens': 8014.0, 'mean_token_accuracy': 0.7986961454153061, 'epoch': 1.37}
{'loss': 1.0112, 'grad_norm': 2.0979325771331787, 'learning_rate': 1e-05, 'entropy': 1.3551450483500957, 'num_tokens': 11175.0, 'mean_token_accuracy': 0.8248906023800373, 'epoch': 1.74}
{'loss': 0.8326, 'grad_norm': 2.0658507347106934, 'learning_rate': 9.975923633360985e-06, 'entropy': 1.58822809566151, 'num_tokens': 12636.0, 'mean_token_accuracy': 0.8517454103990034, 'epoch': 2.0}
{'loss': 0.9869, 'grad_norm': 2.22883939743042, 'learning_rate': 9.903926402016153e-06, 'entropy': 1.5208427868783474, 'num_tokens': 14723.0, 'mean_token_accuracy': 0.839100543409586, 'epoch': 2.37}
{'loss': 1.0355, 'grad_norm': 2.6847474575042725, 'learning_rate': 9.784701678661045e-06, 'entropy': 1.51134667173028, 'num_tokens': 17027.0, 'mean_token_accuracy': 0.8221327364444733, 'epoch': 2.74}
{'loss': 0.8287, 'grad_norm': 1.5327379703521729, 'learning_rate': 9.619397662556434e-06, 'entropy': 1.5797650488940151, 'num_tokens': 18954.0, 'mean_token_accuracy': 0.8232823339375582, 'epoch': 3.0}
{'loss': 0.9047, 'grad_norm': 1.857731819152832, 'learning_rate': 9.409606321741776e-06, 'entropy': 1.5173694603145123, 'num_tokens': 21674.0, 'mean_token_accuracy': 0.8265761323273182, 'epoch': 3.37}
{'loss': 0.8403, 'grad_norm': 2.2449872493743896, 'learning_rate': 9.157348061512728e-06, 'entropy': 1.6338846273720264, 'num_tokens': 23385.0, 'mean_token_accuracy': 0.8478846177458763, 'epoch': 3.74}
{'loss': 0.9171, 'grad_norm': 2.162449359893799, 'learning_rate': 8.865052266813686e-06, 'entropy': 1.4203600829297847, 'num_tokens': 25272.0, 'mean_token_accuracy': 0.8264588605273854, 'epoch': 4.0}
{'loss': 0.88, 'grad_norm': 1.8204199075698853, 'learning_rate': 8.535533905932739e-06, 'entropy': 1.4500420279800892, 'num_tokens': 28245.0, 'mean_token_accuracy': 0.8233850933611393, 'epoch': 4.37}
{'loss': 0.8432, 'grad_norm': 2.259028434753418, 'learning_rate': 8.171966420818227e-06, 'entropy': 1.5782388485968113, 'num_tokens': 30283.0, 'mean_token_accuracy': 0.8271992728114128, 'epoch': 4.74}
{'loss': 0.7479, 'grad_norm': 2.080070972442627, 'learning_rate': 7.777851165098012e-06, 'entropy': 1.613023037260229, 'num_tokens': 31590.0, 'mean_token_accuracy': 0.8610499772158536, 'epoch': 5.0}
{'loss': 0.875, 'grad_norm': 2.467240571975708, 'learning_rate': 7.3569836841299905e-06, 'entropy': 1.6345458924770355, 'num_tokens': 33360.0, 'mean_token_accuracy': 0.8238443247973919, 'epoch': 5.37}
{'loss': 0.7469, 'grad_norm': 1.553347110748291, 'learning_rate': 6.913417161825449e-06, 'entropy': 1.416241329163313, 'num_tokens': 36519.0, 'mean_token_accuracy': 0.8497111350297928, 'epoch': 5.74}
{'loss': 0.8061, 'grad_norm': 2.3499088287353516, 'learning_rate': 6.451423386272312e-06, 'entropy': 1.5946863998066296, 'num_tokens': 37908.0, 'mean_token_accuracy': 0.8347415707328103, 'epoch': 6.0}
{'loss': 0.7342, 'grad_norm': 1.8013039827346802, 'learning_rate': 5.975451610080643e-06, 'entropy': 1.4823154918849468, 'num_tokens': 40619.0, 'mean_token_accuracy': 0.8378305993974209, 'epoch': 6.37}
{'loss': 0.7747, 'grad_norm': 1.9777101278305054, 'learning_rate': 5.490085701647805e-06, 'entropy': 1.5504143610596657, 'num_tokens': 43042.0, 'mean_token_accuracy': 0.8567813038825989, 'epoch': 6.74}
{'loss': 0.7467, 'grad_norm': 2.289457082748413, 'learning_rate': 5e-06, 'entropy': 1.6311028328808872, 'num_tokens': 44226.0, 'mean_token_accuracy': 0.8438582691279325, 'epoch': 7.0}
{'loss': 0.6604, 'grad_norm': 1.6777324676513672, 'learning_rate': 4.509914298352197e-06, 'entropy': 1.5323438607156277, 'num_tokens': 46461.0, 'mean_token_accuracy': 0.8868236690759659, 'epoch': 7.37}
{'loss': 0.7272, 'grad_norm': 1.626431941986084, 'learning_rate': 4.02454838991936e-06, 'entropy': 1.5362535119056702, 'num_tokens': 49072.0, 'mean_token_accuracy': 0.849740494042635, 'epoch': 7.74}
{'loss': 0.8022, 'grad_norm': 2.434525966644287, 'learning_rate': 3.5485766137276894e-06, 'entropy': 1.586422009901567, 'num_tokens': 50544.0, 'mean_token_accuracy': 0.8151074810461565, 'epoch': 8.0}
{'loss': 0.7533, 'grad_norm': 1.9368940591812134, 'learning_rate': 3.0865828381745515e-06, 'entropy': 1.4476257152855396, 'num_tokens': 53147.0, 'mean_token_accuracy': 0.8304522782564163, 'epoch': 8.37}
{'loss': 0.6934, 'grad_norm': 1.795292615890503, 'learning_rate': 2.6430163158700116e-06, 'entropy': 1.604564182460308, 'num_tokens': 55190.0, 'mean_token_accuracy': 0.8688659891486168, 'epoch': 8.74}
{'loss': 0.6212, 'grad_norm': 1.6417334079742432, 'learning_rate': 2.2221488349019903e-06, 'entropy': 1.6127832640301099, 'num_tokens': 56862.0, 'mean_token_accuracy': 0.8961980397051031, 'epoch': 9.0}
{'loss': 0.6379, 'grad_norm': 1.5426075458526611, 'learning_rate': 1.8280335791817733e-06, 'entropy': 1.5530737936496735, 'num_tokens': 59543.0, 'mean_token_accuracy': 0.8657520264387131, 'epoch': 9.37}
{'loss': 0.7107, 'grad_norm': 2.0210888385772705, 'learning_rate': 1.4644660940672628e-06, 'entropy': 1.594536792486906, 'num_tokens': 61573.0, 'mean_token_accuracy': 0.8532840460538864, 'epoch': 9.74}
{'loss': 0.7431, 'grad_norm': 1.956520915031433, 'learning_rate': 1.134947733186315e-06, 'entropy': 1.4753011952746997, 'num_tokens': 63180.0, 'mean_token_accuracy': 0.874210926619443, 'epoch': 10.0}
{'loss': 0.7748, 'grad_norm': 2.0813040733337402, 'learning_rate': 8.426519384872733e-07, 'entropy': 1.5503505766391754, 'num_tokens': 65326.0, 'mean_token_accuracy': 0.8684266731142998, 'epoch': 10.37}
{'loss': 0.6108, 'grad_norm': 1.5283970832824707, 'learning_rate': 5.903936782582253e-07, 'entropy': 1.488686054944992, 'num_tokens': 67813.0, 'mean_token_accuracy': 0.8656345643103123, 'epoch': 10.74}
{'loss': 0.6697, 'grad_norm': 1.5679937601089478, 'learning_rate': 3.8060233744356634e-07, 'entropy': 1.6306933164596558, 'num_tokens': 69498.0, 'mean_token_accuracy': 0.8598770986903798, 'epoch': 11.0}
{'loss': 0.7007, 'grad_norm': 1.9287660121917725, 'learning_rate': 2.152983213389559e-07, 'entropy': 1.5962566509842873, 'num_tokens': 71308.0, 'mean_token_accuracy': 0.8598572909832001, 'epoch': 11.37}
{'loss': 0.6788, 'grad_norm': 1.8448803424835205, 'learning_rate': 9.607359798384785e-08, 'entropy': 1.656359013170004, 'num_tokens': 73415.0, 'mean_token_accuracy': 0.8862172104418278, 'epoch': 11.74}
{'loss': 0.6579, 'grad_norm': 1.6474968194961548, 'learning_rate': 2.4076366639015914e-08, 'entropy': 1.3237988027659329, 'num_tokens': 75816.0, 'mean_token_accuracy': 0.8395405357534235, 'epoch': 12.0}
{'train_runtime': 631.7285, 'train_samples_per_second': 0.817, 'train_steps_per_second': 0.057, 'train_loss': 0.8141101549069086, 'epoch': 12.0}
モデルを保存中...
Llama 3.2 3B QLoRA学習完了！モデルは ../outputs/llama32-3b-typst-qlora-003-004 に保存されました。
テスト実行: ../outputs/llama32-3b-typst-qlora-003-004
テストログ: ../logs/003-004_test_20250917_013643.log
実行中: python3 inference_llama32_3b.py                 --peft_model_path ../outputs/llama32-3b-typst-qlora-003-004                 --input_file ../sample/sample_small.tex                 --output_file ../trained/003-004.md
ベースモデル: meta-llama/Llama-3.2-3B-Instruct
✅ 使用モデル: PEFT（LoRA適用済み）
   PEFTモデル: ../outputs/llama32-3b-typst-qlora-003-004
入力ファイル: ../sample/sample_small.tex
出力ファイル: ../trained/003-004.md
トークナイザーを読み込み中...
4bit量子化設定を準備中...
ベースモデルを読み込み中...

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.89s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.69s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.87s/it]
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
✅ PEFTモデルを読み込み中...
   PEFTパス: ../outputs/llama32-3b-typst-qlora-003-004
   PEFTモデル読み込み完了
LaTeXファイルを読み込み中...
LaTeXテキストを 1 チャンクに分割しました
LaTeX→Typst変換を開始します...
チャンク 1/1 を変換中...
  入力テキスト長: 865 文字
  チャットテンプレート適用成功
  プロンプト長: 1223 文字
  入力トークン数: 415
  生成開始...
  生成完了: 615 トークン
  生成テキスト長: 1706 文字
  プロンプト長: 1223 文字
  生成テキスト開始: system

Cutting Knowledge Date: December 2023
Today Date: 17 Sep 2025

LaTeX 形式の文章を Typst形式 へ変換してください.
重要: TypstはLaTeXの代替となる文書組版システムです。TypeScript（プログラミング言語）ではありません。
出力はTypst構文のみ。プログラミングコードは出力しません。user...
  生成テキスト終了: ...left(u_0, u_1\right) = \left(\varepsilon_0 \varphi, \varepsilon_1 \varphi\right),$$

where $0 < \varepsilon_0 < |\varepsilon_1| = -\varepsilon_1$ and $\varphi$ is a regular non-negative function.

```
  生成テキスト全体の長さ: 1706 文字
  プロンプトの長さ: 1223 文字
  方法1（全体使用）: 1706 文字
  方法3（```typstマーカー）: 629 文字
  方法3（```除去後）: 624 文字
  最終応答: system

Cutting Knowledge Date: December 2023
Today Date: 17 Sep 2025

LaTeX 形式の文章を Typst形式 へ変換してください.
重要: TypstはLaTeXの代替となる文書組版システムです。TypeScript（プログラミング言語）ではありません。
出力はTypst構文のみ。プログラミングコードは出力しません。user...
  最終応答の最後: ...epsilon_0 < |\varepsilon_1| = -\varepsilon_1$ and $\varphi$ is a regular non-negative function.

```
  抽出された応答: system

Cutting Knowledge Date: December 2023
Today Date: 17 Sep 2025

LaTeX 形式の文章を Typst形式 へ変換してくださ...
結果を ../trained/003-004.md に保存中...
変換完了！
完了: Iteration 004 の学習・テストが完了しました。
チェックポイントを更新: ../outputs/llama32-3b-typst-qlora-003-004
=== Iteration 004 の処理完了 ===
--------------------------------------------------
=== Iteration 005 の処理開始 ===
継続学習: ../jsonl/train_003.jsonl (from ../outputs/llama32-3b-typst-qlora-003-004)
学習ログ: ../logs/003-005_training_20250917_013643.log
実行中: python3 train_llama32_3b_qlora_fixed.py             --data ../jsonl/train_003.jsonl             --out ../outputs/llama32-3b-typst-qlora-003-005             --peft_model_path ../outputs/llama32-3b-typst-qlora-003-004             --epochs 12             --batch_size 1             --grad_accum 16             --learning_rate 1e-05             --lora_r 8             --lora_alpha 8             --lora_dropout 0.1
Llama 3.2 3Bモデル: meta-llama/Llama-3.2-3B-Instruct
データ: ../jsonl/train_003.jsonl
出力: ../outputs/llama32-3b-typst-qlora-003-005
学習率: 1e-05
エポック数: 12
バッチサイズ: 1
勾配累積: 16
LoRA rank: 8
LoRA alpha: 8
LoRA dropout: 0.1
==================================================
トークナイザーを読み込み中...
4bit量子化設定を準備中...
モデルを読み込み中...

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.83s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.69s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.86s/it]
/mnt/d/github/typst-tex-converter/venv/lib/python3.11/site-packages/peft/tuners/lora/bnb.py:348: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.
  warnings.warn(
/mnt/d/github/typst-tex-converter/venv/lib/python3.11/site-packages/peft/tuners/tuners_utils.py:196: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!
  warnings.warn(
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 128009}.
既存のPEFTモデルを読み込み中: ../outputs/llama32-3b-typst-qlora-003-004
データセットを読み込み中...
データセットサイズ: 43
Llama 3.2 3B QLoRA学習を開始します...

  0%|          | 0/36 [00:00<?, ?it/s]
  3%|▎         | 1/36 [00:09<05:29,  9.40s/it]
                                              

  3%|▎         | 1/36 [00:09<05:29,  9.40s/it]
  6%|▌         | 2/36 [00:36<11:15, 19.87s/it]
                                              

  6%|▌         | 2/36 [00:36<11:15, 19.87s/it]
  8%|▊         | 3/36 [00:45<08:14, 14.98s/it]
                                              

  8%|▊         | 3/36 [00:45<08:14, 14.98s/it]
 11%|█         | 4/36 [00:59<07:43, 14.49s/it]
                                              

 11%|█         | 4/36 [00:59<07:43, 14.49s/it]
 14%|█▍        | 5/36 [01:23<09:19, 18.05s/it]
                                              

 14%|█▍        | 5/36 [01:23<09:19, 18.05s/it]
 17%|█▋        | 6/36 [01:41<08:56, 17.88s/it]
                                              

 17%|█▋        | 6/36 [01:41<08:56, 17.88s/it]
 19%|█▉        | 7/36 [01:56<08:15, 17.10s/it]
                                              

 19%|█▉        | 7/36 [01:56<08:15, 17.10s/it]
 22%|██▏       | 8/36 [02:15<08:10, 17.51s/it]
                                              

 22%|██▏       | 8/36 [02:15<08:10, 17.51s/it]
 25%|██▌       | 9/36 [02:36<08:24, 18.67s/it]
                                              

 25%|██▌       | 9/36 [02:36<08:24, 18.67s/it]
 28%|██▊       | 10/36 [03:00<08:48, 20.32s/it]
                                               

 28%|██▊       | 10/36 [03:00<08:48, 20.32s/it]
 31%|███       | 11/36 [03:17<08:02, 19.28s/it]
                                               

 31%|███       | 11/36 [03:17<08:02, 19.28s/it]
 33%|███▎      | 12/36 [03:31<07:03, 17.66s/it]
                                               

 33%|███▎      | 12/36 [03:31<07:03, 17.66s/it]
 36%|███▌      | 13/36 [03:57<07:44, 20.21s/it]
                                               

 36%|███▌      | 13/36 [03:57<07:44, 20.21s/it]
 39%|███▉      | 14/36 [04:14<07:02, 19.19s/it]
                                               

 39%|███▉      | 14/36 [04:14<07:02, 19.19s/it]
 42%|████▏     | 15/36 [04:26<06:00, 17.17s/it]
                                               

 42%|████▏     | 15/36 [04:26<06:00, 17.17s/it]
 44%|████▍     | 16/36 [04:38<05:12, 15.65s/it]
                                               

 44%|████▍     | 16/36 [04:38<05:12, 15.65s/it]
 47%|████▋     | 17/36 [05:12<06:38, 20.96s/it]
                                               

 47%|████▋     | 17/36 [05:12<06:38, 20.96s/it]
 50%|█████     | 18/36 [05:21<05:13, 17.39s/it]
                                               

 50%|█████     | 18/36 [05:21<05:13, 17.39s/it]
 53%|█████▎    | 19/36 [05:43<05:20, 18.83s/it]
                                               

 53%|█████▎    | 19/36 [05:43<05:20, 18.83s/it]
 56%|█████▌    | 20/36 [06:05<05:14, 19.64s/it]
                                               

 56%|█████▌    | 20/36 [06:05<05:14, 19.64s/it]
 58%|█████▊    | 21/36 [06:15<04:12, 16.83s/it]
                                               

 58%|█████▊    | 21/36 [06:15<04:12, 16.83s/it]
 61%|██████    | 22/36 [06:37<04:16, 18.34s/it]
                                               

 61%|██████    | 22/36 [06:37<04:16, 18.34s/it]
 64%|██████▍   | 23/36 [07:00<04:16, 19.73s/it]
                                               

 64%|██████▍   | 23/36 [07:00<04:16, 19.73s/it]
 67%|██████▋   | 24/36 [07:10<03:22, 16.89s/it]
                                               

 67%|██████▋   | 24/36 [07:10<03:22, 16.89s/it]
 69%|██████▉   | 25/36 [07:26<03:02, 16.56s/it]
                                               

 69%|██████▉   | 25/36 [07:26<03:02, 16.56s/it]
 72%|███████▏  | 26/36 [07:44<02:51, 17.19s/it]
                                               

 72%|███████▏  | 26/36 [07:44<02:51, 17.19s/it]
 75%|███████▌  | 27/36 [08:04<02:41, 17.96s/it]
                                               

 75%|███████▌  | 27/36 [08:04<02:41, 17.96s/it]
 78%|███████▊  | 28/36 [08:31<02:44, 20.59s/it]
                                               

 78%|███████▊  | 28/36 [08:31<02:44, 20.59s/it]
 81%|████████  | 29/36 [08:49<02:18, 19.73s/it]
                                               

 81%|████████  | 29/36 [08:49<02:18, 19.73s/it]
 83%|████████▎ | 30/36 [08:59<01:41, 16.86s/it]
                                               

 83%|████████▎ | 30/36 [08:59<01:41, 16.86s/it]
 86%|████████▌ | 31/36 [09:12<01:18, 15.71s/it]
                                               

 86%|████████▌ | 31/36 [09:12<01:18, 15.71s/it]
 89%|████████▉ | 32/36 [09:37<01:14, 18.59s/it]
                                               

 89%|████████▉ | 32/36 [09:37<01:14, 18.59s/it]
 92%|█████████▏| 33/36 [09:54<00:54, 18.06s/it]
                                               

 92%|█████████▏| 33/36 [09:54<00:54, 18.06s/it]
 94%|█████████▍| 34/36 [10:07<00:33, 16.68s/it]
                                               

 94%|█████████▍| 34/36 [10:07<00:33, 16.68s/it]
 97%|█████████▋| 35/36 [10:25<00:17, 17.09s/it]
                                               

 97%|█████████▋| 35/36 [10:25<00:17, 17.09s/it]
100%|██████████| 36/36 [10:48<00:00, 18.88s/it]
                                               

100%|██████████| 36/36 [10:48<00:00, 18.88s/it]
                                               

100%|██████████| 36/36 [10:50<00:00, 18.88s/it]
100%|██████████| 36/36 [10:50<00:00, 18.08s/it]
{'loss': 1.0024, 'grad_norm': 2.2408266067504883, 'learning_rate': 0.0, 'entropy': 1.6676786430180073, 'num_tokens': 1968.0, 'mean_token_accuracy': 0.8097846880555153, 'epoch': 0.37}
{'loss': 1.0455, 'grad_norm': 2.260908365249634, 'learning_rate': 2.5e-06, 'entropy': 1.3880700580775738, 'num_tokens': 5029.0, 'mean_token_accuracy': 0.8093724586069584, 'epoch': 0.74}
{'loss': 1.0971, 'grad_norm': 2.8109824657440186, 'learning_rate': 5e-06, 'entropy': 1.5270442745902322, 'num_tokens': 6318.0, 'mean_token_accuracy': 0.8274629007686268, 'epoch': 1.0}
{'loss': 1.211, 'grad_norm': 3.058898687362671, 'learning_rate': 7.500000000000001e-06, 'entropy': 1.6617706082761288, 'num_tokens': 8014.0, 'mean_token_accuracy': 0.7986961454153061, 'epoch': 1.37}
{'loss': 1.0123, 'grad_norm': 2.031210422515869, 'learning_rate': 1e-05, 'entropy': 1.3548519872128963, 'num_tokens': 11175.0, 'mean_token_accuracy': 0.8236325532197952, 'epoch': 1.74}
{'loss': 0.8337, 'grad_norm': 1.992227554321289, 'learning_rate': 9.975923633360985e-06, 'entropy': 1.5882316394285723, 'num_tokens': 12636.0, 'mean_token_accuracy': 0.8517454103990034, 'epoch': 2.0}
{'loss': 0.9883, 'grad_norm': 2.1655938625335693, 'learning_rate': 9.903926402016153e-06, 'entropy': 1.5204171910881996, 'num_tokens': 14723.0, 'mean_token_accuracy': 0.8382443785667419, 'epoch': 2.37}
{'loss': 1.0378, 'grad_norm': 2.6072213649749756, 'learning_rate': 9.784701678661045e-06, 'entropy': 1.510484166443348, 'num_tokens': 17027.0, 'mean_token_accuracy': 0.8221327364444733, 'epoch': 2.74}
{'loss': 0.8302, 'grad_norm': 1.4921296834945679, 'learning_rate': 9.619397662556434e-06, 'entropy': 1.5787624608386646, 'num_tokens': 18954.0, 'mean_token_accuracy': 0.8229299729520624, 'epoch': 3.0}
{'loss': 0.9068, 'grad_norm': 1.8002395629882812, 'learning_rate': 9.409606321741776e-06, 'entropy': 1.5162446051836014, 'num_tokens': 21674.0, 'mean_token_accuracy': 0.8265761323273182, 'epoch': 3.37}
{'loss': 0.8422, 'grad_norm': 2.1845009326934814, 'learning_rate': 9.157348061512728e-06, 'entropy': 1.6329066269099712, 'num_tokens': 23385.0, 'mean_token_accuracy': 0.848803736269474, 'epoch': 3.74}
{'loss': 0.9208, 'grad_norm': 2.0834195613861084, 'learning_rate': 8.865052266813686e-06, 'entropy': 1.4190756461837075, 'num_tokens': 25272.0, 'mean_token_accuracy': 0.8264588605273854, 'epoch': 4.0}
{'loss': 0.8829, 'grad_norm': 1.7680988311767578, 'learning_rate': 8.535533905932739e-06, 'entropy': 1.4485030882060528, 'num_tokens': 28245.0, 'mean_token_accuracy': 0.8233850933611393, 'epoch': 4.37}
{'loss': 0.8463, 'grad_norm': 2.1995351314544678, 'learning_rate': 8.171966420818227e-06, 'entropy': 1.5768573619425297, 'num_tokens': 30283.0, 'mean_token_accuracy': 0.8271992728114128, 'epoch': 4.74}
{'loss': 0.7498, 'grad_norm': 2.02677583694458, 'learning_rate': 7.777851165098012e-06, 'entropy': 1.6116872429847717, 'num_tokens': 31590.0, 'mean_token_accuracy': 0.8619325919584795, 'epoch': 5.0}
{'loss': 0.8785, 'grad_norm': 2.4038071632385254, 'learning_rate': 7.3569836841299905e-06, 'entropy': 1.6329042688012123, 'num_tokens': 33360.0, 'mean_token_accuracy': 0.8332529291510582, 'epoch': 5.37}
{'loss': 0.7496, 'grad_norm': 1.5111583471298218, 'learning_rate': 6.913417161825449e-06, 'entropy': 1.4149156771600246, 'num_tokens': 36519.0, 'mean_token_accuracy': 0.8497111350297928, 'epoch': 5.74}
{'loss': 0.8089, 'grad_norm': 2.3037641048431396, 'learning_rate': 6.451423386272312e-06, 'entropy': 1.593189922246066, 'num_tokens': 37908.0, 'mean_token_accuracy': 0.8347415707328103, 'epoch': 6.0}
{'loss': 0.7376, 'grad_norm': 1.7488080263137817, 'learning_rate': 5.975451610080643e-06, 'entropy': 1.480488359928131, 'num_tokens': 40619.0, 'mean_token_accuracy': 0.8378305993974209, 'epoch': 6.37}
{'loss': 0.7777, 'grad_norm': 1.9270163774490356, 'learning_rate': 5.490085701647805e-06, 'entropy': 1.5486065670847893, 'num_tokens': 43042.0, 'mean_token_accuracy': 0.8567813038825989, 'epoch': 6.74}
{'loss': 0.7485, 'grad_norm': 2.231011152267456, 'learning_rate': 5e-06, 'entropy': 1.6293200037696145, 'num_tokens': 44226.0, 'mean_token_accuracy': 0.8438582691279325, 'epoch': 7.0}
{'loss': 0.6623, 'grad_norm': 1.6318272352218628, 'learning_rate': 4.509914298352197e-06, 'entropy': 1.5304477959871292, 'num_tokens': 46461.0, 'mean_token_accuracy': 0.8868236690759659, 'epoch': 7.37}
{'loss': 0.7301, 'grad_norm': 1.5810648202896118, 'learning_rate': 4.02454838991936e-06, 'entropy': 1.5341791734099388, 'num_tokens': 49072.0, 'mean_token_accuracy': 0.8503848239779472, 'epoch': 7.74}
{'loss': 0.8047, 'grad_norm': 2.375668525695801, 'learning_rate': 3.5485766137276894e-06, 'entropy': 1.5845421661030163, 'num_tokens': 50544.0, 'mean_token_accuracy': 0.8205744353207675, 'epoch': 8.0}
{'loss': 0.7565, 'grad_norm': 1.885187029838562, 'learning_rate': 3.0865828381745515e-06, 'entropy': 1.4453745819628239, 'num_tokens': 53147.0, 'mean_token_accuracy': 0.8304522782564163, 'epoch': 8.37}
{'loss': 0.6953, 'grad_norm': 1.7447463274002075, 'learning_rate': 2.6430163158700116e-06, 'entropy': 1.602398570626974, 'num_tokens': 55190.0, 'mean_token_accuracy': 0.8688659891486168, 'epoch': 8.74}
{'loss': 0.6237, 'grad_norm': 1.5974514484405518, 'learning_rate': 2.2221488349019903e-06, 'entropy': 1.6109875386411494, 'num_tokens': 56862.0, 'mean_token_accuracy': 0.8961980397051031, 'epoch': 9.0}
{'loss': 0.6403, 'grad_norm': 1.4976556301116943, 'learning_rate': 1.8280335791817733e-06, 'entropy': 1.5514205135405064, 'num_tokens': 59543.0, 'mean_token_accuracy': 0.8687282167375088, 'epoch': 9.37}
{'loss': 0.7129, 'grad_norm': 1.9899498224258423, 'learning_rate': 1.4644660940672628e-06, 'entropy': 1.5922786518931389, 'num_tokens': 61573.0, 'mean_token_accuracy': 0.8532840460538864, 'epoch': 9.74}
{'loss': 0.7457, 'grad_norm': 1.8890230655670166, 'learning_rate': 1.134947733186315e-06, 'entropy': 1.4729578440839595, 'num_tokens': 63180.0, 'mean_token_accuracy': 0.8758058222857389, 'epoch': 10.0}
{'loss': 0.777, 'grad_norm': 2.0209665298461914, 'learning_rate': 8.426519384872733e-07, 'entropy': 1.5477826595306396, 'num_tokens': 65326.0, 'mean_token_accuracy': 0.8693882115185261, 'epoch': 10.37}
{'loss': 0.6135, 'grad_norm': 1.4951777458190918, 'learning_rate': 5.903936782582253e-07, 'entropy': 1.4865837171673775, 'num_tokens': 67813.0, 'mean_token_accuracy': 0.8656345643103123, 'epoch': 10.74}
{'loss': 0.671, 'grad_norm': 1.531442403793335, 'learning_rate': 3.8060233744356634e-07, 'entropy': 1.6295182650739497, 'num_tokens': 69498.0, 'mean_token_accuracy': 0.8595247377048839, 'epoch': 11.0}
{'loss': 0.7029, 'grad_norm': 1.8680952787399292, 'learning_rate': 2.152983213389559e-07, 'entropy': 1.5938606932759285, 'num_tokens': 71308.0, 'mean_token_accuracy': 0.8598572909832001, 'epoch': 11.37}
{'loss': 0.6819, 'grad_norm': 1.797411322593689, 'learning_rate': 9.607359798384785e-08, 'entropy': 1.6543576382100582, 'num_tokens': 73415.0, 'mean_token_accuracy': 0.8859284333884716, 'epoch': 11.74}
{'loss': 0.6611, 'grad_norm': 1.5971732139587402, 'learning_rate': 2.4076366639015914e-08, 'entropy': 1.3212570439685474, 'num_tokens': 75816.0, 'mean_token_accuracy': 0.8405286778103221, 'epoch': 12.0}
{'train_runtime': 650.7492, 'train_samples_per_second': 0.793, 'train_steps_per_second': 0.055, 'train_loss': 0.8163048658106062, 'epoch': 12.0}
モデルを保存中...
Llama 3.2 3B QLoRA学習完了！モデルは ../outputs/llama32-3b-typst-qlora-003-005 に保存されました。
スキップ: Iteration 005 のテストをスキップしました（2回に1回の実行）
チェックポイントを更新: ../outputs/llama32-3b-typst-qlora-003-005
=== Iteration 005 の処理完了 ===
--------------------------------------------------
=== Iteration 006 の処理開始 ===
継続学習: ../jsonl/train_003.jsonl (from ../outputs/llama32-3b-typst-qlora-003-005)
学習ログ: ../logs/003-006_training_20250917_013643.log
実行中: python3 train_llama32_3b_qlora_fixed.py             --data ../jsonl/train_003.jsonl             --out ../outputs/llama32-3b-typst-qlora-003-006             --peft_model_path ../outputs/llama32-3b-typst-qlora-003-005             --epochs 12             --batch_size 1             --grad_accum 16             --learning_rate 1e-05             --lora_r 8             --lora_alpha 8             --lora_dropout 0.1
Llama 3.2 3Bモデル: meta-llama/Llama-3.2-3B-Instruct
データ: ../jsonl/train_003.jsonl
出力: ../outputs/llama32-3b-typst-qlora-003-006
学習率: 1e-05
エポック数: 12
バッチサイズ: 1
勾配累積: 16
LoRA rank: 8
LoRA alpha: 8
LoRA dropout: 0.1
==================================================
トークナイザーを読み込み中...
4bit量子化設定を準備中...
モデルを読み込み中...

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.80s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.68s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.85s/it]
/mnt/d/github/typst-tex-converter/venv/lib/python3.11/site-packages/peft/tuners/lora/bnb.py:348: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.
  warnings.warn(
/mnt/d/github/typst-tex-converter/venv/lib/python3.11/site-packages/peft/tuners/tuners_utils.py:196: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!
  warnings.warn(
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 128009}.
既存のPEFTモデルを読み込み中: ../outputs/llama32-3b-typst-qlora-003-005
データセットを読み込み中...
データセットサイズ: 43
Llama 3.2 3B QLoRA学習を開始します...

  0%|          | 0/36 [00:00<?, ?it/s]
  3%|▎         | 1/36 [00:09<05:37,  9.63s/it]
                                              

  3%|▎         | 1/36 [00:09<05:37,  9.63s/it]
  6%|▌         | 2/36 [00:36<11:18, 19.94s/it]
                                              

  6%|▌         | 2/36 [00:36<11:18, 19.94s/it]
  8%|▊         | 3/36 [00:45<08:15, 15.02s/it]
                                              

  8%|▊         | 3/36 [00:45<08:15, 15.02s/it]
 11%|█         | 4/36 [00:59<07:38, 14.33s/it]
                                              

 11%|█         | 4/36 [00:59<07:38, 14.33s/it]
 14%|█▍        | 5/36 [01:23<09:16, 17.94s/it]
                                              

 14%|█▍        | 5/36 [01:23<09:16, 17.94s/it]
 17%|█▋        | 6/36 [01:41<08:54, 17.81s/it]
                                              

 17%|█▋        | 6/36 [01:41<08:54, 17.81s/it]
 19%|█▉        | 7/36 [01:56<08:16, 17.11s/it]
                                              

 19%|█▉        | 7/36 [01:56<08:16, 17.11s/it]
 22%|██▏       | 8/36 [02:15<08:11, 17.57s/it]
                                              

 22%|██▏       | 8/36 [02:15<08:11, 17.57s/it]
 25%|██▌       | 9/36 [02:36<08:23, 18.64s/it]
                                              

 25%|██▌       | 9/36 [02:36<08:23, 18.64s/it]
 28%|██▊       | 10/36 [03:00<08:46, 20.24s/it]
                                               

 28%|██▊       | 10/36 [03:00<08:46, 20.24s/it]
 31%|███       | 11/36 [03:16<07:58, 19.13s/it]
                                               

 31%|███       | 11/36 [03:16<07:58, 19.13s/it]
 33%|███▎      | 12/36 [03:30<07:03, 17.63s/it]
                                               

 33%|███▎      | 12/36 [03:30<07:03, 17.63s/it]
 36%|███▌      | 13/36 [03:56<07:43, 20.16s/it]
                                               

 36%|███▌      | 13/36 [03:56<07:43, 20.16s/it]
 39%|███▉      | 14/36 [04:13<07:00, 19.09s/it]
                                               

 39%|███▉      | 14/36 [04:13<07:00, 19.09s/it]
 42%|████▏     | 15/36 [04:26<05:59, 17.10s/it]
                                               

 42%|████▏     | 15/36 [04:26<05:59, 17.10s/it]
 44%|████▍     | 16/36 [04:38<05:12, 15.63s/it]
                                               

 44%|████▍     | 16/36 [04:38<05:12, 15.63s/it]
 47%|████▋     | 17/36 [05:11<06:38, 20.95s/it]
                                               

 47%|████▋     | 17/36 [05:11<06:38, 20.95s/it]
 50%|█████     | 18/36 [05:20<05:14, 17.48s/it]
                                               

 50%|█████     | 18/36 [05:20<05:14, 17.48s/it]
 53%|█████▎    | 19/36 [05:43<05:21, 18.92s/it]
                                               

 53%|█████▎    | 19/36 [05:43<05:21, 18.92s/it]
 56%|█████▌    | 20/36 [06:04<05:14, 19.68s/it]
                                               

 56%|█████▌    | 20/36 [06:04<05:14, 19.68s/it]
 58%|█████▊    | 21/36 [06:15<04:13, 16.92s/it]
                                               

 58%|█████▊    | 21/36 [06:15<04:13, 16.92s/it]
 61%|██████    | 22/36 [06:37<04:21, 18.67s/it]
                                               

 61%|██████    | 22/36 [06:37<04:21, 18.67s/it]
 64%|██████▍   | 23/36 [07:01<04:19, 19.98s/it]
                                               

 64%|██████▍   | 23/36 [07:01<04:19, 19.98s/it]
 67%|██████▋   | 24/36 [07:11<03:23, 16.99s/it]
                                               

 67%|██████▋   | 24/36 [07:11<03:23, 16.99s/it]
 69%|██████▉   | 25/36 [07:26<03:03, 16.64s/it]
                                               

 69%|██████▉   | 25/36 [07:26<03:03, 16.64s/it]
 72%|███████▏  | 26/36 [07:45<02:53, 17.32s/it]
                                               

 72%|███████▏  | 26/36 [07:45<02:53, 17.32s/it]
 75%|███████▌  | 27/36 [08:05<02:42, 18.05s/it]
                                               

 75%|███████▌  | 27/36 [08:05<02:42, 18.05s/it]
 78%|███████▊  | 28/36 [08:32<02:45, 20.68s/it]
                                               

 78%|███████▊  | 28/36 [08:32<02:45, 20.68s/it]
 81%|████████  | 29/36 [08:49<02:17, 19.70s/it]
                                               

 81%|████████  | 29/36 [08:49<02:17, 19.70s/it]
 83%|████████▎ | 30/36 [08:59<01:41, 16.84s/it]
                                               

 83%|████████▎ | 30/36 [08:59<01:41, 16.84s/it]
 86%|████████▌ | 31/36 [09:13<01:18, 15.77s/it]
                                               

 86%|████████▌ | 31/36 [09:13<01:18, 15.77s/it]
 89%|████████▉ | 32/36 [09:38<01:14, 18.58s/it]
                                               

 89%|████████▉ | 32/36 [09:38<01:14, 18.58s/it]
 92%|█████████▏| 33/36 [09:55<00:54, 18.09s/it]
                                               

 92%|█████████▏| 33/36 [09:55<00:54, 18.09s/it]
 94%|█████████▍| 34/36 [10:08<00:33, 16.67s/it]
                                               

 94%|█████████▍| 34/36 [10:08<00:33, 16.67s/it]
 97%|█████████▋| 35/36 [10:26<00:17, 17.07s/it]
                                               

 97%|█████████▋| 35/36 [10:26<00:17, 17.07s/it]
100%|██████████| 36/36 [10:49<00:00, 18.83s/it]
                                               

100%|██████████| 36/36 [10:49<00:00, 18.83s/it]
                                               

100%|██████████| 36/36 [10:51<00:00, 18.83s/it]
100%|██████████| 36/36 [10:51<00:00, 18.09s/it]
{'loss': 1.0018, 'grad_norm': 2.4177138805389404, 'learning_rate': 0.0, 'entropy': 1.6683795936405659, 'num_tokens': 1968.0, 'mean_token_accuracy': 0.8097846880555153, 'epoch': 0.37}
{'loss': 1.0445, 'grad_norm': 2.4236433506011963, 'learning_rate': 2.5e-06, 'entropy': 1.3888544775545597, 'num_tokens': 5029.0, 'mean_token_accuracy': 0.8093724586069584, 'epoch': 0.74}
{'loss': 1.0961, 'grad_norm': 2.9668824672698975, 'learning_rate': 5e-06, 'entropy': 1.528201081536033, 'num_tokens': 6318.0, 'mean_token_accuracy': 0.8274629007686268, 'epoch': 1.0}
{'loss': 1.2092, 'grad_norm': 3.2720248699188232, 'learning_rate': 7.500000000000001e-06, 'entropy': 1.6628235839307308, 'num_tokens': 8014.0, 'mean_token_accuracy': 0.7986961454153061, 'epoch': 1.37}
{'loss': 1.0107, 'grad_norm': 2.1640515327453613, 'learning_rate': 1e-05, 'entropy': 1.356225661933422, 'num_tokens': 11175.0, 'mean_token_accuracy': 0.8248906023800373, 'epoch': 1.74}
{'loss': 0.8321, 'grad_norm': 2.105156183242798, 'learning_rate': 9.975923633360985e-06, 'entropy': 1.5897158709439365, 'num_tokens': 12636.0, 'mean_token_accuracy': 0.8517454103990034, 'epoch': 2.0}
{'loss': 0.986, 'grad_norm': 2.30998158454895, 'learning_rate': 9.903926402016153e-06, 'entropy': 1.5224517099559307, 'num_tokens': 14723.0, 'mean_token_accuracy': 0.8397073410451412, 'epoch': 2.37}
{'loss': 1.034, 'grad_norm': 2.766644239425659, 'learning_rate': 9.784701678661045e-06, 'entropy': 1.5129481963813305, 'num_tokens': 17027.0, 'mean_token_accuracy': 0.8221327364444733, 'epoch': 2.74}
{'loss': 0.8281, 'grad_norm': 1.5778119564056396, 'learning_rate': 9.619397662556434e-06, 'entropy': 1.5814784277569165, 'num_tokens': 18954.0, 'mean_token_accuracy': 0.8229299729520624, 'epoch': 3.0}
{'loss': 0.9039, 'grad_norm': 1.8715689182281494, 'learning_rate': 9.409606321741776e-06, 'entropy': 1.5196006894111633, 'num_tokens': 21674.0, 'mean_token_accuracy': 0.8265761323273182, 'epoch': 3.37}
{'loss': 0.8382, 'grad_norm': 2.305640697479248, 'learning_rate': 9.157348061512728e-06, 'entropy': 1.6361293271183968, 'num_tokens': 23385.0, 'mean_token_accuracy': 0.8478846177458763, 'epoch': 3.74}
{'loss': 0.9165, 'grad_norm': 2.2015862464904785, 'learning_rate': 8.865052266813686e-06, 'entropy': 1.4223850206895308, 'num_tokens': 25272.0, 'mean_token_accuracy': 0.8264588605273854, 'epoch': 4.0}
{'loss': 0.8788, 'grad_norm': 1.8726590871810913, 'learning_rate': 8.535533905932739e-06, 'entropy': 1.4522549733519554, 'num_tokens': 28245.0, 'mean_token_accuracy': 0.8233850933611393, 'epoch': 4.37}
{'loss': 0.8403, 'grad_norm': 2.315110683441162, 'learning_rate': 8.171966420818227e-06, 'entropy': 1.5813825950026512, 'num_tokens': 30283.0, 'mean_token_accuracy': 0.8271992728114128, 'epoch': 4.74}
{'loss': 0.7467, 'grad_norm': 2.0974671840667725, 'learning_rate': 7.777851165098012e-06, 'entropy': 1.6157631386410107, 'num_tokens': 31590.0, 'mean_token_accuracy': 0.8613714250651273, 'epoch': 5.0}
{'loss': 0.8728, 'grad_norm': 2.498725652694702, 'learning_rate': 7.3569836841299905e-06, 'entropy': 1.6376595944166183, 'num_tokens': 33360.0, 'mean_token_accuracy': 0.834260992705822, 'epoch': 5.37}
{'loss': 0.7459, 'grad_norm': 1.5646134614944458, 'learning_rate': 6.913417161825449e-06, 'entropy': 1.4191866889595985, 'num_tokens': 36519.0, 'mean_token_accuracy': 0.8493253327906132, 'epoch': 5.74}
{'loss': 0.8026, 'grad_norm': 2.3738090991973877, 'learning_rate': 6.451423386272312e-06, 'entropy': 1.5982248457995327, 'num_tokens': 37908.0, 'mean_token_accuracy': 0.8347415707328103, 'epoch': 6.0}
{'loss': 0.7325, 'grad_norm': 1.830989956855774, 'learning_rate': 5.975451610080643e-06, 'entropy': 1.4854546785354614, 'num_tokens': 40619.0, 'mean_token_accuracy': 0.8374447971582413, 'epoch': 6.37}
{'loss': 0.7734, 'grad_norm': 1.997825264930725, 'learning_rate': 5.490085701647805e-06, 'entropy': 1.553704597055912, 'num_tokens': 43042.0, 'mean_token_accuracy': 0.8597717359662056, 'epoch': 6.74}
{'loss': 0.7435, 'grad_norm': 2.309863328933716, 'learning_rate': 5e-06, 'entropy': 1.6349121711470864, 'num_tokens': 44226.0, 'mean_token_accuracy': 0.8431747393174605, 'epoch': 7.0}
{'loss': 0.659, 'grad_norm': 1.6951321363449097, 'learning_rate': 4.509914298352197e-06, 'entropy': 1.5352507643401623, 'num_tokens': 46461.0, 'mean_token_accuracy': 0.8864378668367863, 'epoch': 7.37}
{'loss': 0.7256, 'grad_norm': 1.6408005952835083, 'learning_rate': 4.02454838991936e-06, 'entropy': 1.539468977600336, 'num_tokens': 49072.0, 'mean_token_accuracy': 0.8520537428557873, 'epoch': 7.74}
{'loss': 0.8003, 'grad_norm': 2.4134228229522705, 'learning_rate': 3.5485766137276894e-06, 'entropy': 1.5903328440406106, 'num_tokens': 50544.0, 'mean_token_accuracy': 0.8197326822714373, 'epoch': 8.0}
{'loss': 0.752, 'grad_norm': 1.9492288827896118, 'learning_rate': 3.0865828381745515e-06, 'entropy': 1.450204748660326, 'num_tokens': 53147.0, 'mean_token_accuracy': 0.8304522782564163, 'epoch': 8.37}
{'loss': 0.6922, 'grad_norm': 1.7952930927276611, 'learning_rate': 2.6430163158700116e-06, 'entropy': 1.6082553416490555, 'num_tokens': 55190.0, 'mean_token_accuracy': 0.8688659891486168, 'epoch': 8.74}
{'loss': 0.6186, 'grad_norm': 1.6568412780761719, 'learning_rate': 2.2221488349019903e-06, 'entropy': 1.6167905601588162, 'num_tokens': 56862.0, 'mean_token_accuracy': 0.8961980397051031, 'epoch': 9.0}
{'loss': 0.6366, 'grad_norm': 1.5550566911697388, 'learning_rate': 1.8280335791817733e-06, 'entropy': 1.556851051747799, 'num_tokens': 59543.0, 'mean_token_accuracy': 0.8687282167375088, 'epoch': 9.37}
{'loss': 0.7076, 'grad_norm': 2.0406062602996826, 'learning_rate': 1.4644660940672628e-06, 'entropy': 1.598409865051508, 'num_tokens': 61573.0, 'mean_token_accuracy': 0.8532840460538864, 'epoch': 9.74}
{'loss': 0.7417, 'grad_norm': 1.9258946180343628, 'learning_rate': 1.134947733186315e-06, 'entropy': 1.4783020669763738, 'num_tokens': 63180.0, 'mean_token_accuracy': 0.8772044236009772, 'epoch': 10.0}
{'loss': 0.7722, 'grad_norm': 2.0786936283111572, 'learning_rate': 8.426519384872733e-07, 'entropy': 1.5537407100200653, 'num_tokens': 65326.0, 'mean_token_accuracy': 0.8693882115185261, 'epoch': 10.37}
{'loss': 0.6103, 'grad_norm': 1.5149060487747192, 'learning_rate': 5.903936782582253e-07, 'entropy': 1.4919258207082748, 'num_tokens': 67813.0, 'mean_token_accuracy': 0.8663139119744301, 'epoch': 10.74}
{'loss': 0.6682, 'grad_norm': 1.5657906532287598, 'learning_rate': 3.8060233744356634e-07, 'entropy': 1.6346853321248835, 'num_tokens': 69498.0, 'mean_token_accuracy': 0.8612899617715315, 'epoch': 11.0}
{'loss': 0.6984, 'grad_norm': 1.9356086254119873, 'learning_rate': 2.152983213389559e-07, 'entropy': 1.5995109640061855, 'num_tokens': 71308.0, 'mean_token_accuracy': 0.8598572909832001, 'epoch': 11.37}
{'loss': 0.6767, 'grad_norm': 1.8482896089553833, 'learning_rate': 9.607359798384785e-08, 'entropy': 1.6606888063251972, 'num_tokens': 73415.0, 'mean_token_accuracy': 0.8875432945787907, 'epoch': 11.74}
{'loss': 0.6573, 'grad_norm': 1.650292992591858, 'learning_rate': 2.4076366639015914e-08, 'entropy': 1.3266001668843357, 'num_tokens': 75816.0, 'mean_token_accuracy': 0.8488761294971813, 'epoch': 12.0}
{'train_runtime': 651.3331, 'train_samples_per_second': 0.792, 'train_steps_per_second': 0.055, 'train_loss': 0.8126114358504614, 'epoch': 12.0}
モデルを保存中...
Llama 3.2 3B QLoRA学習完了！モデルは ../outputs/llama32-3b-typst-qlora-003-006 に保存されました。
テスト実行: ../outputs/llama32-3b-typst-qlora-003-006
テストログ: ../logs/003-006_test_20250917_013643.log
実行中: python3 inference_llama32_3b.py                 --peft_model_path ../outputs/llama32-3b-typst-qlora-003-006                 --input_file ../sample/sample_small.tex                 --output_file ../trained/003-006.md
ベースモデル: meta-llama/Llama-3.2-3B-Instruct
✅ 使用モデル: PEFT（LoRA適用済み）
   PEFTモデル: ../outputs/llama32-3b-typst-qlora-003-006
入力ファイル: ../sample/sample_small.tex
出力ファイル: ../trained/003-006.md
トークナイザーを読み込み中...
4bit量子化設定を準備中...
ベースモデルを読み込み中...

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.93s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.72s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.91s/it]
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
✅ PEFTモデルを読み込み中...
   PEFTパス: ../outputs/llama32-3b-typst-qlora-003-006
   PEFTモデル読み込み完了
LaTeXファイルを読み込み中...
LaTeXテキストを 1 チャンクに分割しました
LaTeX→Typst変換を開始します...
チャンク 1/1 を変換中...
  入力テキスト長: 865 文字
  チャットテンプレート適用成功
  プロンプト長: 1223 文字
  入力トークン数: 415
  生成開始...
  生成完了: 629 トークン
  生成テキスト長: 1715 文字
  プロンプト長: 1223 文字
  生成テキスト開始: system

Cutting Knowledge Date: December 2023
Today Date: 17 Sep 2025

LaTeX 形式の文章を Typst形式 へ変換してください.
重要: TypstはLaTeXの代替となる文書組版システムです。TypeScript（プログラミング言語）ではありません。
出力はTypst構文のみ。プログラミングコードは出力しません。user...
  生成テキスト終了: ... $\left(\varepsilon_0, \varepsilon_1\right) \in \mathbb{R}^2$ are small, and $\varphi$ is a regular non-negative function. Our goal is to show the global existence of solutions under these conditions.
  生成テキスト全体の長さ: 1715 文字
  プロンプトの長さ: 1223 文字
  方法1（全体使用）: 1715 文字
  方法3（```typstマーカー）: 638 文字
  最終応答: system

Cutting Knowledge Date: December 2023
Today Date: 17 Sep 2025

LaTeX 形式の文章を Typst形式 へ変換してください.
重要: TypstはLaTeXの代替となる文書組版システムです。TypeScript（プログラミング言語）ではありません。
出力はTypst構文のみ。プログラミングコードは出力しません。user...
  最終応答の最後: ...non-negative function. Our goal is to show the global existence of solutions under these conditions.
  抽出された応答: system

Cutting Knowledge Date: December 2023
Today Date: 17 Sep 2025

LaTeX 形式の文章を Typst形式 へ変換してくださ...
結果を ../trained/003-006.md に保存中...
変換完了！
完了: Iteration 006 の学習・テストが完了しました。
チェックポイントを更新: ../outputs/llama32-3b-typst-qlora-003-006
=== Iteration 006 の処理完了 ===
--------------------------------------------------
=== Iteration 007 の処理開始 ===
継続学習: ../jsonl/train_003.jsonl (from ../outputs/llama32-3b-typst-qlora-003-006)
学習ログ: ../logs/003-007_training_20250917_013643.log
実行中: python3 train_llama32_3b_qlora_fixed.py             --data ../jsonl/train_003.jsonl             --out ../outputs/llama32-3b-typst-qlora-003-007             --peft_model_path ../outputs/llama32-3b-typst-qlora-003-006             --epochs 12             --batch_size 1             --grad_accum 16             --learning_rate 1e-05             --lora_r 8             --lora_alpha 8             --lora_dropout 0.1
Llama 3.2 3Bモデル: meta-llama/Llama-3.2-3B-Instruct
データ: ../jsonl/train_003.jsonl
出力: ../outputs/llama32-3b-typst-qlora-003-007
学習率: 1e-05
エポック数: 12
バッチサイズ: 1
勾配累積: 16
LoRA rank: 8
LoRA alpha: 8
LoRA dropout: 0.1
==================================================
トークナイザーを読み込み中...
4bit量子化設定を準備中...
モデルを読み込み中...

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.81s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.68s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.85s/it]
/mnt/d/github/typst-tex-converter/venv/lib/python3.11/site-packages/peft/tuners/lora/bnb.py:348: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.
  warnings.warn(
/mnt/d/github/typst-tex-converter/venv/lib/python3.11/site-packages/peft/tuners/tuners_utils.py:196: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!
  warnings.warn(
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 128009}.
既存のPEFTモデルを読み込み中: ../outputs/llama32-3b-typst-qlora-003-006
データセットを読み込み中...
データセットサイズ: 43
Llama 3.2 3B QLoRA学習を開始します...

  0%|          | 0/36 [00:00<?, ?it/s]
  3%|▎         | 1/36 [00:09<05:30,  9.45s/it]
                                              

  3%|▎         | 1/36 [00:09<05:30,  9.45s/it]
  6%|▌         | 2/36 [00:36<11:15, 19.86s/it]
                                              

  6%|▌         | 2/36 [00:36<11:15, 19.86s/it]
  8%|▊         | 3/36 [00:45<08:10, 14.87s/it]
                                              

  8%|▊         | 3/36 [00:45<08:10, 14.87s/it]
 11%|█         | 4/36 [00:58<07:32, 14.14s/it]
                                              

 11%|█         | 4/36 [00:58<07:32, 14.14s/it]
 14%|█▍        | 5/36 [01:22<09:12, 17.82s/it]
                                              

 14%|█▍        | 5/36 [01:22<09:12, 17.82s/it]
 17%|█▋        | 6/36 [01:40<08:50, 17.68s/it]
                                              

 17%|█▋        | 6/36 [01:40<08:50, 17.68s/it]
 19%|█▉        | 7/36 [01:55<08:09, 16.89s/it]
                                              

 19%|█▉        | 7/36 [01:55<08:09, 16.89s/it]
 22%|██▏       | 8/36 [02:13<08:05, 17.34s/it]
                                              

 22%|██▏       | 8/36 [02:13<08:05, 17.34s/it]
 25%|██▌       | 9/36 [02:34<08:17, 18.44s/it]
                                              

 25%|██▌       | 9/36 [02:34<08:17, 18.44s/it]
 28%|██▊       | 10/36 [02:59<08:46, 20.24s/it]
                                               

 28%|██▊       | 10/36 [02:59<08:46, 20.24s/it]
 31%|███       | 11/36 [03:15<07:58, 19.12s/it]
                                               

 31%|███       | 11/36 [03:15<07:58, 19.12s/it]
 33%|███▎      | 12/36 [03:29<07:00, 17.52s/it]
                                               

 33%|███▎      | 12/36 [03:29<07:00, 17.52s/it]
 36%|███▌      | 13/36 [03:55<07:40, 20.03s/it]
                                               

 36%|███▌      | 13/36 [03:55<07:40, 20.03s/it]
 39%|███▉      | 14/36 [04:11<06:57, 18.97s/it]
                                               

 39%|███▉      | 14/36 [04:11<06:57, 18.97s/it]
 42%|████▏     | 15/36 [04:24<05:56, 17.00s/it]
                                               

 42%|████▏     | 15/36 [04:24<05:56, 17.00s/it]
 44%|████▍     | 16/36 [04:36<05:10, 15.50s/it]
                                               

 44%|████▍     | 16/36 [04:36<05:10, 15.50s/it]
 47%|████▋     | 17/36 [05:09<06:35, 20.83s/it]
                                               

 47%|████▋     | 17/36 [05:09<06:35, 20.83s/it]
 50%|█████     | 18/36 [05:18<05:13, 17.40s/it]
                                               

 50%|█████     | 18/36 [05:18<05:13, 17.40s/it]
 53%|█████▎    | 19/36 [05:40<05:19, 18.81s/it]
                                               

 53%|█████▎    | 19/36 [05:40<05:19, 18.81s/it]
 56%|█████▌    | 20/36 [06:02<05:13, 19.59s/it]
                                               

 56%|█████▌    | 20/36 [06:02<05:13, 19.59s/it]
 58%|█████▊    | 21/36 [06:12<04:11, 16.75s/it]
                                               

 58%|█████▊    | 21/36 [06:12<04:11, 16.75s/it]
 61%|██████    | 22/36 [06:34<04:15, 18.22s/it]
                                               

 61%|██████    | 22/36 [06:34<04:15, 18.22s/it]
 64%|██████▍   | 23/36 [06:57<04:15, 19.64s/it]
                                               

 64%|██████▍   | 23/36 [06:57<04:15, 19.64s/it]
 67%|██████▋   | 24/36 [07:07<03:21, 16.76s/it]
                                               

 67%|██████▋   | 24/36 [07:07<03:21, 16.76s/it]
 69%|██████▉   | 25/36 [07:22<03:01, 16.45s/it]
                                               

 69%|██████▉   | 25/36 [07:22<03:01, 16.45s/it]
 72%|███████▏  | 26/36 [07:41<02:51, 17.11s/it]
                                               

 72%|███████▏  | 26/36 [07:41<02:51, 17.11s/it]
 75%|███████▌  | 27/36 [08:01<02:41, 17.92s/it]
                                               

 75%|███████▌  | 27/36 [08:01<02:41, 17.92s/it]
 78%|███████▊  | 28/36 [08:27<02:44, 20.52s/it]
                                               

 78%|███████▊  | 28/36 [08:27<02:44, 20.52s/it]
 81%|████████  | 29/36 [08:45<02:16, 19.57s/it]
                                               

 81%|████████  | 29/36 [08:45<02:16, 19.57s/it]
 83%|████████▎ | 30/36 [08:55<01:40, 16.75s/it]
                                               

 83%|████████▎ | 30/36 [08:55<01:40, 16.75s/it]
 86%|████████▌ | 31/36 [09:08<01:18, 15.64s/it]
                                               

 86%|████████▌ | 31/36 [09:08<01:18, 15.64s/it]
 89%|████████▉ | 32/36 [09:33<01:13, 18.47s/it]
                                               

 89%|████████▉ | 32/36 [09:33<01:13, 18.47s/it]
 92%|█████████▏| 33/36 [09:50<00:53, 17.92s/it]
                                               

 92%|█████████▏| 33/36 [09:50<00:53, 17.92s/it]
 94%|█████████▍| 34/36 [10:03<00:33, 16.52s/it]
                                               

 94%|█████████▍| 34/36 [10:03<00:33, 16.52s/it]
 97%|█████████▋| 35/36 [10:21<00:16, 16.93s/it]
                                               

 97%|█████████▋| 35/36 [10:21<00:16, 16.93s/it]
100%|██████████| 36/36 [10:44<00:00, 18.69s/it]
                                               

100%|██████████| 36/36 [10:44<00:00, 18.69s/it]
                                               

100%|██████████| 36/36 [10:45<00:00, 18.69s/it]
100%|██████████| 36/36 [10:45<00:00, 17.94s/it]
{'loss': 1.002, 'grad_norm': 2.2152116298675537, 'learning_rate': 0.0, 'entropy': 1.6677384413778782, 'num_tokens': 1968.0, 'mean_token_accuracy': 0.8103914856910706, 'epoch': 0.37}
{'loss': 1.0451, 'grad_norm': 2.239122152328491, 'learning_rate': 2.5e-06, 'entropy': 1.3883257657289505, 'num_tokens': 5029.0, 'mean_token_accuracy': 0.8093724586069584, 'epoch': 0.74}
{'loss': 1.0966, 'grad_norm': 2.804687023162842, 'learning_rate': 5e-06, 'entropy': 1.5273646333000876, 'num_tokens': 6318.0, 'mean_token_accuracy': 0.8274629007686268, 'epoch': 1.0}
{'loss': 1.21, 'grad_norm': 3.01349139213562, 'learning_rate': 7.500000000000001e-06, 'entropy': 1.6620124131441116, 'num_tokens': 8014.0, 'mean_token_accuracy': 0.7986961454153061, 'epoch': 1.37}
{'loss': 1.0112, 'grad_norm': 1.9994933605194092, 'learning_rate': 1e-05, 'entropy': 1.3556734770536423, 'num_tokens': 11175.0, 'mean_token_accuracy': 0.8242393508553505, 'epoch': 1.74}
{'loss': 0.8324, 'grad_norm': 1.9588735103607178, 'learning_rate': 9.975923633360985e-06, 'entropy': 1.5890347253192554, 'num_tokens': 12636.0, 'mean_token_accuracy': 0.8517454103990034, 'epoch': 2.0}
{'loss': 0.9866, 'grad_norm': 2.1179723739624023, 'learning_rate': 9.903926402016153e-06, 'entropy': 1.521512906998396, 'num_tokens': 14723.0, 'mean_token_accuracy': 0.8397073410451412, 'epoch': 2.37}
{'loss': 1.0355, 'grad_norm': 2.577183485031128, 'learning_rate': 9.784701678661045e-06, 'entropy': 1.5118209086358547, 'num_tokens': 17027.0, 'mean_token_accuracy': 0.8221327364444733, 'epoch': 2.74}
{'loss': 0.8288, 'grad_norm': 1.4609324932098389, 'learning_rate': 9.619397662556434e-06, 'entropy': 1.5804638483307578, 'num_tokens': 18954.0, 'mean_token_accuracy': 0.8229299729520624, 'epoch': 3.0}
{'loss': 0.9041, 'grad_norm': 1.81269109249115, 'learning_rate': 9.409606321741776e-06, 'entropy': 1.5182248279452324, 'num_tokens': 21674.0, 'mean_token_accuracy': 0.8259974308311939, 'epoch': 3.37}
{'loss': 0.8399, 'grad_norm': 2.1509244441986084, 'learning_rate': 9.157348061512728e-06, 'entropy': 1.6350531727075577, 'num_tokens': 23385.0, 'mean_token_accuracy': 0.848803736269474, 'epoch': 3.74}
{'loss': 0.9177, 'grad_norm': 2.059086322784424, 'learning_rate': 8.865052266813686e-06, 'entropy': 1.4211984114213423, 'num_tokens': 25272.0, 'mean_token_accuracy': 0.8264588605273854, 'epoch': 4.0}
{'loss': 0.8802, 'grad_norm': 1.7460602521896362, 'learning_rate': 8.535533905932739e-06, 'entropy': 1.4512071274220943, 'num_tokens': 28245.0, 'mean_token_accuracy': 0.8233850933611393, 'epoch': 4.37}
{'loss': 0.8428, 'grad_norm': 2.1725354194641113, 'learning_rate': 8.171966420818227e-06, 'entropy': 1.5797839313745499, 'num_tokens': 30283.0, 'mean_token_accuracy': 0.8271992728114128, 'epoch': 4.74}
{'loss': 0.7463, 'grad_norm': 2.0354371070861816, 'learning_rate': 7.777851165098012e-06, 'entropy': 1.6149182590571316, 'num_tokens': 31590.0, 'mean_token_accuracy': 0.8619325919584795, 'epoch': 5.0}
{'loss': 0.8742, 'grad_norm': 2.3820760250091553, 'learning_rate': 7.3569836841299905e-06, 'entropy': 1.6361107230186462, 'num_tokens': 33360.0, 'mean_token_accuracy': 0.8326461352407932, 'epoch': 5.37}
{'loss': 0.7467, 'grad_norm': 1.502326488494873, 'learning_rate': 6.913417161825449e-06, 'entropy': 1.4174961894750595, 'num_tokens': 36519.0, 'mean_token_accuracy': 0.8497321903705597, 'epoch': 5.74}
{'loss': 0.8052, 'grad_norm': 2.264049530029297, 'learning_rate': 6.451423386272312e-06, 'entropy': 1.59653808853843, 'num_tokens': 37908.0, 'mean_token_accuracy': 0.8347415707328103, 'epoch': 6.0}
{'loss': 0.7339, 'grad_norm': 1.7293614149093628, 'learning_rate': 5.975451610080643e-06, 'entropy': 1.4836338870227337, 'num_tokens': 40619.0, 'mean_token_accuracy': 0.8374447971582413, 'epoch': 6.37}
{'loss': 0.7745, 'grad_norm': 1.9033490419387817, 'learning_rate': 5.490085701647805e-06, 'entropy': 1.552107397466898, 'num_tokens': 43042.0, 'mean_token_accuracy': 0.8597717359662056, 'epoch': 6.74}
{'loss': 0.7456, 'grad_norm': 2.17427134513855, 'learning_rate': 5e-06, 'entropy': 1.633302699435841, 'num_tokens': 44226.0, 'mean_token_accuracy': 0.8426129384474321, 'epoch': 7.0}
{'loss': 0.6607, 'grad_norm': 1.6030367612838745, 'learning_rate': 4.509914298352197e-06, 'entropy': 1.5337839163839817, 'num_tokens': 46461.0, 'mean_token_accuracy': 0.8864378668367863, 'epoch': 7.37}
{'loss': 0.7272, 'grad_norm': 1.5626857280731201, 'learning_rate': 4.02454838991936e-06, 'entropy': 1.5379896312952042, 'num_tokens': 49072.0, 'mean_token_accuracy': 0.8522959910333157, 'epoch': 7.74}
{'loss': 0.8004, 'grad_norm': 2.3428759574890137, 'learning_rate': 3.5485766137276894e-06, 'entropy': 1.5880130095915361, 'num_tokens': 50544.0, 'mean_token_accuracy': 0.8205744353207675, 'epoch': 8.0}
{'loss': 0.7535, 'grad_norm': 1.8477071523666382, 'learning_rate': 3.0865828381745515e-06, 'entropy': 1.4490179605782032, 'num_tokens': 53147.0, 'mean_token_accuracy': 0.8311316259205341, 'epoch': 8.37}
{'loss': 0.6924, 'grad_norm': 1.7229424715042114, 'learning_rate': 2.6430163158700116e-06, 'entropy': 1.6062395758926868, 'num_tokens': 55190.0, 'mean_token_accuracy': 0.8694727867841721, 'epoch': 8.74}
{'loss': 0.6214, 'grad_norm': 1.5792819261550903, 'learning_rate': 2.2221488349019903e-06, 'entropy': 1.614875771782615, 'num_tokens': 56862.0, 'mean_token_accuracy': 0.8961980397051031, 'epoch': 9.0}
{'loss': 0.6382, 'grad_norm': 1.472161054611206, 'learning_rate': 1.8280335791817733e-06, 'entropy': 1.5553492531180382, 'num_tokens': 59543.0, 'mean_token_accuracy': 0.8657520264387131, 'epoch': 9.37}
{'loss': 0.7108, 'grad_norm': 1.9399759769439697, 'learning_rate': 1.4644660940672628e-06, 'entropy': 1.5961983241140842, 'num_tokens': 61573.0, 'mean_token_accuracy': 0.8542921133339405, 'epoch': 9.74}
{'loss': 0.7414, 'grad_norm': 1.8832621574401855, 'learning_rate': 1.134947733186315e-06, 'entropy': 1.4769298000769182, 'num_tokens': 63180.0, 'mean_token_accuracy': 0.8780870329249989, 'epoch': 10.0}
{'loss': 0.774, 'grad_norm': 1.9967156648635864, 'learning_rate': 8.426519384872733e-07, 'entropy': 1.5519210994243622, 'num_tokens': 65326.0, 'mean_token_accuracy': 0.8664120212197304, 'epoch': 10.37}
{'loss': 0.6113, 'grad_norm': 1.4797054529190063, 'learning_rate': 5.903936782582253e-07, 'entropy': 1.4903541281819344, 'num_tokens': 67813.0, 'mean_token_accuracy': 0.868001326918602, 'epoch': 10.74}
{'loss': 0.6685, 'grad_norm': 1.5184966325759888, 'learning_rate': 3.8060233744356634e-07, 'entropy': 1.6333333362232556, 'num_tokens': 69498.0, 'mean_token_accuracy': 0.8604073470289056, 'epoch': 11.0}
{'loss': 0.7015, 'grad_norm': 1.8411568403244019, 'learning_rate': 2.152983213389559e-07, 'entropy': 1.5977018438279629, 'num_tokens': 71308.0, 'mean_token_accuracy': 0.8598572909832001, 'epoch': 11.37}
{'loss': 0.6797, 'grad_norm': 1.7683265209197998, 'learning_rate': 9.607359798384785e-08, 'entropy': 1.6581327766180038, 'num_tokens': 73415.0, 'mean_token_accuracy': 0.8875432945787907, 'epoch': 11.74}
{'loss': 0.658, 'grad_norm': 1.579408049583435, 'learning_rate': 2.4076366639015914e-08, 'entropy': 1.324739239432595, 'num_tokens': 75816.0, 'mean_token_accuracy': 0.8445471254262057, 'epoch': 12.0}
{'train_runtime': 645.9533, 'train_samples_per_second': 0.799, 'train_steps_per_second': 0.056, 'train_loss': 0.813840475347307, 'epoch': 12.0}
モデルを保存中...
Llama 3.2 3B QLoRA学習完了！モデルは ../outputs/llama32-3b-typst-qlora-003-007 に保存されました。
スキップ: Iteration 007 のテストをスキップしました（2回に1回の実行）
チェックポイントを更新: ../outputs/llama32-3b-typst-qlora-003-007
=== Iteration 007 の処理完了 ===
--------------------------------------------------
=== Iteration 008 の処理開始 ===
継続学習: ../jsonl/train_003.jsonl (from ../outputs/llama32-3b-typst-qlora-003-007)
学習ログ: ../logs/003-008_training_20250917_013643.log
実行中: python3 train_llama32_3b_qlora_fixed.py             --data ../jsonl/train_003.jsonl             --out ../outputs/llama32-3b-typst-qlora-003-008             --peft_model_path ../outputs/llama32-3b-typst-qlora-003-007             --epochs 12             --batch_size 1             --grad_accum 16             --learning_rate 1e-05             --lora_r 8             --lora_alpha 8             --lora_dropout 0.1
Llama 3.2 3Bモデル: meta-llama/Llama-3.2-3B-Instruct
データ: ../jsonl/train_003.jsonl
出力: ../outputs/llama32-3b-typst-qlora-003-008
学習率: 1e-05
エポック数: 12
バッチサイズ: 1
勾配累積: 16
LoRA rank: 8
LoRA alpha: 8
LoRA dropout: 0.1
==================================================
トークナイザーを読み込み中...
4bit量子化設定を準備中...
モデルを読み込み中...

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.95s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.76s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.94s/it]
/mnt/d/github/typst-tex-converter/venv/lib/python3.11/site-packages/peft/tuners/lora/bnb.py:348: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.
  warnings.warn(
/mnt/d/github/typst-tex-converter/venv/lib/python3.11/site-packages/peft/tuners/tuners_utils.py:196: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!
  warnings.warn(
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 128009}.
既存のPEFTモデルを読み込み中: ../outputs/llama32-3b-typst-qlora-003-007
データセットを読み込み中...
データセットサイズ: 43
Llama 3.2 3B QLoRA学習を開始します...

  0%|          | 0/36 [00:00<?, ?it/s]
  3%|▎         | 1/36 [00:09<05:31,  9.47s/it]
                                              

  3%|▎         | 1/36 [00:09<05:31,  9.47s/it]
  6%|▌         | 2/36 [00:36<11:19, 19.98s/it]
                                              

  6%|▌         | 2/36 [00:36<11:19, 19.98s/it]
  8%|▊         | 3/36 [00:46<08:18, 15.11s/it]
                                              

  8%|▊         | 3/36 [00:46<08:18, 15.11s/it]
 11%|█         | 4/36 [00:59<07:42, 14.45s/it]
                                              

 11%|█         | 4/36 [00:59<07:42, 14.45s/it]
 14%|█▍        | 5/36 [01:23<09:18, 18.01s/it]
                                              

 14%|█▍        | 5/36 [01:23<09:18, 18.01s/it]
 17%|█▋        | 6/36 [01:41<08:55, 17.84s/it]
                                              

 17%|█▋        | 6/36 [01:41<08:55, 17.84s/it]
 19%|█▉        | 7/36 [01:56<08:15, 17.10s/it]
                                              

 19%|█▉        | 7/36 [01:56<08:15, 17.10s/it]
 22%|██▏       | 8/36 [02:15<08:10, 17.51s/it]
                                              

 22%|██▏       | 8/36 [02:15<08:10, 17.51s/it]
 25%|██▌       | 9/36 [02:36<08:24, 18.70s/it]
                                              

 25%|██▌       | 9/36 [02:36<08:24, 18.70s/it]
 28%|██▊       | 10/36 [03:00<08:50, 20.39s/it]
                                               

 28%|██▊       | 10/36 [03:00<08:50, 20.39s/it]
 31%|███       | 11/36 [03:17<08:01, 19.26s/it]
                                               

 31%|███       | 11/36 [03:17<08:01, 19.26s/it]
 33%|███▎      | 12/36 [03:31<07:05, 17.74s/it]
                                               

 33%|███▎      | 12/36 [03:31<07:05, 17.74s/it]
 36%|███▌      | 13/36 [03:57<07:45, 20.26s/it]
                                               

 36%|███▌      | 13/36 [03:57<07:45, 20.26s/it]
 39%|███▉      | 14/36 [04:14<07:01, 19.18s/it]
                                               

 39%|███▉      | 14/36 [04:14<07:01, 19.18s/it]
 42%|████▏     | 15/36 [04:27<06:00, 17.19s/it]
                                               

 42%|████▏     | 15/36 [04:27<06:00, 17.19s/it]
 44%|████▍     | 16/36 [04:39<05:14, 15.73s/it]
                                               

 44%|████▍     | 16/36 [04:39<05:14, 15.73s/it]
 47%|████▋     | 17/36 [05:12<06:38, 20.99s/it]
                                               

 47%|████▋     | 17/36 [05:12<06:38, 20.99s/it]
 50%|█████     | 18/36 [05:21<05:14, 17.46s/it]
                                               

 50%|█████     | 18/36 [05:21<05:14, 17.46s/it]
 53%|█████▎    | 19/36 [05:44<05:23, 19.01s/it]
                                               

 53%|█████▎    | 19/36 [05:44<05:23, 19.01s/it]
 56%|█████▌    | 20/36 [06:06<05:16, 19.78s/it]
                                               

 56%|█████▌    | 20/36 [06:06<05:16, 19.78s/it]
 58%|█████▊    | 21/36 [06:17<04:17, 17.19s/it]
                                               

 58%|█████▊    | 21/36 [06:17<04:17, 17.19s/it]
 61%|██████    | 22/36 [06:39<04:22, 18.75s/it]
                                               

 61%|██████    | 22/36 [06:39<04:22, 18.75s/it]
 64%|██████▍   | 23/36 [07:02<04:21, 20.11s/it]
                                               

 64%|██████▍   | 23/36 [07:02<04:21, 20.11s/it]
 67%|██████▋   | 24/36 [07:12<03:24, 17.08s/it]
                                               

 67%|██████▋   | 24/36 [07:12<03:24, 17.08s/it]
 69%|██████▉   | 25/36 [07:28<03:03, 16.72s/it]
                                               

 69%|██████▉   | 25/36 [07:28<03:03, 16.72s/it]
 72%|███████▏  | 26/36 [07:47<02:54, 17.45s/it]
                                               

 72%|███████▏  | 26/36 [07:47<02:54, 17.45s/it]
 75%|███████▌  | 27/36 [08:08<02:45, 18.34s/it]
                                               

 75%|███████▌  | 27/36 [08:08<02:45, 18.34s/it]
 78%|███████▊  | 28/36 [08:35<02:48, 21.01s/it]
                                               

 78%|███████▊  | 28/36 [08:35<02:48, 21.01s/it]
 81%|████████  | 29/36 [08:53<02:20, 20.01s/it]
                                               

 81%|████████  | 29/36 [08:53<02:20, 20.01s/it]
 83%|████████▎ | 30/36 [09:03<01:43, 17.19s/it]
                                               

 83%|████████▎ | 30/36 [09:03<01:43, 17.19s/it]
 86%|████████▌ | 31/36 [09:17<01:19, 15.97s/it]
                                               

 86%|████████▌ | 31/36 [09:17<01:19, 15.97s/it]
 89%|████████▉ | 32/36 [09:42<01:15, 18.79s/it]
                                               

 89%|████████▉ | 32/36 [09:42<01:15, 18.79s/it]
 92%|█████████▏| 33/36 [09:59<00:54, 18.26s/it]
                                               

 92%|█████████▏| 33/36 [09:59<00:54, 18.26s/it]
 94%|█████████▍| 34/36 [10:13<00:33, 16.96s/it]
                                               

 94%|█████████▍| 34/36 [10:13<00:33, 16.96s/it]
 97%|█████████▋| 35/36 [10:31<00:17, 17.36s/it]
                                               

 97%|█████████▋| 35/36 [10:31<00:17, 17.36s/it]
100%|██████████| 36/36 [10:54<00:00, 19.10s/it]
                                               

100%|██████████| 36/36 [10:54<00:00, 19.10s/it]
                                               

100%|██████████| 36/36 [10:56<00:00, 19.10s/it]
100%|██████████| 36/36 [10:56<00:00, 18.24s/it]
{'loss': 1.002, 'grad_norm': 2.386012315750122, 'learning_rate': 0.0, 'entropy': 1.6678120717406273, 'num_tokens': 1968.0, 'mean_token_accuracy': 0.8103914856910706, 'epoch': 0.37}
{'loss': 1.0445, 'grad_norm': 2.4037837982177734, 'learning_rate': 2.5e-06, 'entropy': 1.3884074874222279, 'num_tokens': 5029.0, 'mean_token_accuracy': 0.8093724586069584, 'epoch': 0.74}
{'loss': 1.0964, 'grad_norm': 2.9695520401000977, 'learning_rate': 5e-06, 'entropy': 1.5278582031076604, 'num_tokens': 6318.0, 'mean_token_accuracy': 0.8274629007686268, 'epoch': 1.0}
{'loss': 1.2099, 'grad_norm': 3.2514638900756836, 'learning_rate': 7.500000000000001e-06, 'entropy': 1.6622062027454376, 'num_tokens': 8014.0, 'mean_token_accuracy': 0.7986961454153061, 'epoch': 1.37}
{'loss': 1.0109, 'grad_norm': 2.1543750762939453, 'learning_rate': 1e-05, 'entropy': 1.3559885881841183, 'num_tokens': 11175.0, 'mean_token_accuracy': 0.824311900883913, 'epoch': 1.74}
{'loss': 0.8323, 'grad_norm': 2.1051833629608154, 'learning_rate': 9.975923633360985e-06, 'entropy': 1.589502383362163, 'num_tokens': 12636.0, 'mean_token_accuracy': 0.8517454103990034, 'epoch': 2.0}
{'loss': 0.9866, 'grad_norm': 2.3008551597595215, 'learning_rate': 9.903926402016153e-06, 'entropy': 1.5223053693771362, 'num_tokens': 14723.0, 'mean_token_accuracy': 0.839100543409586, 'epoch': 2.37}
{'loss': 1.0358, 'grad_norm': 2.7579479217529297, 'learning_rate': 9.784701678661045e-06, 'entropy': 1.5123180635273457, 'num_tokens': 17027.0, 'mean_token_accuracy': 0.8221327364444733, 'epoch': 2.74}
{'loss': 0.8285, 'grad_norm': 1.5702319145202637, 'learning_rate': 9.619397662556434e-06, 'entropy': 1.5811975869265469, 'num_tokens': 18954.0, 'mean_token_accuracy': 0.8222464431415905, 'epoch': 3.0}
{'loss': 0.9043, 'grad_norm': 1.8812124729156494, 'learning_rate': 9.409606321741776e-06, 'entropy': 1.5188702829182148, 'num_tokens': 21674.0, 'mean_token_accuracy': 0.8265761323273182, 'epoch': 3.37}
{'loss': 0.8397, 'grad_norm': 2.2950439453125, 'learning_rate': 9.157348061512728e-06, 'entropy': 1.6356830187141895, 'num_tokens': 23385.0, 'mean_token_accuracy': 0.848803736269474, 'epoch': 3.74}
{'loss': 0.9174, 'grad_norm': 2.197089433670044, 'learning_rate': 8.865052266813686e-06, 'entropy': 1.421536619013006, 'num_tokens': 25272.0, 'mean_token_accuracy': 0.8264588605273854, 'epoch': 4.0}
{'loss': 0.8801, 'grad_norm': 1.8561780452728271, 'learning_rate': 8.535533905932739e-06, 'entropy': 1.4514975994825363, 'num_tokens': 28245.0, 'mean_token_accuracy': 0.8231428451836109, 'epoch': 4.37}
{'loss': 0.8429, 'grad_norm': 2.3061466217041016, 'learning_rate': 8.171966420818227e-06, 'entropy': 1.5802567154169083, 'num_tokens': 30283.0, 'mean_token_accuracy': 0.8271992728114128, 'epoch': 4.74}
{'loss': 0.7475, 'grad_norm': 2.1123287677764893, 'learning_rate': 7.777851165098012e-06, 'entropy': 1.6152980002489956, 'num_tokens': 31590.0, 'mean_token_accuracy': 0.8613714250651273, 'epoch': 5.0}
{'loss': 0.8752, 'grad_norm': 2.4998180866241455, 'learning_rate': 7.3569836841299905e-06, 'entropy': 1.6368180140852928, 'num_tokens': 33360.0, 'mean_token_accuracy': 0.8332529291510582, 'epoch': 5.37}
{'loss': 0.7462, 'grad_norm': 1.5721269845962524, 'learning_rate': 6.913417161825449e-06, 'entropy': 1.4183942936360836, 'num_tokens': 36519.0, 'mean_token_accuracy': 0.849718414247036, 'epoch': 5.74}
{'loss': 0.805, 'grad_norm': 2.405885696411133, 'learning_rate': 6.451423386272312e-06, 'entropy': 1.5972948507829146, 'num_tokens': 37908.0, 'mean_token_accuracy': 0.8347415707328103, 'epoch': 6.0}
{'loss': 0.7339, 'grad_norm': 1.828986406326294, 'learning_rate': 5.975451610080643e-06, 'entropy': 1.4845862239599228, 'num_tokens': 40619.0, 'mean_token_accuracy': 0.8368060104548931, 'epoch': 6.37}
{'loss': 0.7745, 'grad_norm': 2.00793194770813, 'learning_rate': 5.490085701647805e-06, 'entropy': 1.553262583911419, 'num_tokens': 43042.0, 'mean_token_accuracy': 0.8567813038825989, 'epoch': 6.74}
{'loss': 0.7454, 'grad_norm': 2.2980940341949463, 'learning_rate': 5e-06, 'entropy': 1.6345414085821672, 'num_tokens': 44226.0, 'mean_token_accuracy': 0.8426129384474321, 'epoch': 7.0}
{'loss': 0.6596, 'grad_norm': 1.7042779922485352, 'learning_rate': 4.509914298352197e-06, 'entropy': 1.5351210460066795, 'num_tokens': 46461.0, 'mean_token_accuracy': 0.8864378668367863, 'epoch': 7.37}
{'loss': 0.7271, 'grad_norm': 1.642480492591858, 'learning_rate': 4.02454838991936e-06, 'entropy': 1.5389362946152687, 'num_tokens': 49072.0, 'mean_token_accuracy': 0.8503848239779472, 'epoch': 7.74}
{'loss': 0.8016, 'grad_norm': 2.4330005645751953, 'learning_rate': 3.5485766137276894e-06, 'entropy': 1.588984413580461, 'num_tokens': 50544.0, 'mean_token_accuracy': 0.8205744353207675, 'epoch': 8.0}
{'loss': 0.7527, 'grad_norm': 1.9567660093307495, 'learning_rate': 3.0865828381745515e-06, 'entropy': 1.4498572759330273, 'num_tokens': 53147.0, 'mean_token_accuracy': 0.831371396780014, 'epoch': 8.37}
{'loss': 0.6927, 'grad_norm': 1.8003907203674316, 'learning_rate': 2.6430163158700116e-06, 'entropy': 1.607713084667921, 'num_tokens': 55190.0, 'mean_token_accuracy': 0.8694727867841721, 'epoch': 8.74}
{'loss': 0.6207, 'grad_norm': 1.6587848663330078, 'learning_rate': 2.2221488349019903e-06, 'entropy': 1.6167547269300981, 'num_tokens': 56862.0, 'mean_token_accuracy': 0.8961980397051031, 'epoch': 9.0}
{'loss': 0.6368, 'grad_norm': 1.551279067993164, 'learning_rate': 1.8280335791817733e-06, 'entropy': 1.5565419383347034, 'num_tokens': 59543.0, 'mean_token_accuracy': 0.8657520264387131, 'epoch': 9.37}
{'loss': 0.7106, 'grad_norm': 2.044733762741089, 'learning_rate': 1.4644660940672628e-06, 'entropy': 1.5979384370148182, 'num_tokens': 61573.0, 'mean_token_accuracy': 0.8532840460538864, 'epoch': 9.74}
{'loss': 0.7424, 'grad_norm': 1.9428529739379883, 'learning_rate': 1.134947733186315e-06, 'entropy': 1.4783042398366062, 'num_tokens': 63180.0, 'mean_token_accuracy': 0.8778879534114491, 'epoch': 10.0}
{'loss': 0.7754, 'grad_norm': 2.087916612625122, 'learning_rate': 8.426519384872733e-07, 'entropy': 1.5532260611653328, 'num_tokens': 65326.0, 'mean_token_accuracy': 0.8664120212197304, 'epoch': 10.37}
{'loss': 0.6108, 'grad_norm': 1.5266973972320557, 'learning_rate': 5.903936782582253e-07, 'entropy': 1.4917264878749847, 'num_tokens': 67813.0, 'mean_token_accuracy': 0.8663139119744301, 'epoch': 10.74}
{'loss': 0.6689, 'grad_norm': 1.5661303997039795, 'learning_rate': 3.8060233744356634e-07, 'entropy': 1.6349257230758667, 'num_tokens': 69498.0, 'mean_token_accuracy': 0.8604073470289056, 'epoch': 11.0}
{'loss': 0.7009, 'grad_norm': 1.9380265474319458, 'learning_rate': 2.152983213389559e-07, 'entropy': 1.5997699834406376, 'num_tokens': 71308.0, 'mean_token_accuracy': 0.8598572909832001, 'epoch': 11.37}
{'loss': 0.6784, 'grad_norm': 1.8492374420166016, 'learning_rate': 9.607359798384785e-08, 'entropy': 1.6602987572550774, 'num_tokens': 73415.0, 'mean_token_accuracy': 0.8869365006685257, 'epoch': 11.74}
{'loss': 0.6572, 'grad_norm': 1.6550756692886353, 'learning_rate': 2.4076366639015914e-08, 'entropy': 1.325790985064073, 'num_tokens': 75816.0, 'mean_token_accuracy': 0.8415168198672208, 'epoch': 12.0}
{'train_runtime': 656.5644, 'train_samples_per_second': 0.786, 'train_steps_per_second': 0.055, 'train_loss': 0.8137495385275947, 'epoch': 12.0}
モデルを保存中...
Llama 3.2 3B QLoRA学習完了！モデルは ../outputs/llama32-3b-typst-qlora-003-008 に保存されました。
テスト実行: ../outputs/llama32-3b-typst-qlora-003-008
テストログ: ../logs/003-008_test_20250917_013643.log
実行中: python3 inference_llama32_3b.py                 --peft_model_path ../outputs/llama32-3b-typst-qlora-003-008                 --input_file ../sample/sample_small.tex                 --output_file ../trained/003-008.md
ベースモデル: meta-llama/Llama-3.2-3B-Instruct
✅ 使用モデル: PEFT（LoRA適用済み）
   PEFTモデル: ../outputs/llama32-3b-typst-qlora-003-008
入力ファイル: ../sample/sample_small.tex
出力ファイル: ../trained/003-008.md
トークナイザーを読み込み中...
4bit量子化設定を準備中...
ベースモデルを読み込み中...

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.71s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.44s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.63s/it]
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
✅ PEFTモデルを読み込み中...
   PEFTパス: ../outputs/llama32-3b-typst-qlora-003-008
   PEFTモデル読み込み完了
LaTeXファイルを読み込み中...
LaTeXテキストを 1 チャンクに分割しました
LaTeX→Typst変換を開始します...
チャンク 1/1 を変換中...
  入力テキスト長: 865 文字
  チャットテンプレート適用成功
  プロンプト長: 1223 文字
  入力トークン数: 415
  生成開始...
  生成完了: 629 トークン
  生成テキスト長: 1715 文字
  プロンプト長: 1223 文字
  生成テキスト開始: system

Cutting Knowledge Date: December 2023
Today Date: 17 Sep 2025

LaTeX 形式の文章を Typst形式 へ変換してください.
重要: TypstはLaTeXの代替となる文書組版システムです。TypeScript（プログラミング言語）ではありません。
出力はTypst構文のみ。プログラミングコードは出力しません。user...
  生成テキスト終了: ... $\left(\varepsilon_0, \varepsilon_1\right) \in \mathbb{R}^2$ are small, and $\varphi$ is a regular non-negative function. Our goal is to show the global existence of solutions under these conditions.
  生成テキスト全体の長さ: 1715 文字
  プロンプトの長さ: 1223 文字
  方法1（全体使用）: 1715 文字
  方法3（```typstマーカー）: 638 文字
  最終応答: system

Cutting Knowledge Date: December 2023
Today Date: 17 Sep 2025

LaTeX 形式の文章を Typst形式 へ変換してください.
重要: TypstはLaTeXの代替となる文書組版システムです。TypeScript（プログラミング言語）ではありません。
出力はTypst構文のみ。プログラミングコードは出力しません。user...
  最終応答の最後: ...non-negative function. Our goal is to show the global existence of solutions under these conditions.
  抽出された応答: system

Cutting Knowledge Date: December 2023
Today Date: 17 Sep 2025

LaTeX 形式の文章を Typst形式 へ変換してくださ...
結果を ../trained/003-008.md に保存中...
変換完了！
完了: Iteration 008 の学習・テストが完了しました。
チェックポイントを更新: ../outputs/llama32-3b-typst-qlora-003-008
=== Iteration 008 の処理完了 ===
--------------------------------------------------
=== Iteration 009 の処理開始 ===
継続学習: ../jsonl/train_003.jsonl (from ../outputs/llama32-3b-typst-qlora-003-008)
学習ログ: ../logs/003-009_training_20250917_013643.log
実行中: python3 train_llama32_3b_qlora_fixed.py             --data ../jsonl/train_003.jsonl             --out ../outputs/llama32-3b-typst-qlora-003-009             --peft_model_path ../outputs/llama32-3b-typst-qlora-003-008             --epochs 12             --batch_size 1             --grad_accum 16             --learning_rate 1e-05             --lora_r 8             --lora_alpha 8             --lora_dropout 0.1
Llama 3.2 3Bモデル: meta-llama/Llama-3.2-3B-Instruct
データ: ../jsonl/train_003.jsonl
出力: ../outputs/llama32-3b-typst-qlora-003-009
学習率: 1e-05
エポック数: 12
バッチサイズ: 1
勾配累積: 16
LoRA rank: 8
LoRA alpha: 8
LoRA dropout: 0.1
==================================================
トークナイザーを読み込み中...
4bit量子化設定を準備中...
モデルを読み込み中...

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.72s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.63s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.79s/it]
/mnt/d/github/typst-tex-converter/venv/lib/python3.11/site-packages/peft/tuners/lora/bnb.py:348: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.
  warnings.warn(
/mnt/d/github/typst-tex-converter/venv/lib/python3.11/site-packages/peft/tuners/tuners_utils.py:196: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!
  warnings.warn(
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 128009}.
既存のPEFTモデルを読み込み中: ../outputs/llama32-3b-typst-qlora-003-008
データセットを読み込み中...
データセットサイズ: 43
Llama 3.2 3B QLoRA学習を開始します...

  0%|          | 0/36 [00:00<?, ?it/s]
  3%|▎         | 1/36 [00:09<05:30,  9.44s/it]
                                              

  3%|▎         | 1/36 [00:09<05:30,  9.44s/it]
  6%|▌         | 2/36 [00:36<11:11, 19.75s/it]
                                              

  6%|▌         | 2/36 [00:36<11:11, 19.75s/it]
  8%|▊         | 3/36 [00:45<08:11, 14.90s/it]
                                              

  8%|▊         | 3/36 [00:45<08:11, 14.90s/it]
 11%|█         | 4/36 [00:58<07:32, 14.14s/it]
                                              

 11%|█         | 4/36 [00:58<07:32, 14.14s/it]
 14%|█▍        | 5/36 [01:22<09:12, 17.83s/it]
                                              

 14%|█▍        | 5/36 [01:22<09:12, 17.83s/it]
 17%|█▋        | 6/36 [01:40<08:49, 17.64s/it]
                                              

 17%|█▋        | 6/36 [01:40<08:49, 17.64s/it]
 19%|█▉        | 7/36 [01:55<08:10, 16.93s/it]
                                              

 19%|█▉        | 7/36 [01:55<08:10, 16.93s/it]
 22%|██▏       | 8/36 [02:13<08:05, 17.33s/it]
                                              

 22%|██▏       | 8/36 [02:13<08:05, 17.33s/it]
 25%|██▌       | 9/36 [02:34<08:16, 18.37s/it]
                                              

 25%|██▌       | 9/36 [02:34<08:16, 18.37s/it]
 28%|██▊       | 10/36 [03:00<08:59, 20.73s/it]
                                               

 28%|██▊       | 10/36 [03:00<08:59, 20.73s/it]
 31%|███       | 11/36 [03:18<08:18, 19.95s/it]
                                               

 31%|███       | 11/36 [03:18<08:18, 19.95s/it]
 33%|███▎      | 12/36 [03:34<07:26, 18.62s/it]
                                               

 33%|███▎      | 12/36 [03:34<07:26, 18.62s/it]
 36%|███▌      | 13/36 [04:02<08:14, 21.52s/it]
                                               

 36%|███▌      | 13/36 [04:02<08:14, 21.52s/it]
 39%|███▉      | 14/36 [04:20<07:29, 20.41s/it]
                                               

 39%|███▉      | 14/36 [04:20<07:29, 20.41s/it]
 42%|████▏     | 15/36 [04:33<06:25, 18.38s/it]
                                               

 42%|████▏     | 15/36 [04:33<06:25, 18.38s/it]
 44%|████▍     | 16/36 [04:47<05:37, 16.88s/it]
                                               

 44%|████▍     | 16/36 [04:47<05:37, 16.88s/it]
 47%|████▋     | 17/36 [05:23<07:11, 22.70s/it]
                                               

 47%|████▋     | 17/36 [05:23<07:11, 22.70s/it]
 50%|█████     | 18/36 [05:33<05:40, 18.92s/it]
                                               

 50%|█████     | 18/36 [05:33<05:40, 18.92s/it]
 53%|█████▎    | 19/36 [05:57<05:49, 20.53s/it]
                                               

 53%|█████▎    | 19/36 [05:57<05:49, 20.53s/it]
 56%|█████▌    | 20/36 [06:21<05:42, 21.43s/it]
                                               

 56%|█████▌    | 20/36 [06:21<05:42, 21.43s/it]
 58%|█████▊    | 21/36 [06:32<04:34, 18.27s/it]
                                               

 58%|█████▊    | 21/36 [06:32<04:34, 18.27s/it]
 61%|██████    | 22/36 [06:56<04:38, 19.88s/it]
                                               

 61%|██████    | 22/36 [06:56<04:38, 19.88s/it]
 64%|██████▍   | 23/36 [07:20<04:37, 21.38s/it]
                                               

 64%|██████▍   | 23/36 [07:20<04:37, 21.38s/it]
 67%|██████▋   | 24/36 [07:31<03:38, 18.23s/it]
                                               

 67%|██████▋   | 24/36 [07:31<03:38, 18.23s/it]
 69%|██████▉   | 25/36 [07:48<03:16, 17.90s/it]
                                               

 69%|██████▉   | 25/36 [07:48<03:16, 17.90s/it]
 72%|███████▏  | 26/36 [08:09<03:05, 18.57s/it]
                                               

 72%|███████▏  | 26/36 [08:09<03:05, 18.57s/it]
 75%|███████▌  | 27/36 [08:30<02:55, 19.47s/it]
                                               

 75%|███████▌  | 27/36 [08:30<02:55, 19.47s/it]
 78%|███████▊  | 28/36 [08:59<02:58, 22.34s/it]
                                               

 78%|███████▊  | 28/36 [08:59<02:58, 22.34s/it]
 81%|████████  | 29/36 [09:18<02:29, 21.30s/it]
                                               

 81%|████████  | 29/36 [09:18<02:29, 21.30s/it]
 83%|████████▎ | 30/36 [09:29<01:49, 18.27s/it]
                                               

 83%|████████▎ | 30/36 [09:29<01:49, 18.27s/it]
 86%|████████▌ | 31/36 [09:44<01:25, 17.11s/it]
                                               

 86%|████████▌ | 31/36 [09:44<01:25, 17.11s/it]
 89%|████████▉ | 32/36 [10:11<01:20, 20.21s/it]
                                               

 89%|████████▉ | 32/36 [10:11<01:20, 20.21s/it]
 92%|█████████▏| 33/36 [10:30<00:59, 19.71s/it]
                                               

 92%|█████████▏| 33/36 [10:30<00:59, 19.71s/it]
 94%|█████████▍| 34/36 [10:44<00:36, 18.15s/it]
                                               

 94%|█████████▍| 34/36 [10:44<00:36, 18.15s/it]
 97%|█████████▋| 35/36 [11:04<00:18, 18.59s/it]
                                               

 97%|█████████▋| 35/36 [11:04<00:18, 18.59s/it]
100%|██████████| 36/36 [11:29<00:00, 20.54s/it]
                                               

100%|██████████| 36/36 [11:29<00:00, 20.54s/it]
                                               

100%|██████████| 36/36 [11:31<00:00, 20.54s/it]
100%|██████████| 36/36 [11:31<00:00, 19.20s/it]
{'loss': 1.0021, 'grad_norm': 2.1798250675201416, 'learning_rate': 0.0, 'entropy': 1.6673270389437675, 'num_tokens': 1968.0, 'mean_token_accuracy': 0.8103914856910706, 'epoch': 0.37}
{'loss': 1.0451, 'grad_norm': 2.203592538833618, 'learning_rate': 2.5e-06, 'entropy': 1.3878592662513256, 'num_tokens': 5029.0, 'mean_token_accuracy': 0.8093724586069584, 'epoch': 0.74}
{'loss': 1.0963, 'grad_norm': 2.7560312747955322, 'learning_rate': 5e-06, 'entropy': 1.5269450816241177, 'num_tokens': 6318.0, 'mean_token_accuracy': 0.8274629007686268, 'epoch': 1.0}
{'loss': 1.2109, 'grad_norm': 2.964674949645996, 'learning_rate': 7.500000000000001e-06, 'entropy': 1.6613271981477737, 'num_tokens': 8014.0, 'mean_token_accuracy': 0.7986961454153061, 'epoch': 1.37}
{'loss': 1.012, 'grad_norm': 1.9761823415756226, 'learning_rate': 1e-05, 'entropy': 1.3550515621900558, 'num_tokens': 11175.0, 'mean_token_accuracy': 0.8249186985194683, 'epoch': 1.74}
{'loss': 0.8335, 'grad_norm': 1.957632303237915, 'learning_rate': 9.975923633360985e-06, 'entropy': 1.58796766129407, 'num_tokens': 12636.0, 'mean_token_accuracy': 0.8517454103990034, 'epoch': 2.0}
{'loss': 0.9888, 'grad_norm': 2.105733633041382, 'learning_rate': 9.903926402016153e-06, 'entropy': 1.5209044478833675, 'num_tokens': 14723.0, 'mean_token_accuracy': 0.8388511762022972, 'epoch': 2.37}
{'loss': 1.0385, 'grad_norm': 2.5514633655548096, 'learning_rate': 9.784701678661045e-06, 'entropy': 1.5109331235289574, 'num_tokens': 17027.0, 'mean_token_accuracy': 0.8221327364444733, 'epoch': 2.74}
{'loss': 0.8308, 'grad_norm': 1.459030270576477, 'learning_rate': 9.619397662556434e-06, 'entropy': 1.5797536048022183, 'num_tokens': 18954.0, 'mean_token_accuracy': 0.8229299729520624, 'epoch': 3.0}
{'loss': 0.9073, 'grad_norm': 1.7921710014343262, 'learning_rate': 9.409606321741776e-06, 'entropy': 1.5172080770134926, 'num_tokens': 21674.0, 'mean_token_accuracy': 0.8259974308311939, 'epoch': 3.37}
{'loss': 0.8433, 'grad_norm': 2.14060115814209, 'learning_rate': 9.157348061512728e-06, 'entropy': 1.6336551941931248, 'num_tokens': 23385.0, 'mean_token_accuracy': 0.8478846177458763, 'epoch': 3.74}
{'loss': 0.9212, 'grad_norm': 2.058034896850586, 'learning_rate': 8.865052266813686e-06, 'entropy': 1.4199990413405679, 'num_tokens': 25272.0, 'mean_token_accuracy': 0.8263365030288696, 'epoch': 4.0}
{'loss': 0.8844, 'grad_norm': 1.7364070415496826, 'learning_rate': 8.535533905932739e-06, 'entropy': 1.4498042725026608, 'num_tokens': 28245.0, 'mean_token_accuracy': 0.8226729221642017, 'epoch': 4.37}
{'loss': 0.8481, 'grad_norm': 2.15472149848938, 'learning_rate': 8.171966420818227e-06, 'entropy': 1.5781745091080666, 'num_tokens': 30283.0, 'mean_token_accuracy': 0.8271992728114128, 'epoch': 4.74}
{'loss': 0.7505, 'grad_norm': 2.0298705101013184, 'learning_rate': 7.777851165098012e-06, 'entropy': 1.6134657913988286, 'num_tokens': 31590.0, 'mean_token_accuracy': 0.8619325919584795, 'epoch': 5.0}
{'loss': 0.8811, 'grad_norm': 2.3692386150360107, 'learning_rate': 7.3569836841299905e-06, 'entropy': 1.6344665214419365, 'num_tokens': 33360.0, 'mean_token_accuracy': 0.8332529291510582, 'epoch': 5.37}
{'loss': 0.7512, 'grad_norm': 1.50371515750885, 'learning_rate': 6.913417161825449e-06, 'entropy': 1.4162122383713722, 'num_tokens': 36519.0, 'mean_token_accuracy': 0.8497111350297928, 'epoch': 5.74}
{'loss': 0.8112, 'grad_norm': 2.2796852588653564, 'learning_rate': 6.451423386272312e-06, 'entropy': 1.5949289798736572, 'num_tokens': 37908.0, 'mean_token_accuracy': 0.8347415707328103, 'epoch': 6.0}
{'loss': 0.7392, 'grad_norm': 1.7264552116394043, 'learning_rate': 5.975451610080643e-06, 'entropy': 1.482303787022829, 'num_tokens': 40619.0, 'mean_token_accuracy': 0.8378305993974209, 'epoch': 6.37}
{'loss': 0.7803, 'grad_norm': 1.9229851961135864, 'learning_rate': 5.490085701647805e-06, 'entropy': 1.5504726581275463, 'num_tokens': 43042.0, 'mean_token_accuracy': 0.8567813038825989, 'epoch': 6.74}
{'loss': 0.7508, 'grad_norm': 2.195441722869873, 'learning_rate': 5e-06, 'entropy': 1.6319798122752796, 'num_tokens': 44226.0, 'mean_token_accuracy': 0.8426129384474321, 'epoch': 7.0}
{'loss': 0.6646, 'grad_norm': 1.6197736263275146, 'learning_rate': 4.509914298352197e-06, 'entropy': 1.5328516513109207, 'num_tokens': 46461.0, 'mean_token_accuracy': 0.8859817199409008, 'epoch': 7.37}
{'loss': 0.7326, 'grad_norm': 1.5705844163894653, 'learning_rate': 4.02454838991936e-06, 'entropy': 1.5362492203712463, 'num_tokens': 49072.0, 'mean_token_accuracy': 0.849740494042635, 'epoch': 7.74}
{'loss': 0.8089, 'grad_norm': 2.38458251953125, 'learning_rate': 3.5485766137276894e-06, 'entropy': 1.5864295742728494, 'num_tokens': 50544.0, 'mean_token_accuracy': 0.8167909817262129, 'epoch': 8.0}
{'loss': 0.7599, 'grad_norm': 1.8765149116516113, 'learning_rate': 3.0865828381745515e-06, 'entropy': 1.4470697231590748, 'num_tokens': 53147.0, 'mean_token_accuracy': 0.8317571990191936, 'epoch': 8.37}
{'loss': 0.6982, 'grad_norm': 1.7396900653839111, 'learning_rate': 2.6430163158700116e-06, 'entropy': 1.605034712702036, 'num_tokens': 55190.0, 'mean_token_accuracy': 0.8688659891486168, 'epoch': 8.74}
{'loss': 0.6256, 'grad_norm': 1.5876938104629517, 'learning_rate': 2.2221488349019903e-06, 'entropy': 1.6143015189604326, 'num_tokens': 56862.0, 'mean_token_accuracy': 0.8956262848593972, 'epoch': 9.0}
{'loss': 0.6422, 'grad_norm': 1.4940288066864014, 'learning_rate': 1.8280335791817733e-06, 'entropy': 1.5539792589843273, 'num_tokens': 59543.0, 'mean_token_accuracy': 0.8691140189766884, 'epoch': 9.37}
{'loss': 0.7158, 'grad_norm': 1.9709622859954834, 'learning_rate': 1.4644660940672628e-06, 'entropy': 1.5952626392245293, 'num_tokens': 61573.0, 'mean_token_accuracy': 0.8532840460538864, 'epoch': 9.74}
{'loss': 0.7485, 'grad_norm': 1.9154270887374878, 'learning_rate': 1.134947733186315e-06, 'entropy': 1.4754001769152554, 'num_tokens': 63180.0, 'mean_token_accuracy': 0.874210926619443, 'epoch': 10.0}
{'loss': 0.7818, 'grad_norm': 2.022409677505493, 'learning_rate': 8.426519384872733e-07, 'entropy': 1.5506043136119843, 'num_tokens': 65326.0, 'mean_token_accuracy': 0.8673301823437214, 'epoch': 10.37}
{'loss': 0.6153, 'grad_norm': 1.5097999572753906, 'learning_rate': 5.903936782582253e-07, 'entropy': 1.4894626438617706, 'num_tokens': 67813.0, 'mean_token_accuracy': 0.8656345643103123, 'epoch': 10.74}
{'loss': 0.6732, 'grad_norm': 1.5371233224868774, 'learning_rate': 3.8060233744356634e-07, 'entropy': 1.632684350013733, 'num_tokens': 69498.0, 'mean_token_accuracy': 0.8598770986903798, 'epoch': 11.0}
{'loss': 0.7056, 'grad_norm': 1.8609124422073364, 'learning_rate': 2.152983213389559e-07, 'entropy': 1.5971117913722992, 'num_tokens': 71308.0, 'mean_token_accuracy': 0.8598572909832001, 'epoch': 11.37}
{'loss': 0.6851, 'grad_norm': 1.7882188558578491, 'learning_rate': 9.607359798384785e-08, 'entropy': 1.6572042182087898, 'num_tokens': 73415.0, 'mean_token_accuracy': 0.8862172104418278, 'epoch': 11.74}
{'loss': 0.6635, 'grad_norm': 1.6038819551467896, 'learning_rate': 2.4076366639015914e-08, 'entropy': 1.3234422206878662, 'num_tokens': 75816.0, 'mean_token_accuracy': 0.843869539824399, 'epoch': 12.0}
{'train_runtime': 691.0954, 'train_samples_per_second': 0.747, 'train_steps_per_second': 0.052, 'train_loss': 0.817985917131106, 'epoch': 12.0}
モデルを保存中...
Llama 3.2 3B QLoRA学習完了！モデルは ../outputs/llama32-3b-typst-qlora-003-009 に保存されました。
スキップ: Iteration 009 のテストをスキップしました（2回に1回の実行）
チェックポイントを更新: ../outputs/llama32-3b-typst-qlora-003-009
=== Iteration 009 の処理完了 ===
--------------------------------------------------
=== 汎用的継続学習完了 ===
最終チェックポイント: ../outputs/llama32-3b-typst-qlora-003-009
総実行回数: 10回
=== 002-010をbaseに003-000から003-010への学習完了 ===
終了時刻: Wed Sep 17 03:46:11 JST 2025
================================
